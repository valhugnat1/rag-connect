{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open AI api key \n",
    "#\n",
    "\n",
    "# Pipcone API key \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinecone-client\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of URLs:\n",
      "https://mlops-platform-documentation.craft.ai/\n",
      "https://mlops-platform-documentation.craft.ai/administration.html\n",
      "https://mlops-platform-documentation.craft.ai/getStarted/Setup.html\n",
      "https://mlops-platform-documentation.craft.ai/getStarted/Setup.html\n",
      "https://mlops-platform-documentation.craft.ai/getStarted/part_1.html\n",
      "https://mlops-platform-documentation.craft.ai/getStarted/part_2.html\n",
      "https://mlops-platform-documentation.craft.ai/getStarted/part_3.html\n",
      "https://mlops-platform-documentation.craft.ai/getStarted/part_4.html\n",
      "https://mlops-platform-documentation.craft.ai/user_workflow/introduction.html\n",
      "https://mlops-platform-documentation.craft.ai/user_workflow/introduction.html\n",
      "https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html\n",
      "https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html\n",
      "https://mlops-platform-documentation.craft.ai/environment/introduction.html\n",
      "https://mlops-platform-documentation.craft.ai/environment/introduction.html\n",
      "https://mlops-platform-documentation.craft.ai/environment/createEnvironment.html\n",
      "https://mlops-platform-documentation.craft.ai/environment/workOnEnvironment.html\n",
      "https://mlops-platform-documentation.craft.ai/environment/saveOnDataStore.html\n",
      "https://mlops-platform-documentation.craft.ai/stepPipeline/introduction.html\n",
      "https://mlops-platform-documentation.craft.ai/stepPipeline/introduction.html\n",
      "https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html\n",
      "https://mlops-platform-documentation.craft.ai/stepPipeline/manageStep.html\n",
      "https://mlops-platform-documentation.craft.ai/stepPipeline/composePipeline.html\n",
      "https://mlops-platform-documentation.craft.ai/deployment/introduction.html\n",
      "https://mlops-platform-documentation.craft.ai/deployment/introduction.html\n",
      "https://mlops-platform-documentation.craft.ai/deployment/chooseExecutionRule.html\n",
      "https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html\n",
      "https://mlops-platform-documentation.craft.ai/deployment/executePipeline.html\n",
      "https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html\n",
      "https://mlops-platform-documentation.craft.ai/deployment/metrics.html\n",
      "https://mlops-platform-documentation.craft.ai/histo_doc_sdk.html\n",
      "https://mlops-platform-documentation.craft.ai/changeLog.html\n",
      "https://mlops-platform-documentation.craft.ai/administration.html\n"
     ]
    }
   ],
   "source": [
    "# Function to read URLs from a file and return a list\n",
    "def read_urls_from_file(file_name):\n",
    "    urls = []\n",
    "    try:\n",
    "        with open(file_name, 'r') as file:\n",
    "            for line in file:\n",
    "                urls.append(line.strip())  # Strip to remove leading/trailing whitespaces\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File '{file_name}' not found.\")\n",
    "    return urls\n",
    "\n",
    "# Specify the file name generated from the crawling script\n",
    "file_name = \"mlops-platform-documentation_craft_ai.txt\"  # Replace with your generated file name\n",
    "\n",
    "# Read URLs from the file\n",
    "url_list = read_urls_from_file(file_name)\n",
    "\n",
    "# Print the list of URLs\n",
    "print(\"List of URLs:\")\n",
    "for url in url_list:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n\\n\\n\\nIntroduction — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nIntroduction\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction\\uf0c1\\nYou are a Data Scientist struggling with data, code, and models in your projects.\\nYou are an ML Engineer who has trouble replicating pipelines and monitoring models in production.\\nYou are Head of a Data Science team, who has trouble shipping quickly models to production.\\nYou are a CTO who has difficulty to move Python code into large-scale production and manage the DevOps workload on an AI project.\\n🚀 Then, you are a potential user of Craft AI’s MLOps platform, which aims to accelerate the deployment and improve the management of your Machine Learning models in production.\\n\\nWhat is Craft AI?\\uf0c1\\nCraft AI is an MLOps platform for data science teams.\\nTo use Craft AI, the basic workflow with the platform is:\\n\\nChoose a configured environment on the cloud provider of your choice\\nCreate Machine Learning pipelines with your Python code\\nDeploy and execute the pipelines on environments running on Kubernetes\\nMonitor the performance of the models in production and the health of the infrastructure\\n\\nThe platform aims to be an end-to-end MLOps tool that brings together all the MLOps functionalities required for the successful implementation of an AI project. It is therefore composed of the following main features:\\n\\nEnvironments\\nMachine Learning Pipelines\\nDeployments\\n\\n\\n\\n\\nMission\\uf0c1\\nOur mission is to democratize the use of trustworthy Artificial Intelligence on a day-to-day basis. How do we do this? By empowering Data Science teams to master their AI project from start to finish.  Our vision of AI is that every project of Data Science should be in production, responsible and profitable!\\nTo achieve this, we have developed a MLOps platform that allows anyone to put Python code into production, on a large scale, in a few clicks. We allow Data Scientist to deploy their models, choose their environments (development and production) and create pipelines to optimize their ML workflows.\\nHow does it work? We will contain your code in step to allow you to create ready to production ML and DL pipelines in an environment adapted to each project’s needs.\\n\\n\\nHistory\\uf0c1\\nCraft AI is a French company founded in 2015. We originally developed AI solutions for the energy, industry, healthcare, education and retail sectors. Our unique ability to deploy thousands of ML models at large scale with a focus on being always explainable, energy frugal and fair, drove us to develop a MLOps platform as a front end to our expertise. Today, with our MLOps platform, we can share our expertise in large-scale model deployment, at large scale.\\n\\n\\nWhat is MLOps?\\uf0c1\\nMachine Learning Operations (MLOps), aims to provide an end-to-end development process to design, build and manage reproducible, testable, and evolvable ML-powered software.\\nBeing an emerging field, MLOps is rapidly gaining momentum amongst Data Scientists, ML Engineers and AI enthusiasts. Following this trend, MLOps differentiates the ML models management from traditional software engineering like DevOps and suggests the following MLOps capabilities:\\n\\nMLOps aims to unify the release/production cycle for ML and software application release.\\nMLOps enables automated testing of machine learning artifacts (e.g. data validation, ML model testing, and ML model integration testing)ML\\nMLOps enables the application of agile principles to machine learning projects.\\nMLOps enables supporting machine learning models and datasets to build these models as first-class citizens within CI/CD systems.\\nMLOps reduces technical debt across machine learning models.\\nMLOps must be a language-, framework-, platform-, and infrastructure-agnostic practice.\\n\\n\\n\\nBenefits of using Craft AI\\uf0c1\\nThe main benefits of the platform for the users are:\\n\\nNo longer taking 6 months to deploy ML models in production but only a few clicks!\\nAllowing a Data Science team to be autonomous on AI in production without DevOps skills.\\nEnabling large scale production of Python code without refactoring to Java or C.\\nAutomating the execution of the pipelines to save time for Data Science teams.\\nEnsuring an efficient use of computing resources and reduce the cloud bill.\\nImproving model performance over time by automatically triggering re-training pipelines when performance drops.\\n\\n\\n\\nSDK Python technical documentation\\uf0c1\\nsdk_0.31.2.pdf\\n\\n\\n\\n\\n\\n\\n\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/', 'title': 'Introduction — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nAdministration — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nAdministration\\n\\n\\n\\n\\n\\n\\n\\n\\nAdministration\\uf0c1\\nThe MLOps - Craft AI Platform is composed of a graphical interface\\nallowing you to visualize, create, manage and monitor the objects\\nnecessary for the realization of your AI projects.\\nIt is composed of:\\n\\nHomepage: Visualize and create projects.\\nParameters: Manage the company’s account and users.\\nProject settings: Manage the configuration of a project.\\nEnvironments: See the environment(s) and their information within\\na project.\\nExecutions: See the execution(s) and their information within an\\nenvironment or a deployment.\\n\\n\\nNote: We strongly recommend Google Chrome browser to use the\\ngraphical interface of the platform.\\n\\nSummary:\\n\\nUsers\\nProjects\\nToken SDK\\n\\n\\nUsers\\uf0c1\\nSummary:\\n\\nManage user\\nLogin\\nGet user with ID\\n\\n\\nManage user\\uf0c1\\nThe management of user is available by email for the moment. In a next\\nversion, it will be possible to add, edit and delete a user directly on\\nthe platform UI.\\n\\nAdd a user\\uf0c1\\nSend a message to Craft AI with your request and the following\\ninformation:\\n\\nFirst and last name of the user\\nEmail of the user\\n\\n\\n\\n   🆕 Adding users will arrive later on the platform.\\n\\n\\n\\nAccess rights\\uf0c1\\nEach user has access to one or more defined projects.\\nEach user who has access to a project has access to all the information and actions in it.\\n\\n\\n   🆕 Advanced access rights will arrive later on the platform.\\n\\n\\n\\nDelete a user\\uf0c1\\nSend a message to Craft AI with your request and the following\\ninformation:\\n\\nFirst and last name of the user\\nEmail of the user\\n\\n\\n\\n   🆕 The deletion of users will arrive later on the platform.\\n\\n\\n\\n\\nLogin\\uf0c1\\nHere is the URL to access the platform:\\nhttps://mlops-platform.craft.ai\\n\\nFirst connection\\uf0c1\\nPrerequisites: The user must be added to the platform (Add a user).\\n\\nThe user connects to the platform: https://mlops-platform.craft.ai\\nThe connection will not work, but Craft AI will receive the request\\nand can add the user.\\nThe user receives an email/slack from Craft AI to inform him that he\\ncan log in.\\nThe user logs in with the same link and has access to the platform.\\n\\n\\n\\nForgot your password\\uf0c1\\n\\nIn the Login popup, click on Don’t remember your password?\\nEnter your email, you will receive an email to modify your password.\\n\\n\\n\\n\\nGet user with ID\\uf0c1\\n\\nFunction definition\\uf0c1\\nWhile using the SDK you may encounter outputs parameters containing a user when using specific functions. Each user is identified with a unique ID. That is the case for example in the sdk.get_pipeline_execution() function output with the parameter created_by. To match the user ID with the corresponding information (name and email) you can use the get_user() function.\\nCraftAiSdk.get_user(user_id)\\n\\n\\n\\n\\nParameters\\uf0c1\\n\\nuser_id (str) – The ID of the user.\\n\\n\\n\\nReturns\\uf0c1\\nThe user information in dict type, with the following keys:\\n\\nid (str): ID of the user.\\nname (str): Name of the user.\\nemail (str): Email of the user.\\n\\n\\n\\n\\n\\nProjects\\uf0c1\\nA project is a complete use case. From data import, through\\nexperimentation, testing and production, to performance monitoring over\\ntime.\\nUsers can create as many projects as they want. Each project is\\nhermetically sealed from the others. All users on the platform can\\naccess to all projects.\\nTo work on a project, it must include at least one environment. The\\nuser can create several environments in a project.\\nSummary:\\n\\nCreate a project\\nManange a prject\\n\\n\\nCreate a project\\uf0c1\\n\\nFrom the homepage, you can create a new project by clicking on the\\n“New project” button.\\nA project creation page opens in which you have to fill in the fields\\n:\\n\\n[Mandatory]Project name : Enter the name of your project (in\\nlowercase and with “-”), you will not be able to modify it later.\\n[Mandatory]Python version : Select the version of Python\\nthat will be applied to this project. It will be possible to\\nchoose a different version when creating each step.\\n[Mandatory]Repository URL : Enter the SSH URL of your repository.\\n\\nHow to get my repository URL\\nOn GitHub, from your repository, click on the “Code” button and\\nchoose SSH. The URL must start with “git@”.\\n\\nOn GitLab, from your repository, click on the “Clone” button and\\ncopy SSH URL. The URL must start with “git@”.\\n\\n\\n\\n[Mandatory]Deploy key : Enter your Github / GitLab private key.\\nHow to generate my deploy key :\\n\\nFor security reasons, to get access to your GitHub project, the\\nplatform uses a Deploy Key with the RSA SSH KEY standard. The\\ndeploy key is a special key that grants access to a specific\\nrepository; it is not the same as personal keys used commonly\\nby users to access their repositories, although they are both\\nSSH keys.\\nThe deploy key has two elements:\\n\\nThe public key, which must be set in the GitHub\\nadministration settings for the repository.\\nThe private key, which must be sent to the Craft AI MLOps\\nPlatform, so it can access the repository.\\n\\nPlease follow these steps:\\n\\nYou will need to generate an SSH key on your computer.\\n\\nOn Linux and macOS\\n\\nMove to a new directory and run the following command, by\\nreplacing *your-key-filename* by a name of\\nyour choosing.\\nssh-keygen -m PEM -t rsa -b 4096 -f *your-key-filename* -q -N \"\" -C \"\"\\nThis will generate two files: a file named\\n*your-key-filename* with the “private key” used\\nfor creating a step, and a file named\\n*your-key-filename*.pub with the “public key”\\nused to create the Deploy Key on GitHub. These files\\nshould not be included in the step’s directory. Only the\\ntype of SSH key generated by this command is accepted\\nwhen creating the Step.\\nExample on Linux / macOS :\\nssh-keygen -m PEM -t rsa -b 4096 -f *keyAccesPlatformCraftAI* -q -N \"\" -C \"\"\\n\\n\\n\\n\\nOn window\\n\\n\\nCheck that OpenSSH is installed and install it if it\\nis not the case. (Go in Settings > Apps & features >\\nOptional feature to get list of features and click on\\nadd feature to find and install OpenSSH)\\nPress the Windows key.\\nType cmd.\\nUnder Best Match, right-click Command Prompt.\\nClick Run as Administrator.\\nIf prompted, click Yes in\\nthe ``Do you want to allow this app to make changes to your device?``\\npop-up.\\nIn the command prompt, type the following : ssh-keygen\\nBy default, the system will save the keys to C:Usersyour_username/.ssh/id_rsa. You\\ncan use the default name, or you can choose more\\ndescriptive names. This can help distinguish between\\nkeys, if you are using multiple key pairs. To stick to\\nthe default option, press  Enter.\\nYou’ll be asked to enter a\\npassphrase. Hit Enter to skip this step.\\nThe system will generate the key pair, and display\\nthe key fingerprint and a randomart image.\\nOpen your file browser.\\nNavigate\\nto C:Usersyour_username/.ssh.\\nYou should see two files. The identification is saved\\nin the id_rsa file and the public key is\\nlabeled id_rsa.pub. This is your SSH key pair.\\n\\n\\n\\n\\n\\n\\nFor GitHub :\\n\\nHead to the homepage of your repository on GitHub.\\nGo to the Settings page.\\nOnce there, select the tab on the left named Deploy Keys\\nSelect Add deploy key on the Deploy Keys page.\\n\\n\\nInsert the name you want for your deploy key\\nCopy/paste the public key (content of\\n*your-key-filename*.pub) in the second text box.\\nClick on “Add key” (you don’t need to allow write\\naccess)\\n\\nFor GitLab :\\n\\nHead to the homepage of your repository on GitLab.\\nClick on Settings (left bar) then go to Repository\\nClick on Expand in the Deploy keys section\\nInsert the name you want for your deploy key.\\nCopy/paste the public key (content of\\n*your-key-filename*.pub) in the second text box.\\nClick on Add key (you don’t need to Grant write permissions to this key)\\n\\n\\n\\nDefault branch : Enter the Git branch you want as default\\nfor this project. If this field is empty, we will use the default\\nGit branch. It will be possible to choose a different default\\nbranch within an environment.\\n[Mandatory]Folders : Enter file(s) or folder(s) that will be\\naccessible by the platform. By default, type / to have access\\nto all the repositories. It will be possible to choose different\\nfiles or folders within an environment.\\nRequirements.txt : Enter the path to the requirements.txt file\\nwith the list of library to install automatically on this project.\\nIt will be possible to add a different file within an environment.\\nSystem dependencies : Enter the list of the APT and/or APK\\npackages that you want to install automatically on this project\\n(for Ubuntu distribution). It will be possible to add different\\npackages within an environment.\\n\\n\\nClick on “Create project”, you will see the project card displayed,\\nand you can enter in it to create a first environment.\\n\\nThere are no access rights at first, all users of the platform have\\naccess to all projects.\\n\\n\\nNote: You will be able to modify these elements (except the\\nproject name) in the Settings section of your project.\\n\\nNext step: Create an environment in this project.\\n\\n\\nManage a project\\uf0c1\\n\\nEdit a project\\uf0c1\\nWithin your project, click on Settings to view and edit your project\\ninformation.\\n\\nOn this page, you will find the information you set up when you created\\nthe project.\\nYou can change them, except the name of the project.\\n\\nWarning\\nDon’t forget to save and validate the confirmation slider\\nto make the changes effective.\\n\\nThe changes will apply to steps created after this modification. These\\nchanges may affect your steps and can make them non-functional.\\n\\n\\n\\nUsers in a project\\uf0c1\\nInitially, all users of the platform have access to all projects without\\nspecial access rights.\\n\\n\\n   🆕 The access rights per user within a project will arrive later on the\\n   platform.\\n\\n\\n\\nDelete a project\\uf0c1\\nInitially, you cannot delete a project.\\nSubmit an email request to delete a project from the platform.\\n\\n\\n   🆕 The deletion of a project will arrive later on the platform.\\n\\n\\n\\n\\n\\nAccess the SDK\\uf0c1\\nThe Craft AI MLOps Platform is composed of a Python SDK allowing you\\nto use, from your IDE, the functions developed by Craft AI to create\\nyour steps, pipelines and deployments.\\nSummary:\\n\\nGet token access\\nConnect to the SDK\\n\\n\\nGet token access\\uf0c1\\nIn the header, click on your name and go to the “Parameters” page.\\n\\nAt the bottom of the “Account” page, you will find your SDK token,\\nwhich will allow you to identify yourself to use the SDK.\\nYou can regenerate it, the old token will be obsolete, and you will\\nhave to identify yourself again to the SDK to access it.\\n\\nWarning\\nThe SDK token is strictly personal. You must keep it to yourself.\\n\\n\\n\\n\\nConnect to the SDK\\uf0c1\\nYou must install the SDK from a Python terminal, with the command :\\npip install craft-ai-sdk\\n\\n\\nRun the following commands in a Python terminal to initialize the\\nSDK.\\n\\nEnter the CRAFT_AI_ENVIRONMENT_URL, you can find on Get\\nenvironment URL.\\nIt should look like https://brisk-hawk-charcoal-zeta-42.mlops-platform.craft.ai.\\nIf you don’t have any environments in your project yet, you should\\nCreate an environment.\\n\\nEnter your personal CRAFT_AI_SDK_TOKEN, you can find it in “Parameters” on the platform web UI.\\nexport CRAFT_AI_ENVIRONMENT_URL=\"*your-env-URL*\"\\nexport CRAFT_AI_SDK_TOKEN=\"*your-token-acces*\"\\n\\n\\n\\nExecute the following Python code to set up SDK Python object.\\nThe SDK will automatically take into account your environment variables for the installation of the connection to the platform.\\nimport os\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk()\\n\\n\\nYou can also specify it directly in the constructor, although this method is not recommended.\\nimport os\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk(\\n    environment_url=\"*your-env-URL*\",\\n    sdk_token=\"*your-token-acces*\"\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nGet Started — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nGet Started\\n\\n\\n\\n\\n\\n\\n\\n\\nGet Started\\uf0c1\\nThe goal of this use case is to accompany you in understanding the mechanics of the platform with a simple and basic machine learning application.\\nThe final goal of this get started will be to be able to generate\\npredictions on the Craft AI platform. The model used to generate predictions\\nwill be the iris one, but you can use any Python code.\\nTo achieve this goal, we will go through several parts:\\n\\n\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\n\\nPart 0 : Setup\\uf0c1\\nThis page is about the setup of the platform in your Python code and in\\nthe Craft AI platform UI.\\nThere are 2 ways to access the platform:\\n\\nWith the Python SDK, in line of code\\nWith the web interface, in a browser\\n\\nPrerequisites\\n\\nPython 3.8 or higher is required to be installed on your\\ncomputer.\\nWe strongly recommend the use of Google Chrome browser to use the\\nUI of the platform.\\n\\n\\n\\n*Pipelines have one step for the moment. Multi-step pipelines will be\\navailable soon.\\nWe’ve already created for you :\\n\\nA project\\nA repository GitHub linked to the project\\nAn environment setup in the project with datastore and workers\\n\\n\\nSet up the Python SDK\\uf0c1\\nThe Python SDK can be installed with\\npip from a terminal.\\npip install craft-ai-sdk\\n\\n\\nFor this use case, you also need NumPy if you don’t have already\\ninstalled it on your machine.\\npip install numpy\\n\\n\\n\\n\\nInitialization of Python SDK\\uf0c1\\nRun the following commands in a Python terminal to initialize the SDK:\\n\\nSet the value of CRAFT_AI_ENVIRONMENT_URL into a local environment\\nvariable with your environment URL (you should get it from the Environment page on the UI. Environnement URL is NOT the HTML address of the UI).\\nSet the value of CRAFT_AI_SDK_TOKEN into a local\\nenvironment variable (On the UI, click on your name in the top right corner and then go to the “Parameters” page. There you can generate and find your SDK token at the bottom. The SDK token is strictly personal. You must keep it to yourself.)\\nA good practice would be to create a script per environment that\\ncontains the setup of these 2 local environment variables. For example,\\nyou could create an .env-test-platform-craft-ai file.\\n\\n#!/bin/bash\\n\\nexport CRAFT_AI_ENVIRONMENT_URL=\"*your-env-URL*\"\\nexport CRAFT_AI_ACCESS_TOKEN=\"*your-token-acces*\"\\n\\n\\n\\n\\nHere, we use the source command to define the values of the\\nvariables written to the file.\\nsource .env-test-platform-craft-ai\\n\\n\\n\\nExecute the following Python code to set up SDK Python object.\\nimport os\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk(\\n    environment_url=os.environ.get(\"CRAFT_AI_ENVIRONMENT_URL\"),\\n    sdk_token=os.environ.get(\"CRAFT_AI_ACCESS_TOKEN\")\\n)\\n\\n\\n🎉 Well done! You’re ready to execute your first code on the platform!\\n\\n\\n\\n\\nWhat’s next ?\\uf0c1\\nNow that we have configured the platform, we can create our first\\nobjects and run a “Hello World” on the platform.\\nNext step: Part 1: Deploy a simple pipeline\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/Setup.html', 'title': 'Get Started — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nGet Started — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nGet Started\\n\\n\\n\\n\\n\\n\\n\\n\\nGet Started\\uf0c1\\nThe goal of this use case is to accompany you in understanding the mechanics of the platform with a simple and basic machine learning application.\\nThe final goal of this get started will be to be able to generate\\npredictions on the Craft AI platform. The model used to generate predictions\\nwill be the iris one, but you can use any Python code.\\nTo achieve this goal, we will go through several parts:\\n\\n\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\n\\nPart 0 : Setup\\uf0c1\\nThis page is about the setup of the platform in your Python code and in\\nthe Craft AI platform UI.\\nThere are 2 ways to access the platform:\\n\\nWith the Python SDK, in line of code\\nWith the web interface, in a browser\\n\\nPrerequisites\\n\\nPython 3.8 or higher is required to be installed on your\\ncomputer.\\nWe strongly recommend the use of Google Chrome browser to use the\\nUI of the platform.\\n\\n\\n\\n*Pipelines have one step for the moment. Multi-step pipelines will be\\navailable soon.\\nWe’ve already created for you :\\n\\nA project\\nA repository GitHub linked to the project\\nAn environment setup in the project with datastore and workers\\n\\n\\nSet up the Python SDK\\uf0c1\\nThe Python SDK can be installed with\\npip from a terminal.\\npip install craft-ai-sdk\\n\\n\\nFor this use case, you also need NumPy if you don’t have already\\ninstalled it on your machine.\\npip install numpy\\n\\n\\n\\n\\nInitialization of Python SDK\\uf0c1\\nRun the following commands in a Python terminal to initialize the SDK:\\n\\nSet the value of CRAFT_AI_ENVIRONMENT_URL into a local environment\\nvariable with your environment URL (you should get it from the Environment page on the UI. Environnement URL is NOT the HTML address of the UI).\\nSet the value of CRAFT_AI_SDK_TOKEN into a local\\nenvironment variable (On the UI, click on your name in the top right corner and then go to the “Parameters” page. There you can generate and find your SDK token at the bottom. The SDK token is strictly personal. You must keep it to yourself.)\\nA good practice would be to create a script per environment that\\ncontains the setup of these 2 local environment variables. For example,\\nyou could create an .env-test-platform-craft-ai file.\\n\\n#!/bin/bash\\n\\nexport CRAFT_AI_ENVIRONMENT_URL=\"*your-env-URL*\"\\nexport CRAFT_AI_ACCESS_TOKEN=\"*your-token-acces*\"\\n\\n\\n\\n\\nHere, we use the source command to define the values of the\\nvariables written to the file.\\nsource .env-test-platform-craft-ai\\n\\n\\n\\nExecute the following Python code to set up SDK Python object.\\nimport os\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk(\\n    environment_url=os.environ.get(\"CRAFT_AI_ENVIRONMENT_URL\"),\\n    sdk_token=os.environ.get(\"CRAFT_AI_ACCESS_TOKEN\")\\n)\\n\\n\\n🎉 Well done! You’re ready to execute your first code on the platform!\\n\\n\\n\\n\\nWhat’s next ?\\uf0c1\\nNow that we have configured the platform, we can create our first\\nobjects and run a “Hello World” on the platform.\\nNext step: Part 1: Deploy a simple pipeline\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/Setup.html', 'title': 'Get Started — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content=\"\\n\\n\\n\\n\\nPart 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nGet Started\\nPart 1: Execute a simple pipeline\\n\\n\\n\\n\\n\\n\\n\\n\\nPart 1: Execute a simple pipeline\\uf0c1\\nThe main goal of the Craft AI platform is to allow to deploy easily your\\nmachine learning pipelines. In this first part, you will learn how you\\ncan execute some simple code on the platform.\\n\\nOverview of the first use case\\uf0c1\\nIn this part we will use the platform to build a simple “hello\\nworld” application by showing you how to execute a basic Python code\\nthat prints “Hello world” and displays the number of days until 2025.\\nYou will learn how to:\\n\\nPackage your application code into a step on the platform\\nEmbed it in a pipeline\\nExecute it on the platform\\nCheck the logs of the executions on the web interface\\n\\n\\n\\n\\nCreate a step with the SDK\\uf0c1\\nThe first thing to do to build an application on the Craft AI platform\\nis to create a step.\\nA Step is the equivalent of a Python function in the Craft AI platform.\\nLike a regular function, a step is defined by the inputs it\\ningests, the code it runs, and the outputs it returns. For this\\n“hello world” use case, we are focusing on the code part so we will\\nignore inputs and outputs for now.\\nA step is created from any function located in the source code on your\\nrepository, using the create_step() method of thesdk object.\\nIt is very important to understand that the platform can only create\\nsteps from the code present in your GitHub / Gitlab repository in the branch\\nspecified during setup. If you have some uncommitted changes, they won’t\\nbe taken into account at step creation.\\nWe’ve added this code to your repository in the get_started folder:\\nimport datetime\\n\\ndef helloWorld() -> None:\\n\\n    now = datetime.datetime.now()\\n    difference = datetime.datetime(2025, 1, 1) - now\\n\\n    print(f'Hello world ! Number of days to 2025 : {difference}')\\n\\n\\nOur helloworld function is located\\nin src/part-1-helloWorld.py so we can create our first step on the platform\\nas follow:\\nsdk.create_step(\\n    step_name='part-1-hello-world',\\n    function_path='src/part-1-helloWorld.py',\\n    function_name='helloWorld'\\n)\\n\\n\\nIts main arguments are:\\n\\nThe step_name is the name of the step that will be created. This\\nis the identifier you will use later to refer to this step.\\nThe function_path argument is the path of the Python module\\ncontaining the function that you want to execute for this step. This\\npath must be relative to the root of the git repository.\\nThe function_name argument is the name of the function that you\\nwant to execute for this step.\\n\\nThe above code should give you the following output:\\n>>> Please wait while step is being created. This may take a while...\\n>>> Steps creation succeeded\\n>>> {'name': 'part-1-hello-world'}\\n\\n\\nYou can view the list of steps that you created in the platform with the\\nlist_steps() function of the SDK.\\nstep_list = sdk.list_steps()\\nprint(step_list)\\n\\n\\n>>> [{'name': 'part-1-hello-world',\\n>>>   'created_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>>   'updated_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>>   'status': 'Ready',\\n>>>   'repository_branch': 'main',\\n>>>   'commit_id': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx',\\n>>>   'repository_url': 'git@github.com:xxxxx/xxxxx.git'}]\\n\\n\\nYou can see your step and its status of creation at Ready.\\nYou can also get the information of a specific step with the\\nget_step() function of the SDK.\\nstep_info = sdk.get_step('part-1-hello-world')\\nprint(step_info)\\n\\n\\n>>> {\\n>>>   'parameters': {\\n>>>     'step_name': 'part-1-hello-world',\\n>>>     'function_path': 'src/part-1-helloWorld.py',\\n>>>     'function_name': 'helloWorld',\\n>>>     'description': None,\\n>>>     'container_config': {\\n>>>       'repository_branch': 'main',\\n>>>       'repository_url': 'git@github.com:xxxxx/xxxxx.git',\\n>>>       'repository_deploy_key': 'xxx ... xxx',\\n>>>       'dockerfile_path': None\\n>>>     },\\n>>>     'inputs': [],\\n>>>     'outputs': []\\n>>>   },\\n>>>   'creation_info': {\\n>>>     'commit_id': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx',\\n>>>     'created_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>>     'updated_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>>     'status': 'Ready'\\n>>>   }\\n>>> }\\n\\n\\n🎉 Now your step has been created. You can now create your Pipeline (and\\nafter that, you’ll execute it on the platform).\\n\\n\\nCreate a pipeline with the SDK\\uf0c1\\n\\nThe step part-1-hello-world containing our helloWorld code is\\nnow created in the platform and ready to be used in a pipeline that\\nwe will then be executed.\\nA pipeline is a machine learning workflow, consisting of one or\\nmore steps, that can be easily deployed on the Craft AI platform.\\nThis way, you can create a full pipeline formed with a directed acyclic graph (DAG) by\\nspecifying the output of one step as the input of another step.\\nIn the future, it will be possible to assemble multiple steps into a complex machine\\nlearning pipeline. For now, the platform only allows single step\\npipelines.\\nTo create a pipeline consisting of the previous step, you must use the\\ncreate_pipeline() function of the SDK.\\nsdk.create_pipeline(\\n    pipeline_name='part-1-hello-world',\\n    step_name='part-1-hello-world',\\n)\\n\\n\\nThis function has two arguments:\\n\\nThe pipeline_name is the name of the pipeline you have just\\ncreated. As for the step_name you will then refer to the pipeline\\nusing this name\\nThe step_name is the name of the step used in the pipeline.\\n\\nAfter executing this function, you should see the following output :\\n>>> Pipeline creation succeeded\\n>>> {'pipeline_name': 'part-1-hello-world',\\n>>> 'created_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>> 'steps': ['part-1-hello-world'],\\n>>> 'open_inputs': [],\\n>>> 'open_outputs': []}\\n\\n\\n🎉 Now that our pipeline is created (around our step), we want to execute\\nit. To do this, we will run the pipeline with the sdk function, run_pipeline(), and it will execute the code contained in the step.\\n\\n\\nExecute your pipeline (run)\\uf0c1\\nYou can execute  a pipeline on the platform directly with the run_pipeline() function.\\nThis function has two arguments:\\n\\nThe name of the existing pipeline to execute (pipeline_name)\\nOptional (only if you have inputs): a dict of inputs to pass to the pipeline with input names as dict keys and corresponding values as dict values.\\n\\nsdk.run_pipeline(pipeline_name='part-1-hello-world')\\n\\n\\n>>> The pipeline execution may take a while, you can check its status and get information on the Executions page of the front-end.\\n>>> Its execution ID is 'part-1-hello-world-xxxxx'.\\n>>> Pipeline execution results retrieval succeeded\\n>>> Pipeline execution startup succeeded\\n\\n\\n🎉 Now, you have created a step for the helloWorld\\nfunction, included it in a pipeline and execute it on the platform! Our hello world\\napplication is built and ready to be executed again!\\n\\n\\nGet information about an execution\\uf0c1\\nNow, we have executed the pipeline.\\nThe return of the function allows us to see that the pipeline has been successfully executed, however\\nit does not give us the logs of the\\nexecution (we can receive outputs with the return of the run pipeline, but\\nwe did not put any here).\\nIt is possible to have the returns of the executions with the SDK, let’s\\nsee how it works.\\nOnce your pipeline is executed, you can now see the\\npipeline executions with the sdk.list_pipeline_executions()\\ncommand.\\nsdk.list_pipeline_executions(\\n    pipeline_name='part-1-hello-world'\\n)\\n\\n\\n>>> [{'execution_id': 'part-1-hello-world-XXXX',\\n>>> 'status': 'Succeeded',\\n>>> 'created_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>> 'end_date': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>> 'created_by': 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx',\\n>>> 'pipeline_name': 'part-1-hello-world',\\n>>> 'deployment_id': 'xxxxxxxx-xxxx-xxxx-xxx-xxxxxxxx',\\n>>> 'steps':\\n>>>     [{'name': 'part-1-hello-world',\\n>>>       'status': 'Succeeded',\\n>>>       'end_date': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>>       'start_date': 'xxxx-xx-xxTxx:xx:xx.xxxZ'}]}]\\n\\n\\nFurthermore, you can get the specific logs of an execution with the\\nsdk.get_pipeline_execution_logs() command. You will have to fill\\nin the execution ID, which can be found with the previous command. The\\nlogs are given to us line by line in JSON, however, we can display them\\nclearly with the command in the print() below. This way, we will\\nalso be able to see the error messages of the step code through this if\\nthe execution of the code encounters any.\\npipeline_executions = sdk.list_pipeline_executions(\\n    pipeline_name='part-1-hello-world'\\n)\\n\\nlogs = sdk.get_pipeline_execution_logs(\\n    pipeline_name='part-1-hello-world',\\n      execution_id=pipeline_executions[-1]['execution_id'] # [-1] to get the last execution\\n)\\n\\nprint('\\\\n'.join(log['message'] for log in logs))\\n\\n\\n>>> Please wait while logs are being downloaded. This may take a while…\\n>>> Hello world ! Number of days to 2024 : xxx\\n\\n\\nTo be able to find more easily the list of executions as well as the\\ninformation and associated logs, you can use the user interface, as\\nfollows:\\n\\nConnect to\\nhttps://mlops-platform.craft.ai\\nClick on your project:\\n\\n\\n\\n\\nClick on the Execution page and on “Select an execution”: this displays the list of\\nenvironments:\\n\\n\\n\\n\\nSelect your environment to get the list of runs and deployments:\\n\\n\\n\\n\\nFinally, click on a run name to get its executions:\\n\\n\\n\\n\\nYou have the “General” tab to get general information about your\\nexecution and the “Logs” tab where you can see and download the execution\\nlogs:\\n\\n\\n\\n\\n\\n🎉 You can now get your execution’s logs.\\n\\nWhat we have learned\\uf0c1\\nIn this part we learned how to easily build, deploy and use a simple\\napplication with the Craft AI platform with the following workflow:\\n\\nThese 3 main steps are the fundamental workflow to work with the\\nplatform and we will see them over and over throughout this tutorial.\\nNow that we know how to run our code on the platform, it is time to\\ncreate more complex steps to have a real ML use case.\\nNext step : Part 2: Execute a simple ML model\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nPart 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nGet Started\\nPart 2: Execute a simple ML model\\n\\n\\n\\n\\n\\n\\n\\n\\nPart 2: Execute a simple ML model\\uf0c1\\n\\nIntroduction\\uf0c1\\nThe previous part showed the main concepts of the platform and how to use the basics of the SDK.\\nWith what you already know you are able to execute really simple pipelines.\\nBut in order to build more realistic applications, using more complex code on your data with dependencies such\\nas Python libraries, it is needed to learn more advanced functionnalities and especially how to configure\\nthe execution context of a step and how to retrieve data stored on the platform.\\nThis page will present the same commands as the previous ones going through more available\\nfunctionalities offered by the platform, with a real Machine Learning use case.\\nWe will improve this Machine Learning application later in Part 3 and Part 4.\\nYou can find all the code used in this part and its structure here\\nPrerequisites\\n\\nPython 3.8 or higher is required to be installed on your\\ncomputer.\\nHave done the previous Part of this tutorial (Part 1: Execute a simple pipeline).\\n\\n\\n\\nFirst Machine Learning use case\\uf0c1\\nHere we will build a pipeline to train and store a simple ML model with the iris\\ndataset.\\n\\nOverview of the use case\\uf0c1\\nThe iris dataset describes four features (petal length, petal width,\\nsepal length, sepal width) from three different types of irises (Setosa,\\nVeriscolour, Virginica).\\n\\nThe goal of our application is to classify flower type based on the\\nprevious four features. In this part we will start our new use case\\nby retrieving the iris dataset from the Data Store (that we will introduce just below), building a pipeline to train a simple ML model on the\\ndataset and store it on the Data Store.\\n\\n\\n\\nStoring Data on the Platform\\uf0c1\\nThe Data Store is a file storage on which you can upload and download unlimited files\\nand organize them as you want using the SDK.\\nAll your steps can download and upload files from and to the Data Store.\\nPushing the iris dataset to the Data Store\\nIn our case the first thing we want to do is to upload the iris dataset to the Data Store.\\nYou can do so with the upload_data_store_object function from the SDK like so:\\nfrom io import BytesIO\\nfrom sklearn import datasets\\nimport pandas as pd\\n\\niris = datasets.load_iris(as_frame=True)\\niris_df = pd.concat([iris.data, iris.target], axis=1)\\n\\nfile_buffer = BytesIO(iris_df.to_parquet())\\nsdk.upload_data_store_object(\\n   filepath_or_buffer=file_buffer,\\n   object_path_in_datastore=\"get_started/dataset/iris.parquet\"\\n)\\n\\n\\nThe argument filepath_or_buffer can be a string or a file-like object.\\nIf a string, it is the path to the file to be uploaded,\\nif a file-like object you have to pass an IO object (something you don’t write to the disk\\nbut stay in the memory). Here we choose to use a BytesIO object.\\n\\n\\nThe code we want to execute\\uf0c1\\nWe will use the following code that trains a sklearn KNN classifier on\\nthe iris dataset from the Data Store and put the trained model on the Data Store.\\nimport joblib\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndef trainIris():\\n\\n   sdk = CraftAiSdk()\\n\\n   sdk.download_data_store_object(\\n      object_path_in_datastore=\"get_started/dataset/iris.parquet\",\\n      filepath_or_buffer=\"iris.parquet\",\\n   )\\n   dataset_df = pd.read_parquet(\"iris.parquet\")\\n\\n   X = dataset_df.loc[:, dataset_df.columns != \"target\"].values\\n   y = dataset_df.loc[:, \"target\"].values\\n\\n   np.random.seed(0)\\n   indices = np.random.permutation(len(X))\\n\\n   n_train_samples = int(0.8 * len(X))\\n   train_indices = indices[:n_train_samples]\\n   val_indices = indices[n_train_samples:]\\n\\n   X_train = X[train_indices]\\n   y_train = y[train_indices]\\n   X_val = X[val_indices]\\n   y_val = y[val_indices]\\n\\n   knn = KNeighborsClassifier()\\n   knn.fit(X_train, y_train)\\n\\n   mean_accuracy = knn.score(X_val, y_val)\\n   print(\"Mean accuracy:\", mean_accuracy)\\n\\n   joblib.dump(knn, \"iris_knn_model.joblib\")\\n\\n   sdk.upload_data_store_object(\\n      \"iris_knn_model.joblib\", \"get_started/models/iris_knn_model.joblib\"\\n   )\\n\\n\\n\\n\\nCleaning object\\uf0c1\\nBefore we really start building our new use case, we might want to\\nclean the platform from the objects we created in Part\\n1.\\nTo do this, we need to use the functions associated with each object,\\nhere the pipeline and the step.\\n\\nWarning\\nThese objects have dependencies on each other, we have\\nto delete them in a certain order. First the pipeline,\\nand then the step.\\n\\nsdk.delete_pipeline(pipeline_name=\"part-1-hello-world\")\\n\\nsdk.delete_step(step_name=\"part-1-hello-world\")\\n\\n\\n\\n\\n\\nIn depth step configuration\\uf0c1\\nIn the rest of this part we will follow the same workflow as in the\\nprevious one:\\n\\nNow it is time to use the create_step() method of craft-ai-sdk\\nobject to create step like before. This time though, we will see how we\\ncan define a bit more the step and its execution context. We are going\\nto focus on two parameters.\\n\\nPython libraries\\uf0c1\\nAs you might have noticed, the code above uses external Python libraries\\n(craft_ai_sdk, joblib, numpy, pandas and scikit learn). In the previous step we conveniently built an\\napplication that didn’t require any external dependency but this time if\\nwe want this code to work on the platform we have to inform it that this\\nstep requires some Python libraries to run properly.\\nTo do so we create a requirements.txt file,\\ncontaining the list of Python libraries used in our step function:\\ncraft_ai_sdk==xx.xx.xx\\njoblib==xx.xx.xx\\nnumpy==xx.xx.xx\\nscikit_learn==xx.xx.xx\\npandas==xx.xx.xx\\npyarrow==xx.xx.xx\\n\\n\\nIn this case we place it at the root of the repo, but you can put it wherever you want.\\n\\nWarning\\nAs for the code, the platform only sees what’s on your repository so don’t\\nforget to push your requirement file on your Git repository.\\n\\nYou can now specify the path by default of this file in the Libraries & Packages section of your project\\nsettings using the web interface and all the steps created in this project will have these libraries\\ninstalled and ready to be used.\\n\\n\\nIncluded folder\\uf0c1\\nWhen creating your step, you may not need to include all the files of\\nyour project repositories in your step. You can specify the file(s) and\\nfolder(s) to include from the GitHub / GitLab repository to prevent the step from\\naccessing all code available in the repository by default, using the\\nincluded_folders option.\\nOnce again you can set this option in your project settings page in the\\nweb interface in the Folder field.\\n🎉 Now all the steps created in this project will have the relevant\\nlibraries installed and only the necessary files will be included\\n\\n\\n\\n\\nCreate a step\\uf0c1\\nWhen you created your project in the platform (cf Part 0), you have set up different parameters\\n(like the repository URL, the deploy key or the Python version you are using) and we also set up new parameters in\\nthe previous section.\\nBy default, your step will apply those parameters during its creation (that is why you didn’t need to add any parameters\\nwhen creating your step in Part 1).\\nHowever, sometimes you want to define them only at the step level and keep the previous ones at the project level.\\nThis is the role of the create_step() function’s parameter container_config . You can pass as a dictionary the\\nset of parameters you want to use for the step creation. It allows you to be really specific in the execution\\ncontext of your step.\\n💡 For example, if you need to build a step embedding a function from\\nanother repository, you can specify the new Git repository URL and\\ndeploy key at the step level using the container_config parameter.\\nYour project parameters will remain unchanged.\\n\\nWarning\\nThe execution context of a step is non persistent.\\nIt means that as soon as the execution is done, everything written in\\nmemory and on disk during the execution will be deleted. Therefore, a\\nstep is not the right place to store data or any information. You should use\\nthe Data Store instead.\\n\\nHere we will specify the requirements_path and the\\nincluded_folders.\\nNote: You can specify them in the user interface at the project\\nlevel as well. Just remember that they will be used by default if you\\ndon’t specify others at the step level.\\nIn order for our requirements.txt file to be taken into account, we\\nmust also add it to the container_config parameter with the\\nrequirements_path\\nsdk.create_step(\\n    step_name=\"part-2-iristrain\",\\n    function_path=\"src/part-2-irisModel.py\",\\n    function_name=\"trainIris\",\\n    description=\"This function creates a classifier model for iris\",\\n    container_config = {\\n        \"requirements_path\" : \"requirements.txt\", #put here the path to the requirements.txt\\n        \"included_folders\" : [\"src\"]\\n    }\\n)\\n\\n\\nIt may also be useful to describe precisely the steps created to be able\\nto understand their purpose afterward. To do so, you can fill in the\\ndescription parameter during the step creation.\\nℹ️ To go further with step creation If you want to create a step\\nbased on the code of a repository not indicated in the project (or just\\non another branch), you can also specify it during the step creation.\\nYou can find all the function arguments you can modify here.\\n🎉 Now your step has been created. You can now create your Pipeline.\\nFrom here, we reproduce the same steps as before with the creation of\\nthe pipeline and we execute it.\\n\\n\\nCreate a pipeline\\uf0c1\\nCreate a pipeline with the create_pipeline() method of the SDK.\\nsdk.create_pipeline(\\n    pipeline_name=\"part-2-iristrain\",\\n        step_name=\"part-2-iristrain\"\\n)\\n\\n\\n\\n\\nExecute your Pipeline (run) and get the execution logs\\uf0c1\\nNow you can execute your pipeline as in Part\\n1.\\nsdk.run_pipeline(pipeline_name=\"part-2-iristrain\")\\n\\n\\nTo be able to find more easily the list of executions as the\\ninformation and associated logs, you can use the user interface, like in the previous part.\\nThe output is a list of iris categories :\\n>> [2 0 2 0 2 2 0 0 2 0 0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2\\n1 1 1 2 2 2 1 0 1 2 2 0 1 1 2 1 0 0 0 2 1 2 0]\\n\\n\\n🎉 You can now execute a more realistic Machine Learning pipeline.\\nNow that we can have more complex code in our steps and we know how to\\nparametrize the execution context of our steps, we would like to be able to\\ngive it input elements to vary the result and receive the result easily.\\nFor this, we can use the input/output feature offered by the platform.\\nNext step: Part 3: Execute with input and output\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nPart 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nGet Started\\nPart 3: Execute a ML use case with inputs and outputs\\n\\n\\n\\n\\n\\n\\n\\n\\nPart 3: Execute a ML use case with inputs and outputs\\uf0c1\\n\\nIntroduction\\uf0c1\\nIn Part 2, we have built and run our first ML pipeline to retrieve\\ndata from the data store, train a model and store it on the data store.\\nWhat if we want to use our pipeline to perform predictions with this\\ntrained model on new data ? Currently, we can not pass new data to our\\nmodel.\\n⇒ We need to add an Input to our pipeline.\\nMoreover, what if we want to provide these predictions to a final user?\\n⇒ We need to add an Output to our pipeline.\\nThis part will show you how to do this with the Craft AI platform:\\n\\nWe will first create the code of the predictIris() function\\nso that it can receive data and return predictions.\\nThen, we will see how to create a step, a pipeline and run it on the platform\\nwith input data and return the corresponding predictions\\nas an output.\\n\\nBy the end of this part, we will have built a runable pipeline that allows\\nto get the predictions of the iris species on new data with a\\nsimple execution:\\nPrerequisites\\n\\nPython 3.8 or higher is required to be installed on your\\ncomputer.\\nHave done the previous parts of this tutorial ( Part 0.\\nSetup,\\nPart 1: Execute a simple pipeline\\nand Part 2: Execute a simple ML model\\n).\\n\\n\\n\\nMachine Learning use case with I/O\\uf0c1\\nHere we will build a pipeline to retrieve the trained model stored in the last part\\nand make a prediction on new data.\\n\\nOverview of the use case\\uf0c1\\n\\n\\n\\nThe code we want to execute\\uf0c1\\nFirst we have to implement our code to compute predictions with a stored model on\\nthe data store on any (correctly prepared) data given as input instead of computing\\npredictions on a test set.\\nHence, our file src/part-3-iris-predict.py is as follows:\\nfrom io import BytesIO\\nfrom craft_ai_sdk import CraftAiSdk\\nimport joblib\\nimport pandas as pd\\n\\n\\ndef predictIris(input_data: dict, input_model_path:str):\\n\\n   sdk = CraftAiSdk()\\n\\n   f = BytesIO()\\n   sdk.download_data_store_object(input_model_path, f)\\n   model = joblib.load(f)\\n\\n   input_dataframe = pd.DataFrame.from_dict(input_data, orient=\"index\")\\n   predictions = model.predict(input_dataframe)\\n\\n   final_predictions = predictions.tolist()\\n\\n   return {\"predictions\": final_predictions}\\n\\n\\nIn this code:\\n\\nWe add the argument input_data. Here, we choose it to be a\\ndictionary like the one below:\\n{\\n    1: {\\n        \\'sepal length (cm)\\': 6.7,\\n      \\'sepal width (cm)\\': 3.3,\\n      \\'petal length (cm)\\': 5.7,\\n      \\'petal width (cm)\\': 2.1\\n    },\\n  2: {\\n      \\'sepal length (cm)\\': 4.5,\\n      \\'sepal width (cm)\\': 2.3,\\n      \\'petal length (cm)\\': 1.3,\\n      \\'petal width (cm)\\': 0.3\\n  },\\n}\\n\\n\\nIt contains the data on which we want to compute predictions.\\n\\nWe retrieve our trained model with the download_data_store_object() function of the sdk by passing the model path.\\nAt the end, we convert our input_data dictionary into a Pandas\\ndataframe, and we compute predictions with our trained model.\\nAs you can see, the function now returns a Python dict with one\\nfield called “predictions” that contains the predictions value. The\\nplatform only accepts step function with one return value of type\\n``dict``. Each item of this dict will be an output of the step and\\nthe key associated with each item will be the name of this output on\\nthe platform.\\nMoreover, you can see that we converted our result from a numpy\\nndarray to a list. That is because the values of the inputs and\\noutputs are restricted to native Python types such as int, float,\\nbool, string, list and dict with elements of those types. More\\nprecisely anything that is json-serializable. Later, the platform\\nmight handle more complex input and output types such as numpy array\\nor even pandas dataframe.\\n\\nDont forget to update your requirements.txt file,\\ncontaining the list of Python libraries used in our step function:\\njoblib==xx.xx.xx\\npandas==xx.xx.xx\\ncraft_ai_sdk==xx.xx.xx\\n\\n\\n\\nWarning\\nSince we added a new functiion, we must add and commit our changes with\\nGit and push them to GitHub so that the platform can take them into\\naccount!\\n\\n\\n\\n\\nStep creation with Input and Output\\uf0c1\\nNow, let’s create our step on the platform. Here, since we have inputs and an output, our step is the combination of three elements: an\\ninput, an output and the Python function above. We will first declare\\nthe inputs and the output. Then, we will use the function\\nsdk.create_step() as in Part\\n2\\nto create the whole step.\\n\\n\\nDeclare Input and Output of our new step\\uf0c1\\nTo manage inputs and outputs of a step, the platform requires you to\\ndeclare them using the Input and Output classes from the\\nSDK.\\nFor our Iris application, the inputs and outputs declaration would\\nlook like this:\\nfrom craft_ai_sdk.io import Input, Output\\n\\nprediction_input = Input(\\n   name=\"input_data\",\\n   data_type=\"json\"\\n)\\n\\nmodel_input = Input(\\n   name=\"input_model_path\",\\n   data_type=\"string\"\\n)\\n\\nprediction_output = Output(\\n   name=\"predictions\",\\n   data_type=\"json\"\\n)\\n\\n\\nBoth objects have two main attributes:\\n\\nThe name of the Input or Output\\n\\nFor the inputs it corresponds to the names of the arguments of\\nyour step’s function. In our case name=\"input_data\" and \"input_model_path\", as in\\nthe first line of function :\\ndef predictIris(input_data: dict, input_model_path:str):\\n\\n\\n\\nFor the output it must be a key in the dictionary returned by\\nyour step’s function. In our case, name=\"predictions\"as in\\nthe last line of function :\\nreturn {\"predictions\": final_predictions}\\n\\n\\n\\n\\n\\nThe data_type describing the type of data it can accept. It\\ncan be one of: string, number, boolean, json, array, file.\\n\\nFor the inputs we want a dictionary and a string as we specified, which\\ncorresponds to data_type=\"json\" and data_type=\"string\".\\nFor the output, we return a dictionary which corresponds to\\ndata_type=\"json\".\\n\\n\\n\\nNow, we have everything we need to create, as\\nbefore,\\nthe step and the pipeline corresponding to our\\npredictIris() function.\\n\\n\\nCreate step\\uf0c1\\nNow as in Part 2, it is time to create our step on the platform using\\nthe sdk.create_step() function, but this time we specify our inputs\\nand output:\\nsdk.create_step(\\n   step_name=\"part-3-irisio\",\\n   function_path=\"src/part-3-iris-predict.py\",\\n   function_name=\"predictIris\",\\n   description=\"This function retrieves the trained model and classifies the input data by returning the prediction.\",\\n   inputs=[prediction_input, model_input],\\n   outputs=[prediction_output],\\n   container_config={\\n     \"included_folders\": [\"src\"],\\n     \"requirements_path\": \"requirements.txt\",\\n   },\\n)\\n\\n\\nThis is exclatly like in part 2 except for two parameters :\\n\\ninputs containing the list of Input objects we declared above\\n(here, prediction_input and model_input).\\noutputs containing the list of Output objects we declared\\nabove (here, prediction_output).\\n\\nWhen step creation is finished, you obtain an output describing your\\nstep (including its inputs and outputs) as below:\\n>> Step \"part-3-irisio\" created\\n  Inputs:\\n    - input_data (json)\\n    - input_model_path (string)\\n  Outputs:\\n    - predictions (json)\\n>> Steps creation succeeded\\n>> {\\'name\\': \\'part-3-irisio\\',\\n \\'inputs\\': [{\\'name\\': \\'input_data\\', \\'data_type\\': \\'json\\'}, {\\'name\\': \\'input_model_path\\', \\'data_type\\': \\'string\\'}],\\n \\'outputs\\': [{\\'name\\': \\'predictions\\', \\'data_type\\': \\'json\\'}]}\\n\\n\\nNow that our step is created in the platform, we can embed it in a\\npipeline and run it.\\n\\n\\n\\nCreate and run your pipeline\\uf0c1\\n\\nCreate pipeline\\uf0c1\\nLet’s create our pipeline here with sdk.create_pipeline() as in\\nPart\\n2:\\nsdk.create_pipeline(\\n   pipeline_name=\"part-3-irisio\",\\n   step_name=\"part-3-irisio\",\\n)\\n\\n\\nYou quickly obtain this output, which describes the pipeline, its step\\nand its inputs and outputs:\\n>> Pipeline creation succeeded\\n>> {\\'pipeline_name\\': \\'part-3-irisio\\',\\n \\'created_at\\': \\'xxxx-xx-xxTxx:xx:xx.xxxZ\\',\\n \\'steps\\': [\\'part-3-irisio\\'],\\n \\'open_inputs\\': [{\\'input_name\\': \\'input_data\\',\\n   \\'step_name\\': \\'part-3-irisio\\',\\n   \\'data_type\\': \\'json\\'}, {\\'input_name\\': \\'input_model_path\\',\\n   \\'step_name\\': \\'part-3-irisio\\',\\n   \\'data_type\\': \\'string\\'}],\\n \\'open_outputs\\': [{\\'output_name\\': \\'predictions\\',\\n   \\'step_name\\': \\'part-3-irisio\\',\\n   \\'data_type\\': \\'json\\'}]}\\n\\n\\n🎉 You’ve created your first step & pipeline with inputs and\\noutputs!\\nLet’s run this pipeline.\\n\\n\\n\\nRun the pipeline with new input data\\uf0c1\\n\\nPrepare input data\\uf0c1\\nNow, our pipeline needs data as input (formatted as we said\\nabove ⬆️). Let’s prepare it, simply by choosing some of the rows of iris\\ndataset we did not use when training our model:\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn import datasets\\n\\nnp.random.seed(0)\\nindices = np.random.permutation(150)\\niris_X, iris_y = datasets.load_iris(return_X_y=True, as_frame=True)\\niris_X_test = iris_X.loc[indices[90:120],:]\\n\\nnew_data = iris_X_test.to_dict(orient=\"index\")\\n\\n\\nLet’s check the data we created:\\nprint(new_data)\\n\\n\\nWe get the following output:\\n>> 124: {\\'sepal length (cm)\\': 6.7,\\n\\'sepal width (cm)\\': 3.3,\\n\\'petal length (cm)\\': 5.7,\\n\\'petal width (cm)\\': 2.1\\n},\\n41: {\\'sepal length (cm)\\': 4.5\\n...\\n\\n\\nFinally, we need to encapsulate this dictionary in another one\\nwhose key is \"input_data\" (the name of the input of our step,\\ni.e.\\xa0the name of the argument of our step’s function).\\nWe define also the path to our trained model on the data store with the value\\nassociated to the key \"input_model_path\".\\ninputs = {\\n    \"input_data\": new_data,\\n    \"input_model_path\": \"get_started/models/iris_knn_model.joblib\"\\n}\\n\\n\\nIn particular, when your step has several inputs, this dictionary should\\nhave as many keys as the number of inputs the step have.\\n\\n\\nExecute the pipeline (RUN)\\uf0c1\\nFinally, we can execute our pipeline with the data we’ve just prepared by\\ncalling the run_pipeline() function almost as in Part 2 and passing our dictionary inputs\\nto the inputs arguments of the function:\\noutput_predictions = sdk.run_pipeline(\\n                        pipeline_name=\"part-3-irisio\",\\n                        inputs=inputs)\\n\\n\\nFinally, our output can be obtained like this:\\nprint(output_predictions[\"outputs\"][\\'predictions\\'])\\n\\n\\nThis gives the output we want (with the predictions!):\\n>> {\\'predictions\\': [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2]}\\n\\n\\nMoreover, you can check the logs on the\\nUI, by clicking on the Executions\\ntab of your environment, selecting your pipeline and choosing the last\\nexecution.\\n🎉 Congratulations! You have run a pipeline to which we can pass\\nnew data, the path to our trained model and get predictions.\\nNext step: Part 4: Deploy a ML use case with inputs and outputs\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nPart 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nGet Started\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\n\\n\\n\\n\\n\\n\\nPart 4: Deploy a ML use case with inputs and outputs\\uf0c1\\n\\nIntroduction\\uf0c1\\nIn Part 3, we have built and run our second ML pipeline to retrieve our trained model from the data store,\\nprovide some new data to it as input and retrieve the result as an output of our pipeline execution.\\nWhat if we want to let an external user execute our predict pipeline?\\nOr if we want to schedule the execution of the pipeline that trains our model periodically?\\n⇒ We need to deploy one pipeline via an endpoint and another one with a scheduled execution.\\nThis part will show you how to do this with the Craft AI platform:\\n\\nWe will first update the code of the predictIris() function\\nso that it can retrieve directly from the data store the trained model\\nand returns the predictions as a json to the user.\\nWe will also update the code of the trainIris() function\\nso that it re trains the model on a specific dataset (that could be often updated)\\nand uploads the trained model directly to the datastore.\\nThen, we will see how to create a step and a pipeline that we will deploy\\non the platform in two different ways, and that could be executed periodicly\\nand by a call.\\n\\nPrerequisites\\n\\nPython 3.8 or higher is required to be installed on your\\ncomputer.\\nHave done the previous parts of this tutorial ( Part 0.\\nSetup,\\nPart 1: Execute a simple pipeline,\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs).\\n\\n\\n\\nMachine Learning application with I/O\\uf0c1\\nHere we will build an application based on what we did on the last part. We will expose our service\\nto external users and schedule periodic executions.\\n\\nOverview of the use case\\uf0c1\\n\\nTo get the predictions via and endpoint:\\n\\n\\n\\nTo retrain the model periodicly (we will focus on this case later):\\n\\n\\n\\n\\nThe code we want to execute\\uf0c1\\nWe will first focus on the construction of the endpoint the final user will be able to target.\\nFirst we have to update our code to retrieve directly the model from the data store\\nwithout any call to the sdk in the code and to return a file on the data store with the predictions inside.\\nHence, our file src/part-4-iris-predict.py is as follows:\\nimport joblib\\nimport pandas as pd\\nimport json\\n\\ndef predictIris(input_data: dict, input_model:dict):\\n\\n   model = joblib.load(input_model[\\'path\\'])\\n\\n   input_dataframe = pd.DataFrame.from_dict(input_data, orient=\"index\")\\n   predictions = model.predict(input_dataframe)\\n\\n   return {\"predictions\": predictions.tolist()}\\n\\n\\nWhat changed are only how we get the trained model.\\nmodel = joblib.load(input_model[\\'path\\'])\\n\\n\\ninput_model is a dictionary in which the key path refers to the file’s path where is located the file\\non the step environnement.\\nThis input is a file data type.\\nDon’t forget to update your requirements.txt file,\\ncontaining the list of Python libraries used in our step function:\\njoblib==xx.xx.xx\\npandas==xx.xx.xx\\n\\n\\n\\nWarning\\nAs for the code, the platform only sees what’s on your repository so don’t\\nforget to push your requirements file on your Git repository.\\n\\n\\n\\n\\nStep creation with Input and Output\\uf0c1\\nAs we did in part 3, we will first declare\\nthe inputs and the output. Then, we will use the function\\nsdk.create_step() to create the whole step.\\n\\n\\nDeclare Input and Output of our new step\\uf0c1\\nThe only difference now is the data type we will assign\\nto input_model.\\nThis is now a file that we want to retrieve from the data store.\\nTo do so, we define the inputs and output like below:\\nfrom craft_ai_sdk.io import Input, Output\\n\\nprediction_input = Input(\\n   name=\"input_data\",\\n   data_type=\"json\"\\n)\\n\\nmodel_input = Input(\\n   name=\"input_model\",\\n   data_type=\"file\"\\n)\\n\\nprediction_output = Output(\\n   name=\"predictions\",\\n   data_type=\"json\"\\n)\\n\\n\\nWe have just seen the code of the step has been adapted to handle file objects.\\nNow, we have everything we need to create, as\\nbefore,\\nthe step and the pipeline corresponding to our\\npredictIris() function.\\n\\n\\nCreate your step\\uf0c1\\nNow as in Part 3, it is time to create our step on the platform using\\nthe sdk.create_step() function, with our inputs\\nand output:\\nsdk.create_step(\\n   step_name=\"part-4-iris-deployment\",\\n   function_path=\"src/part-4-iris-predict.py\",\\n   function_name=\"predictIris\",\\n   description=\"This function retrieves the trained model and classifies the input data by returning the prediction.\",\\n   inputs=[prediction_input, model_input],\\n   outputs=[prediction_output],\\n   container_config={\\n     \"included_folders\": [\"src\"],\\n     \"requirements_path\": \"requirements.txt\",\\n   },\\n)\\n\\n\\nWhen the step creation is finished, you obtain an output describing your\\nstep (including its inputs and outputs) as below:\\n>> Step \"part-4-iris-deployment\" created\\n  Inputs:\\n    - input_data (json)\\n    - input_model (file)\\n  Outputs:\\n    - predictions (json)\\n>> Steps creation succeeded\\n>> {\\'name\\': \\'part-4-iris-deployment\\',\\n \\'inputs\\': [{\\'name\\': \\'input_data\\', \\'data_type\\': \\'json\\'}, {\\'name\\': \\'input_model\\', \\'data_type\\': \\'file\\'}],\\n \\'outputs\\': [{\\'name\\': \\'predictions\\', \\'data_type\\': \\'json\\'}]}\\n\\n\\nNow that our step is created in the platform, we can embed it in a\\npipeline and deploy it.\\n\\n\\nCreate your pipeline\\uf0c1\\nLet’s create our pipeline here with sdk.create_pipeline() as in\\nPart\\n3:\\nsdk.create_pipeline(\\n   pipeline_name=\"part-4-iris-deployment\",\\n   step_name=\"part-4-iris-deployment\",\\n)\\n\\n\\nYou quickly obtain this output, which describes the pipeline, its step\\nand its inputs and outputs:\\n>> Pipeline creation succeeded\\n>> {\\'pipeline_name\\': \\'part-4-iris-deployment\\',\\n \\'created_at\\': \\'xxxx-xx-xxTxx:xx:xx.xxxZ\\',\\n \\'steps\\': [\\'part-4-iris-deployment\\'],\\n \\'open_inputs\\': [{\\'input_name\\': \\'input_data\\',\\n   \\'step_name\\': \\'part-4-iris-deployment\\',\\n   \\'data_type\\': \\'json\\'}, {\\'input_name\\': \\'input_model\\',\\n   \\'step_name\\': \\'part-4-iris-deployment\\',\\n   \\'data_type\\': \\'file\\'}],\\n \\'open_outputs\\': [{\\'output_name\\': \\'predictions\\',\\n   \\'step_name\\': \\'part-4-iris-deployment\\',\\n   \\'data_type\\': \\'json\\'}]}\\n\\n\\n🎉 You’ve created your second step & pipeline with inputs and\\noutput!\\n\\n\\n\\nCreate your deployments with input and output mappings\\uf0c1\\nHere, we want to be able to execute the pipeline, either by launching the execution with an url link or\\nat a certain time, but not by a run anymore.\\nLet’s try the first case.\\nWe want the user to be able to:\\n\\nsend the input data directly to the application via an url link\\nretrieve the results directly from the endpoint\\n\\nWe want also to specify the path to the stored model on the data store,\\nso that the service will take this model directly from the data store.\\nThe user won’t be the one selecting the model used, it’s only on the technical side.\\n\\nCreate the endpoint with IO mappings\\uf0c1\\nAn endpoint is a publicly accessible URL that launches the execution of the Pipeline.\\nWithout the platform, you would need to write an api with a library like Flask, Fast API or Django and deploy it on a server that you would have to maintain.\\n\\n\\n\\nIO Mappings\\uf0c1\\nWhen you start a new deployment, the data flow has to be configured with a mapping, that you can create with the sdk.\\nFor our endpoint, we have to define the IO mappings defined on the schema above, like this:\\ninputs_mapping_endpoint = [\\n   InputSource(\\n      step_input_name=\"input_model\",\\n      datastore_path=\"get_started/models/iris_knn_model.joblib\"\\n      ),\\n   InputSource(\\n      step_input_name=\"input_data\",\\n      endpoint_input_name=\"input_data\"\\n      )\\n]\\n\\noutput_mapping_endpoint = [\\n   OutputDestination(\\n      step_output_name=\"predictions\",\\n      endpoint_output_name=\"iris_type\")\\n]\\n\\n\\n\\n\\nCreate the endpoint\\uf0c1\\nWith the platform you can create an endpoint with a simple call to the sdk.create_deployment() function of the SDK,\\nby choosing the endpoint for the argument execution_rule.\\nYou also have to specify a deployment_name, used to refer to the created endpoint and that is further used in its URL.\\nendpoint = sdk.create_deployment(\\n   execution_rule=\"endpoint\",\\n   pipeline_name=\"part-4-iris-deployment\",\\n   deployment_name=\"part-4-iris-endpoint\",\\n   inputs_mapping=inputs_mapping_endpoint,\\n   outputs_mapping=output_mapping_endpoint\\n)\\n\\n\\n\\n\\nTarget the endpoint\\uf0c1\\nPrepare the input data\\nNow, our endpoint needs data as input, like we did for last part:\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn import datasets\\n\\nnp.random.seed(0)\\nindices = np.random.permutation(150)\\niris_X, iris_y = datasets.load_iris(return_X_y=True, as_frame=True)\\niris_X_test = iris_X.loc[indices[90:120],:]\\n\\nnew_data = iris_X_test.to_dict(orient=\"index\")\\n\\n\\nWe need to encapsulate this dictionary in another one\\nwhose key is \"input_data\" (the name of the input of our step,\\ni.e.\\xa0the name of the argument of our step’s function).\\nWe don’t need to define the path to our trained model because it is already defined with the output mapping we have just done.\\ninputs = {\\n    \"input_data\": new_data\\n}\\n\\n\\nCall the endpoint with the input data\\nendpoint_url = sdk.base_environment_url  + \"/endpoints/\" + endpoint[\"name\"]\\nendpoint_token = endpoint[\"endpoint_token\"]\\nrequest = requests.post(endpoint_url, headers={\"Authorization\": f\"EndpointToken {endpoint_token}\"}, json=inputs)\\nrequest.json()\\n\\n\\nThe HTTP code 200 indicates that the request has been taken into account. In case of an error, we can expect an error code starting with 4XX or 5XX.\\nIt is a way to execute your deployment. But, obviously, you can execute it in any other way (curl command in bash, Postman…).\\n\\nWarning\\nAs the request is based on the POST method, note that you can’t directly target your endpoint and recieve the output by entering it in your web navigator.\\n\\nLet’s check we can get the predictions as output of the endpoint:\\nprint(request.json()[\\'outputs\\'][\\'iris_type\\'])\\n\\n\\nMoreover, you can check the logs on the\\nUI, by clicking on the Executions\\ntab of your environment, selecting your pipeline and choosing the last\\nexecution.\\n🎉 You’ve created your first deployment and you’ve just called it!\\n\\n\\nRetrain the model periodically\\uf0c1\\nLet’s imagine that our dataset is frequently updated, for instance we get new labeled\\niris data every day. In this case we might want to retrain our model by triggering our\\ntraining pipeline part4-iris-train every day.\\nThe platform can do this automatically using the periodic execution rule in our deployment.\\nA periodic execution rule allows to schedule a pipeline execution at a certain time.\\nFor example, every Monday at a certain time, every month, every 5 minutes etc.\\nThe inputs and output have to be defined, with a constant value or a data store mappings.\\nFirst we will update our trainIris function so that it produces a file output containing our model,\\nthat we will then map to the datastore.\\nYou can check the entire updated version of this function in src/part-4-iris-predict.py.\\nThe only change is done at the return of the function:\\n.. code:: python\\n\\nreturn {“model”: {“path”: “iris_knn_model.joblib”}}\\n\\nWe can then create the step and pipeline as we are used to.\\ntrain_output = Output(\\n   name=\"model\",\\n   data_type=\"file\"\\n)\\n\\nsdk.create_step(\\n   step_name=\"part-4-iristrain\",\\n   function_path=\"src/part-4-iris-predict.py\",\\n   function_name=\"trainIris\",\\n   outputs=[train_output],\\n)\\n\\nsdk.create_pipeline(\\n   pipeline_name=\"part-4-iristrain\",\\n   step_name=\"part-4-iristrain\"\\n)\\n\\n\\nNow let’s create a deployment that executes our pipeline every 5 minutes. In our case, we will map the prediction output\\n(which is our only I/O) to the datastore on the same path that is used in the pediction endpoint deployment.\\nThis way our prediction pipeline will automatically use the latest version of our model for predictions.\\nNote that the schedule argument takes the CRON syntax (examples here: https://crontab.guru/).\\n\\nAdapt IO mapping\\nLet’s create a deployment that will schedule our pipeline to be executed every 5 minutes with the same\\nIO mappings as in the endpoint, except that the new data is a constant input and not something we define during the execution.\\noutput_mapping_periodic = OutputDestination(\\n   step_output_name=\"model\",\\n   datastore_path=\"get_started/models/iris_knn_model.joblib\"\\n)\\n\\n\\nCreate periodic deployment\\nAnd now, below is how we create a deployment that will schedule our pipeline execution:\\nperiodic = sdk.create_deployment(\\n   execution_rule=\"periodic\",\\n   pipeline_name=\"part-4-iristrain\",\\n   deployment_name=\"part-4-iristrain\",\\n   schedule=\"*/5 * * * *\",\\n   outputs_mapping=[output_mapping_periodic],\\n\\n)\\n\\n\\nOur training pipeline will now be executed every 5 minutes, updating our model with the potential new data.\\nThe predict pipeline will then use this updated model automatically.\\nYou can check that you actually have a new execution every 5 minutes using the sdk or via the web interface.\\n🎉 Congrats! You’ve created your second deployment and planned it to run every 5 minutes!\\n\\n\\n\\nConclusion\\uf0c1\\n🎉 After this Get Started, you have learned how to use the basic functionalities of the platform!\\nYou know now the entire workflow to create a pipeline and deploy it.\\n\\nYou are now able to:\\n\\nDeploy your code through a pipeline in a few lines of code, run it whenever you want and have the logs to analyze the execution.\\nUse the Data Store on the platform to upload and download files, models, images, etc.\\nExecute your pipeline via an endpoint that is accessible from outside with a secured token, or via a periodic execution.\\nMake your inputs flexible: set constant values to avoid users to fill in, let users enter inputs values via the endpoint directly, or use the data store to retrieve or put objects.\\n\\n\\nNote\\n🎉 If you want to go further\\nOne concept has not been explained to you: the metrics.\\nIf you want to go further and discover this feature, you can read the associated documentation.\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nUser workflow — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nUser workflow\\n\\n\\n\\n\\n\\n\\n\\n\\nUser workflow\\uf0c1\\nWelcome to the comprehensive documentation on the user workflow for accessing and deploying\\nyour applications on our platform. This guide will walk you through the essential steps\\nrequired to connect to the platform, access your data, and deploy your models effectively.\\nWhether you are new to the platform or an experienced user, this documentation will provide\\nyou with a step-by-step approach to make the most of our services.\\n\\nUser workflow\\n\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/introduction.html', 'title': 'User workflow — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nUser workflow — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nUser workflow\\n\\n\\n\\n\\n\\n\\n\\n\\nUser workflow\\uf0c1\\nWelcome to the comprehensive documentation on the user workflow for accessing and deploying\\nyour applications on our platform. This guide will walk you through the essential steps\\nrequired to connect to the platform, access your data, and deploy your models effectively.\\nWhether you are new to the platform or an experienced user, this documentation will provide\\nyou with a step-by-step approach to make the most of our services.\\n\\nUser workflow\\n\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/introduction.html', 'title': 'User workflow — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nAccessing your data — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nUser workflow\\nAccessing your data\\n\\n\\n\\n\\n\\n\\n\\n\\nAccessing your data\\uf0c1\\nWhen executing your code within Pipelines on the Craft AI Platform, one crucial aspect is the ability to retrieve and store data. In this context, we will explore two approaches to acquiring data during our Pipeline Executions:\\n\\nUsing the Data Store provided by the Craft AI Platform, which offers a convenient way to store and retrieve objects on the environment.\\nConnecting to external data sources like you are used to. By accessing your own organization database, cloud external database, some open source data, data available via FTP or some data storage such as the classic ones offered by AWS, Azure, or Google Cloud Platform, we can expand the range of data available for our Pipeline Executions.\\n\\nBy combining the capabilities of both the Data Store and external data sources, we can ensure a reliable and efficient data retrieval system for our executions. We will take a closer look at the techniques and practices of retrieving data for pipeline executions.\\n\\nWarning\\nBe aware that when an execution is launched on the platform, what is on its execution context is not persistent (i.e. it does not remain after the execution), all data kept in memory or on the disk is removed at the end of the execution. It is possible to read, write and manipulate data during the execution, but everything in the execution context at the end of the execution is deleted.\\nThis allows you to have an identical and stable execution context for each run, while avoiding needlessly saturating the disk with your executions.\\nTo ensure the persistence of your data, you can use data sources: It can be the Data Store, your own database or external data storage.\\n\\n\\nSummary\\uf0c1\\n\\nHow to store data on the Data Store\\nHow to retrieve data from the Data Store\\nHow to access an external Data Source\\n\\n\\n\\nHow to store data on the Data Store\\uf0c1\\nHere we will see how to store files created within a step.\\nTo do this, we will see 2 methods:\\n\\nWith the dedicated SDK function upload_data_store_object()\\nAdvantages:\\n\\nMore flexibility at the Data Store path level. This method allows you to upload files to any location on the Data Store, as you can easily change the file path by providing a different input to the code.\\n\\nDrawbacks:\\n\\nNeed to initialize the SDK in the step.\\nNo tracking (the file path values given as inputs are not stored).\\n\\n\\nWith an Output mapping to the Data Store\\nAdvantages:\\n\\nNo need to initialize the SDK on the step.\\ntracking of the file used in this execution.\\n\\nDrawbacks:\\n\\nNeed to define the Storage location on the Dataupload_data_store_object() Store before creating the deployment and the path can’t be modified afterwards.\\nOnly one possible storage location.\\n\\n\\n\\n\\nWith the dedicated SDK function upload_data_store_object()\\uf0c1\\nYou can also access the Data Store directly from the step code by using the SDK function upload_data_store_object().\\n\\nWarning\\n⚠️ Note that you need to import the craft_ai_sdk library at the beginning of the code, and have it in your requirements.txt.\\n\\nThe SDK connection is initialized without token or environment URL, since the step will already be executed in the environment.\\nExample\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndef preprocess_data():\\n\\n    # SDK initialization\\n    sdk = CraftAiSdk()\\n\\n    # Using an existing function get_data() to get raw data\\n    df_data = get_data()\\n    # Using an existing function preprocess_data() to preprocess\\n    # the previously retrieved data\\n    df_preprocessed_data = preprocess_data(df_data)\\n    # The existing function create_csv()  writes the dataframe \\n    # df_preprocessed_data as a csv in path_to_preprocessed_data\\n    create_csv(df_preprocessed_data, path_to_preprocessed_data)\\n    \\n    # Upload into datastore with the SDK function upload_data_store_object()\\n    sdk.upload_data_store_object(\\n        filepath_or_buffer=path_to_preprocessed_data, \\n        object_path_in_datastore=\"data/preprocessed/data_preprocessed.txt\"\\n    )\\n\\n\\nThen, simply create the step and the pipeline to execute this code.\\n\\nWith an Output mapping to the Data Store\\uf0c1\\nHere we will see how to store files that were created within a step to the data store.\\nTo do so, we have a few points to follow:\\n\\nAdapt the code that will be executed in the step, especially by specifying the path on which the file is accessible on the step.\\n\\ndef yourFunction() :\\n    ...\\n    return {\"outputfile\" : \\n        {\"path\": **path of the file on the step**}\\n    }\\n\\n\\n\\nBefore creating the step, define the output of the step with a data_type as file, and create the step and the pipeline as we are used to.\\n\\nfrom craft_ai_sdk.io import Output\\n\\nstep_output = Output(\\n    name=step_output_name,\\n    data_type=\"file\", \\n)\\n\\nsdk.create_step(\\n    function_path=**the path to your function**,\\n    function_name=\"your_function\", \\n    step_name=**your step name**,\\n    outputs=[step_output]\\n)\\n\\nsdk.create_pipeline(pipeline_name=**your pipeline name**, \\n\\tstep_name=**your step name**)\\n\\n\\n\\nDefine the output mapping (with the sdk object OutputDestination) that will be used for the execution of your pipeline, in this case, a datastore_path is needed to map the output of the step to a specific path on the Data Store.\\n\\nfrom craft_ai_sdk.io import OutputDestination\\n\\noutput_mapping = OutputDestination(\\n\\tstep_output_name=step_output_name,\\n\\tdatastore_path=**path on which we want to store the file**,\\n)\\n\\nsdk.run_pipeline(\\n\\tpipeline_name=**your pipeline name**,\\n\\toutputs_mapping=[output_mapping]\\n)\\n\\n\\nExample\\nIn this part, we will create a deployment with an endpoint execution rule that returns 2 files, 1 file with text in .txt format and another with a fake confusion matrix in format .csv.\\nFirst, we write the code to create the 2 files.To be able to pass the files to the Data Store, we specify the paths of the 2 files on the step.\\n\\n\\n    📖 Note the step execution context is an isolated container, and it\\'s the platform that will then copy the files from the execution context to the Data Store (thanks to the output we’ve created).\\n\\nDon’t forget to indicate the dependencies into requirements.txt.\\nimport numpy as np\\nimport pandas as pd\\n\\ndef createFiles() :\\n\\n    # Define file into local step contenaire \\n    path_text = \"file_text_output.txt\"\\n    path_matrix = \"confusion_matrix.csv\"\\n\\n    # Create a fake confusion matrix into .csv file\\n    confusion_matrix = np.array([[100, 20, 5],\\n                                [30, 150, 10],\\n                                [10, 5, 200]])\\n    class_labels = [\\'Class A\\', \\'Class B\\', \\'Class C\\'] # Define the class labels\\n    df = pd.DataFrame(confusion_matrix, index=class_labels, columns=class_labels) # Create a DataFrame from the confusion matrix\\n    df.to_csv(path_matrix, index=True, header=True) # Save the DataFrame as a CSV file\\n\\n    # Create .txt file\\n    text_file = open(path_text, \\'wb\\')  # Open the file in binary mode\\n    text_file.write(\"Result of step send in file output :) \".encode(\\'utf-8\\'))  # Encode the string to bytes\\n    text_file.close()\\n\\n    # Return the path of the file in the container of the current step execution.\\n    fileOjb = {\\n        \"txtFile\" : {\"path\": path_text}, \\n        \"csvFile\" : {\"path\": path_matrix}\\n    }\\n    return fileOjb\\n\\n\\n\\nWarning\\n⚠️ Remember to push step code to a GitHub repository defined in information project into Craft AI platform.\\n\\nAfter the initialization of SDK connection, we can create the 2 outputs and then the step.\\nFor this step, we assume that all the information is already specified in the project settings (language and repository information).\\n# Output creation\\nstep_output_txt = Output(\\n    name=\"txtFile\",\\n    data_type=\"file\", \\n)\\n\\nstep_output_csv = Output(\\n    name=\"csvFile\",\\n    data_type=\"file\", \\n)\\n\\n# Step creation with output (we supose repository is setup in info project)\\nsdk.create_step(\\n    function_path=\"src/createFiles.py\",\\n    function_name=\"createFiles\", \\n    step_name=\"doc-2o-datastore-step\",\\n    outputs=[step_output_txt, step_output_csv]\\n)\\n\\n\\nNow, we can create the pipeline.\\nsdk.create_pipeline(pipeline_name=\"doc-2o-datastore-pipeline\", \\n    step_name=\"doc-2o-datastore-step\")\\n\\n\\nTo create an accessible endpoint, we need to create a deployment with two output mappings to the Data Store we created earlier.\\nLet’s pretend we want to store the files at \"docExample/resultText.txt” and \"docExample/resultMatrix.csv\" on the Data Store.\\nendpoint_output_txt = OutputDestination(\\n    step_output_name=\"txtFile\",\\n    datastore_path=\"docExample/resultText.txt\",\\n)\\n\\nendpoint_output_csv = OutputDestination(\\n    step_output_name=\"csvFile\",\\n    datastore_path=\"docExample/resultMatrix.csv\",\\n)\\n\\nendpoint = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"doc-2o-datastore-pipeline\",\\n    deployment_name=\"doc-2o-datastore-dep\",\\n    outputs_mapping=[output_mapping_txt, output_mapping_csv]\\n)\\n\\n\\nAfter that, we can trigger the endpoint with the Python code below (we can use any other tool like postman, curl …).\\nimport requests \\n\\nendpoint_url = **your-url-env**+\"/endpoints/doc-2o-datastore-dep\"\\nheaders = {\"Authorization\": \"EndpointToken \"+endpoint[\"endpoint_token\"]}\\n\\nresponse = requests.post(endpoint_url, headers=headers)\\n\\nprint (response.status_code, response.json())\\n\\n\\n\\n\\n\\n\\nHow to retrieve data from the Data Store\\uf0c1\\nHere, we will see how to retrieve files already stored on the Data Store within a step.\\nTo do this, we will see 2 methods:\\n\\nWith the dedicated SDK function download_data_store_object()\\nAdvantages:\\n\\nMore flexibility at the Data Store path level. This method allows you to download files from any location on the Data Store, as you can easily change the file path by providing a different input to the code.\\n\\nDrawbacks:\\n\\nNeed to initialize the SDK in the step.\\nNo tracking (the file path values given as inputs are not stored).\\n\\n\\nWith an Input mapping to the Data Store\\nAdvantages:\\n\\nNo need to initialize the SDK on the step.\\ntracking of the file used in this execution.\\n\\nDrawbacks:\\n\\nNeed to define the Storage location on the Data Store before creating the deployment and the path can’t be modified afterwards.\\nOnly one possible storage location.\\n\\n\\n\\n\\nWith the dedicated SDK function download_data_store_object()\\uf0c1\\nYou can access the Data Store directly from the step code by using the SDK function download_data_store_object().\\n\\n\\n✅ Note that you need to import the craft_ai_sdk library at the beginning of the code, and have it in your requirements.txt.\\n\\nThe connection is initialized without token or environment URL, since the Step will already be executed in the environment.\\nExample\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndef retrievePredictions(id_prediction: int):\\n\\n    # SDK initialization\\n    sdk = CraftAiSdk()  \\n\\n    # Download the file containing the predictions \\n    # of the id_prediction at the local path \"predictions.txt\"\\n    sdk.download_data_store_object(\\n        object_path_in_datastore=f\"data/predictions_{id_prediction}.csv\", \\n        filepath_or_buffer=\"predictions.txt\"\\n    )\\n\\n        # Open and print the content of the file now stored locally\\n    with open(\"predictions.txt\") as f:\\n        contents = f.readlines()\\n        print (contents)\\n\\n\\nThen, simply create the step and the pipeline to execute this code.\\n\\n\\nWith an Input mapping to the Data Store\\uf0c1\\nWe will need to define the Input as a file for the step, the input mapping that connects the Data Store to the step and therefore the code embedded within the step to correctly read the file. Let’s start with the latter.\\nTo do so, we have a few points to follow:\\n\\nAdapt the code to access and read your file. Indeed, the input that will be passed to the step will have a predefined form as a dictionary with path as key and the file path as a value. You thus need to access it the same way you retrieve the value of a dictionary.\\nThe file will be downloaded in the execution environment before the step is executed. You can then use the file as you would use any other file in the execution environment.\\nHere, we have a function readFile that aims to read the input file and print its content.\\ndef read_file (entryFile: dict) :\\n\\n    # Access the file with its local path (entryFile[\"path\"]) on the step\\n    with open(entryFile[\"path\"]) as f:\\n        contents = f.readlines()\\n        print (contents)\\n\\n\\n\\nWarning\\n⚠️ One the code is updated, remember to push step code to a GitHub repository defined in information project into Craft AI platform.\\n\\n\\nBefore creating the step, define the input of the step with a data_type as file, and create the step and pipeline as we are used to.\\n\\n\\n    ✅ We assume that all the information is already specified in the project settings (language, repository and branch information).\\n\\n\\n\\nfrom craft_ai_sdk.io import Input\\n\\n# Define the input of the step \\nstep_input = Input(\\n    name=\"entryFile\",\\n    data_type=\"file\",    \\n)\\n\\n# Create the step \\nsdk.create_step(\\n    function_path=\"src/read_file.py\",\\n    function_name=\"read_file\", \\n    step_name=\"file-datastore-step\",\\n    inputs=[step_input]\\n)\\n\\n# Create the pipeline\\nsdk.create_pipeline(pipeline_name=\"file-datastore-pipeline\", \\n    step_name=\"file-datastore-step\")\\n\\n\\nOnce the pipeline is created, we define the correct input mapping and run the pipeline with it.\\n\\nDefine the input mapping (with the SDK object InputSource) that will be used for the execution of your pipeline, in this case, a datastore_path is needed to map the input of the step to the file on the Data Store.\\nLet’s pretend the file we want to retrieve is stored at myFolder/text.txt on the Data Store.\\nfrom craft_ai_sdk.io import InputSource\\n\\ninput_mapping = InputSource(\\n    step_input_name=\"entryFile\"\\n    datastore_path=\"myFolder/text.txt\", # Path of the output file in the datastore\\n)\\n\\n# Run pipeline using mapping defined with InputSource object\\nsdk.run_pipeline(\\n\\tpipeline_name=\"file-datastore-pipeline\", \\n\\tinputs_mapping=[input_mapping]\\n)\\n\\n\\nYou can check the logs for your file content.\\npipeline_executions = sdk.list_pipeline_executions(\\n    pipeline_name=\"file-datastore-pipeline\"\\n)\\n\\nlogs = sdk.get_pipeline_execution_logs(\\n    pipeline_name=pipeline_name, \\n    execution_id=pipeline_executions[0][\\'execution_id\\']\\n)\\n\\nprint(\\'\\\\n\\'.join(log[\"message\"] for log in logs))\\n\\n\\n\\n\\n\\n\\n\\nHow to access an external Data Source\\uf0c1\\nThe connection with an external data source (database or data storage) involves the following steps:\\n\\nUsing the same code you would use without the Craft AI platform to access your data storage. You only have to encapsulate your code within a step and a pipeline, like any code you would like to execute on the Craft AI platform. You also may have to adapt the inputs and outputs of your main function to respect the Craft AI formats.\\nEmbedding your credentials on your platform environment to use them in your step code to access the database.\\n\\n  🌟 To securely use credentials, a common good practice is to define **environment variables** to store the credentials safely. If you want to do so, the SDK offers you an easy solution: the `create_or_update_environment_variable()` function.\\nsdk.create_or_update_environment_variable(\"USERNAME\", \"username_db_1\")\\n\\n\\nThen, when you need the credentials, you access it with the following command :\\nimport os\\nimport dotenv\\n\\ndotenv.load_dotenv()\\n\\nusername = os.environ[\"USERNAME\"]\\n\\n\\n\\n\\n\\n\\nFrom an external Database\\uf0c1\\nMoreover, if you try to access to an external Database, we may have to whitelist the IP of your Craft AI platform environment(s) if necessary in your data source configuration. The environments IP are available on the page dedicated to your project environments.\\n\\nHere is an example of the few points to do in order to access a specific external database directly from our Craft AI environment platform by using the usual credentials and be able to filter on some data on the database:\\n\\n\\n   🛡️ If needed, add the Craft AI environment platform URL to the whitelist of the external database you want to access.\\n\\n\\nFirst, we embed the credentials we usually use to access to the database in our environment by setting them as environment variables.\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_HOST\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_ADMIN\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_PASS\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_NAME\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_PORT\",\\n    environment_variable_value=\"xxx\")\\n\\n\\n\\nSecondly, we encapsulate the code we would use without the Craft AI platform in a function, here filter_data_from_database(), that will be executed within a step:\\n\\nBelow, we use an existing function create_db_connection() that creates a connection to the database with the credentials brought on our Craft AI environment platform.\\nThen, we use an existing function filter_data() that retrieves some data in a specific table on the database by filtering with the ids inputs on a specific column.\\nAs input of our filter_data_from_database() function, we have this list of integer, ids, and as output we have a dictionary whose key is filtered_data. ids and filtered_data are respectively input and output of the step we would define after.\\n\\nHere is the corresponding code:\\nimport os\\nimport dotenv\\n\\ndotenv.load_dotenv()\\n\\nDB_HOST = os.environ[\"DB_HOST\"]\\nDB_ADMIN = os.environ[\"DB_ADMIN\"]\\nDB_PASS = os.environ[\"DB_PASS\"]\\nDB_NAME = os.environ[\"DB_NAME\"]\\nDB_PORT = os.environ[\"DB_PORT\"]\\n\\ndef filter_data_from_database(ids: List[int]) :\\n    try:\\n        # With an existing function create_db_connection,\\n        # Creating a connection to the database with the \\n        # credentials brought on our Craft AI environment platform.\\n        conn = create_db_connection(\\n            host=DB_HOST,\\n            user=DB_ADMIN,\\n            password=DB_PASS,\\n            database=DB_NAME,\\n            port=DB_PORT\\n        )\\n\\n        # With an existing function filter_data, \\n        # Retrieving some data in a specific table on the database \\n        # by filtering with the ids inputs on a specific column.\\n        df_data_filtered = filter_data(conn, ids)\\n        df_data_filtered_final = df_data_filtered.tolist()\\n    finally:\\n        conn.close()\\n    return {\"filtered_data\": df_data_filtered_final}\\n\\n\\n\\n\\n\\n\\nFrom an external Data Storage\\uf0c1\\nHere is an example of the few points to do in order to access a specific external data storage directly from a Craft AI environment platform by using the usual credentials and be able to retrieve one specific csv file:\\n\\nFirst, we embed the credentials we usually use to access to the data storage in our environment by setting them as environment variables.\\n\\nsdk = CraftAiSdk(\\n    sdk_token=**our-sdk-token**, \\n    environment_url=**our-environment-url**)\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"SERVER_PUBLIC_KEY\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"SERVER_SECRET_KEY\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"REGION_NAME\",\\n    environment_variable_value=\"xxx\")\\n\\n\\n\\nSecondly, we encapsulate the code we would use without the Craft AI platform in a function that will be executed within a step.\\n\\nBelow, we use an existing function configure_client() that configures a client (data storage specific) with the credentials brought on our Craft AI environment platform to access the data storage.\\nThen, we use an existing function get_object_from_bucket() **that, with the configured client, retrieves the object key in the bucket bucket on the data storage.\\n\\n\\n\\nimport os\\nimport dotenv\\n\\ndotenv.load_dotenv()\\n\\nSERVER_PUBLIC_KEY = os.environ[\"SERVER_PUBLIC_KEY\"]\\nSERVER_SECRET_KEY = os.environ[\"SERVER_SECRET_KEY\"]\\nREGION_NAME = os.environ[\"REGION_NAME\"]\\n\\ndef filter_data_from_data_storage(bucket: str, key:str)\\n\\n    # With an existing function configure_client, \\n    # Configuring a client (data storage specific) \\n    # with the credentials brought on our Craft AI environment platform.\\n    client = configure_client(\\n        public_key=SERVER_PUBLIC_KEY, \\n        secret_key=SERVER_SECRET_KEY, \\n        region=REGION_NAME)\\n\\n    # With an existing function get_object_from_bucket,\\n    # Retrieving the object key in the bucket bucket on the data storage.\\n    buffer = get_object_from_bucket(\\n        client=client,\\n        bucket_name=bucket,\\n        key=key\\n    )\\n\\n    dataframe = pd.read_csv(buffer)\\n    return dataframe\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nRun and serve your pipelines — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nUser workflow\\nRun and serve your pipelines\\n\\n\\n\\n\\n\\n\\n\\n\\nRun and serve your pipelines\\uf0c1\\nData scientists have a crucial role in the creation and implementation of machine learning models, converting advanced algorithms into useful applications. In tasks like data processing, training, scheduling, and model deployment, data scientists often face the challenge of initiating pipelines using different methods while efficiently handling various inputs. This adaptability is vital because it allows them to address a wide range of scenarios and demands.\\nTo streamline this process, the MLOps platform provides a robust and user-friendly deployment features, empowering data scientists to execute and serve their pipelines efficiently.\\nLearn about the various ways to execute your pipelines, which include:\\n\\nPipeline run\\nPipeline deployment, with 2 execution rules\\n\\nEndpoint rule\\nPeriodic rule\\n\\n\\n\\nOne of the fundamental elements provided by the MLOps platform to run and serve your models is the ability to customize the sources of the inputs you provide to a pipeline and the ways you want to retrieve your outputs. The main idea is to easily connect different data sources to your inputs (data directly coming from the final user, or coming from the environment for example) and destinations (delivering output to the final user, writing it in the data store …). This is known as mapping in the platform, and it plays a central role when creating deployments or running pipelines.\\n\\nSummary\\uf0c1\\n\\nHow to run your pipeline\\nHow to define source and destination for step input and output ?\\n\\n\\n\\nPrerequisites\\uf0c1\\nBefore using the Craft AI platform, make sure you have completed the following prerequisites:\\n\\nGet access to an environment\\nConnect the SDK\\nCreate a step\\nCreate a pipeline\\n\\n\\n\\n    ℹ️ Make sure that your code inside your step with inputs/outputs is able to read inputs from function parameters and send outputs with the return of the function.\\n    More information [here](../stepPipeline/createStep).\\n\\n\\n\\nHow to execute your pipeline\\uf0c1\\nAs a data scientist, I want to be able to execute my Python code contained in my pipelines in various scenarios :\\n\\nI (or data scientist in my team) want to launch my pipeline on the fly via the craft AI SDK, so I can use the run pipeline.\\nI want to be able to make my pipeline accessible to external users, inside or outside my organization, via a  Application Programmatic Interface (API), I can use the endpoint deployment feature to accomplish this.\\nI want my pipeline to be automatically execute at regular intervals without manual intervention, the periodic deployment feature can be employed.\\n\\n\\nRun your pipeline\\uf0c1\\nThe run pipeline functionality enables you to trigger a pipeline directly from the Craft AI platform’s SDK. This option offers the advantage of being user-friendly and efficient.\\nPlease note that the run pipeline feature is exclusively accessible through the Craft AI SDK and requires proper connection to the appropriate platform environment. It is recommended for conducting experiments and conducting internal testing purposes.\\nTo execute a run on a pipeline with no input and no output, simply use this function:\\n# Run pipeline function \\nsdk.run_pipeline(pipeline_name=\"your-pipeline-name\")\\n\\n\\nTo execute a run on a pipeline with inputs and outputs, you can use the same function and add the parameter inputs and give an object with inputs’ names as keys (should be the same name as defined in input object give at the step creation) and values you want to provide to your step at execution:\\n# Creating an object with predefined input values for the run,\\n# which will be provided as inputs to the run pipeline function\\ninputs_values = {\\n\\t\"number1\": 9, # Value for Input \"number1\" of the step (defined during step creation)\\n\\t\"number2\": 6 # Value for Input \"number2\" of the step (defined during step creation) \\n}\\n\\n# Running the Pipeline and Receiving Output\\noutput_values = sdk.run_pipeline(pipeline_name=\"your-pipeline-name\", inputs=inputs_values)\\n\\nprint (outputs_values[\"outputs\"])\\n\\n\\nFor example, you can obtain an output like this :\\n>> {\\n    \\'output_name_1\\': \\'Lorem ipsum dolor sit amet, consectetur adipiscing elit.\\',\\n    \\'output_name_2\\': 42\\n }\\n\\n\\nThis function will return you the output of your pipeline. Like the inputs, it’s represented as an object with the outputs’ name as keys and the outputs values as values.\\n\\nWarning\\n⚠️ Don’t forget to adapt your code to get value from input and return output. You must have created inputs and outputs objects at the step creation stage to get values.\\nMore information here.\\n\\nIt is also possible to run a pipeline with specific mapping (to connect inputs/outputs to the data store, environment variable, etc.), that will be covered in this section.\\n\\n\\nCreate an endpoint\\uf0c1\\nTriggering a pipeline via an endpoint will enable you to make your application available from any programming language/tool (website, mobile application, etc.).\\nTo make your pipeline available via an endpoint, we need to create a deployment using the create_deployment() function which has the  execution_rule parameter set as endpoint. In return, we’ll get the URL of the endpoint and its authentication token so that we can call it and trigger our pipeline.\\n# Deployment creation with endpoint as execution rule\\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n)\\n\\n\\n\\n\\n    ℹ️ Deployment creation will be soon available on web UI.\\n\\nYou can trigger this endpoint from anywhere with any programming language if you have:\\n\\nEnvironment URL (Can be found in web UI and it’s the same you have used to initiate your SDK)\\nDeployment name\\nEndpoint token (given as result of the deployment creation, which secures access to the endpoint)\\n\\nTo trigger the deployed pipeline as an endpoint, you have a couple of options:\\n\\nUtilize the dedicated function provided by the Craft AI SDK.\\n# Execution of pipeline using the endpoint\\nsdk.trigger_endpoint(\\n\\tendpoint_name=\"your-deployment-name\", \\n\\tendpoint_token=endpoint_info[\"endpoint_token\"] # Token recieve after deployment creation \\n)\\n\\n\\n\\nImplement a HTTP request to the endpoint using any programming language, similar to the example shown with the command curl :\\ncurl -X POST -H \"Authorization: EndpointToken <ENDPOINT_TOKEN>\" \"<CRAFT_AI_ENVIRONMENT_URL>/endpoints/<DEPLOYMENT_NAME>\"\\n\\n\\n\\nAn other example with Javascript syntax (using axios) :\\nconst axios = require(\\'axios\\');\\n\\nconst ENDPOINT_TOKEN = \\'<ENDPOINT_TOKEN>\\';\\nconst CRAFT_AI_ENVIRONMENT_URL = \\'<CRAFT_AI_ENVIRONMENT_URL>\\';\\nconst DEPLOYMENT_NAME = \\'<DEPLOYMENT_NAME>\\';\\n\\nconst headers = {\\n  \\'Authorization\\': `EndpointToken ${ENDPOINT_TOKEN}`\\n};\\n\\nconst url = `${CRAFT_AI_ENVIRONMENT_URL}/endpoints/${DEPLOYMENT_NAME}`;\\n\\naxios.post(url, null, { headers })\\n  .then(response => {\\n    console.log(\\'Response:\\', response.data);\\n  })\\n  .catch(error => {\\n    console.error(\\'Error:\\', error.message);\\n  });\\n\\n\\n\\n\\n\\n\\n    ℹ️  The URL of your endpoint follows the structure :   <CRAFT_AI_ENVIRONMENT_URL>/endpoints/<YOUR_DEPLOYMENT_NAME>\\nThe inputs and outputs defined in your step are automatically linked to the deployment and consequently to the endpoint. You can therefore send a JSON within your request where the parameters correspond to the step inputs so that it can be used as a parameter for your function in the step. The deployment will return a similar object with your outputs as parameters.\\nYou can call the endpoint with inputs using the SDK function or with any other programming language (like before) by specifying the inputs in JSON format in the request body.\\n\\nWith Craft AI SDK :\\n# Value of inputs to be given to trigger pipeline function\\ninputs_values = {\\n\\t\"number1\": 9, # Input \"number1\" defined in step creation \\n\\t\"number2\": 6 # Input \"number2\" defined in step creation \\n}\\n\\n# Execution of pipeline using the endpoint\\nsdk.trigger_endpoint(\\n\\tendpoint_name=\"your-deployment-name\", \\n\\tendpoint_token=endpoint_info[\"endpoint_token\"] # Token recieve after deployment creation \\n\\tinputs=inputs_values\\n)\\n\\n\\n\\nWith curl :\\ncurl -X POST -H \"Authorization: EndpointToken <ENDPOINT_TOKEN>\" -H \"Content-Type: application/json\" -d \\'{\"number1\": 9, \"number2\": 6}\\' \"<CRAFT_AI_ENVIRONMENT_URL>/endpoints/<YOUR-DEPLOYMENT-NAME>\"\\n\\n\\n\\nWith JavaScript using axios :\\nconst axios = require(\\'axios\\');\\n\\nconst ENDPOINT_TOKEN = \\'<ENDPOINT_TOKEN>\\';\\nconst CRAFT_AI_ENVIRONMENT_URL = \\'<CRAFT_AI_ENVIRONMENT_URL>\\';\\nconst YOUR_DEPLOYMENT_NAME = \\'<YOUR-DEPLOYMENT-NAME>\\';\\n\\nconst requestData = {\\n  number1: 9,\\n  number2: 6\\n};\\n\\nconst config = {\\n  headers: {\\n    \\'Authorization\\': `EndpointToken ${ENDPOINT_TOKEN}`,\\n    \\'Content-Type\\': \\'application/json\\'\\n  }\\n};\\n\\nconst url = `${CRAFT_AI_ENVIRONMENT_URL}/endpoints/${YOUR_DEPLOYMENT_NAME}`;\\n\\naxios.post(url, requestData, config)\\n  .then(response => {\\n    console.log(\\'Response:\\', response.data);\\n  })\\n  .catch(error => {\\n    console.error(\\'Error:\\', error);\\n  });\\n\\n\\n\\n\\n\\n\\n    ℹ️ For inputs and outputs, don\\'t forget to have adapted the step code, to have declared the inputs and outputs at step level and to have used types (string, integer, etc.) compatible with the data you are going to manipulate.\\n\\n\\n\\nPeriodic\\uf0c1\\nYou might need to trigger your Python code regularly, whether it’s every X minutes, every hour, or at a specific date, automatically. To achieve this, you can create a periodic deployment for your pipeline. This type of deployment uses the CRON format for scheduling its triggers.\\nTo set up a periodic deployment, you employ the same function as you would for endpoints. However, you specify the periodic trigger mode and define when it should trigger by providing a CRON rule in the schedule parameter.\\ndeployment_info = sdk.create_deployment(\\n    execution_rule=\"periodic\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    schedule=\"* * * * *\" # Will be executed every minute of every day \\n)\\n\\n\\n\\nMore information and help about CRON here.\\n\\n\\n\\nHow to define sources and destinations for step inputs and outputs ?\\uf0c1\\nIn the previous section, we saw that deployments can be used to trigger the execution of a pipeline, but they can also be used to give and receive information via inputs and outputs. When a pipeline is triggered, it may need to receive or send this information to different sources or destinations.\\nExample:\\nIn the diagram below, we assume that we have deployed an endpoint pipeline. An API is therefore available to the user who triggers the execution of the pipeline each time a request is sent to the API. The request can contain information required to execute the pipeline, just as the pipeline can send information back to the user, as shown in the diagram below.\\n\\nTo direct these flows to the right place, the platform allows you to map the step inputs and outputs to different sources and destinations using InputSource and OutputDestination objects. We’ll look at four different types of mapping:\\n\\nConstant mapping\\nEndpoint mapping (value or file)\\nEnvironment variable mapping\\nNone / void mapping\\n\\nInputs and outputs are not compatible with all types of deployment. To make things clearer, here is a summary table:\\n\\n\\n\\nConstant\\nEndpoint value\\nEndpoint file\\nEnvironment variable\\nData store file\\nNone / void\\n\\n\\n\\nRun\\n✅\\n❌\\n❌\\n✅ (input only)\\n✅\\n✅\\n\\nEndpoint\\n✅\\n✅\\n✅ (limited to 1 file per call)\\n✅ (input only)\\n✅\\n✅\\n\\nPeriodic\\n✅\\n❌\\n❌\\n✅ (input only)\\n✅\\n✅\\n\\n\\n\\n\\nConstant\\uf0c1\\nIf I want my deployment to always use the same value as input, I can use a mapping to a constant.\\n\\n\\n    ℹ️ Note that the same pipeline can have multiple deployments with multiple different constant values for the same input.\\n\\nWe will be using the constant value for an endpoint deployment here, but the process is the same for other types of deployment (periodic).\\nTo do this, we create an InputSource object for each input in the pipeline that we want to deploy, specifying the name of the input for each mapping and the value of the input using the constant_value parameter.\\nExample :\\n# Endpoint Input mapping\\nendpoint_input1 = InputSource(\\n    step_input_name=\"number1\",\\n    constant_value=6,\\n)\\n\\nendpoint_input2 = InputSource(\\n    step_input_name=\"number2\",\\n    constant_value=3,\\n)\\n\\n# Deployment creation using inputSource object \\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    inputs_mapping=[endpoint_input1, endpoint_input2]\\n)\\n\\n\\n\\nWarning\\n⚠️ Step must be created with one or more inputs and the code contained in my step must be suitable for receiving a constant. More information about it here.\\nAll input types are compatible, except for file.\\n\\n\\n\\nEndpoint\\uf0c1\\nAs explained above, when a deployment is of the endpoint type, the inputs and outputs of the associated steps have as their default source and destination the parameters of the HTTP request.\\nYou can therefore send a JSON within your request where the keys correspond to the step inputs so that it can be used as parameters for your function in the step. The deployment will return a similar object with your outputs as keys.\\nIf you need to change this default behavior, you can do so using InputSource and OutputDestination. You can define new names that will only be seen at the endpoint level for the external user. It allows you to specify under which names should inputs be passed in the request JSON by your final user or for the outputs, under which names they will be returned.\\nThis input mapping works with any type of input or output (integer, file, string, etc.).\\n# Endpoint Input mapping\\nendpoint_input1 = InputSource(\\n    step_input_name=\"number1\",\\n    endpoint_input_name=\"number_a\",\\n)\\n\\nendpoint_input2 = InputSource(\\n    step_input_name=\"number2\",\\n    endpoint_input_name=\"number_b\",\\n) \\n\\nendpoint_output = OutputDestination(\\n    step_output_name=\"number3\",\\n    endpoint_output_name=\"result\",\\n)\\n\\n# Deployment creation using 2 input mapping and 1 ouptut mapping\\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    inputs_mapping=[endpoint_input1, endpoint_input2],\\n    outputs_mapping=[endpoint_output]\\n)\\n\\n\\nNow that we have created our deployment, we can trigger the pipeline through the endpoint by passing the inputs and reading the outputs with the new mappings.\\n# Value of inputs to be given to trigger pipeline function\\ninputs_values = {\\n\\t\"number_a\": 9, # Using mapping defined before linked to number1\\n\\t\"number_b\": 6 # Using mapping defined before linked to number2\\n}\\n\\n# Execution of pipeline using the endpoint\\nendpointOutput = sdk.trigger_endpoint(\\n\\tendpoint_name=\"your-deployment-name\", \\n\\tendpoint_token=endpoint_info[\"endpoint_token\"] # Token received after deployment creation \\n\\tinputs=inputs_values\\n)\\n\\n# Print result, note you should go into \"outputs\" before\\nprint (endpointOutput[\"outputs\"][\"result\"])\\n\\n\\nThese changes are effective for any endpoint trigger method (curl, JS, etc.).\\n\\n\\n    ℹ️ Obviously, this type of mapping is only available if the deployment is an endpoint.\\n\\n\\n\\nEnvironment variable\\uf0c1\\nAs a data scientist, I may need to use data common for my entire environment in my pipeline. This can be achieved by mapping environment variables to inputs.\\nTo do this, we also use the mapping system with the InputSource object. Environment variables are only available for inputs and not for outputs (but it is still possible to define them directly in the step code).\\nFirst, let’s look at how to initialize the two environment variables (on the platform environment), we’re going to use:\\n# Creation of env variable for input1\\nsdk.create_or_update_environment_variable(\\n\\tenvironment_variable_name=\"RECETTE_VAR_ENV_INPUT1\",\\n\\tenvironment_variable_value=6\\n)\\n\\n# Creation of env variable for input2\\nsdk.create_or_update_environment_variable(\\n\\tenvironment_variable_name=\"RECETTE_VAR_ENV_INPUT2\",\\n\\tenvironment_variable_value=4\\n)\\n\\n\\nNow, we can create an InputSource object and use it at the deployment creation.\\n# Creation of InputSource to get env variable into input \\nendpoint_input1 = InputSource(\\n\\t step_input_name=\"number1\",\\n\\t environment_variable_name=\"RECETTE_VAR_ENV_INPUT1\",\\n)\\n\\nendpoint_input2 = InputSource(\\n\\t step_input_name=\"number2\",\\n\\t environment_variable_name=\"RECETTE_VAR_ENV_INPUT2\",\\n)\\n\\n# Endpoint creation\\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    inputs_mapping=[endpoint_input1, endpoint_input2]\\n)\\n\\n\\nThe deployment is created, we can trigger it without giving any object into data of HTTP request, it will take current environment variable value.\\n# Value of inputs to be given to execute pipeline function\\ninputs_value = {\\n\\t\"number_a\": 9, # Using mapping defined before linked to number1\\n\\t\"number_b\": 6 # Using mapping defined before linked to number2\\n}\\n\\n# Execution of pipeline using the endpoint\\nsdk.trigger_endpoint(\\n\\tendpoint_name=\"your-deployment-name\", \\n\\tendpoint_token=endpoint_info[\"endpoint_token\"] # Token recieve after deployment creation \\n\\tinputs=inputs_value\\n)\\n\\n\\n\\n\\nData store\\uf0c1\\nYou may need to transfer files between your data store and your step (in one direction or the other). To do this, you can also use inputs and outputs mapping system. You’ll find all the explanations you need on this page.\\n\\n\\nVoid / None\\uf0c1\\nAll step inputs and outputs have to be mapped when you deploy a pipeline with inputs and outputs (otherwise it will raise an error when creating the deployment). If you don’t really want to give/receive data in these inputs and outputs, you can map them to None. This will create a None object in Python for the inputs and send the outputs into the void.\\n# Endpoint Input mapping\\nendpoint_input1 = InputSource(\\n    step_input_name=\"number1\",\\n    is_null=True,\\n)\\n\\nendpoint_input2 = InputSource(\\n    step_input_name=\"number2\",\\n    is_null=True,\\n) \\n\\nendpoint_output = OutputDestination(\\n     step_output_name=\"number3\",\\n     is_null=True\\n)\\n\\n# Deployment creation using 2 input mappings and 1 ouptut mapping\\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    inputs_mapping=[endpoint_input1, endpoint_input2],\\n    outputs_mapping=[endpoint_output]\\n)\\n\\n\\nNow that the deployment has been created, it can be triggered without giving or receiving any input or output.\\n# Execution of pipeline using the endpoint who will return any output (object with value)\\nsdk.trigger_endpoint(\\n\\tendpoint_name=\"your-deployment-name\", \\n\\tendpoint_token=endpoint_info[\"endpoint_token\"] # Token received after deployment creation\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nEnvironments — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nEnvironments\\n\\n\\n\\n\\n\\n\\n\\n\\nEnvironments\\uf0c1\\nAn environment is the infrastructure used by the platform to store\\ndata and run computation.\\n\\nBy default, our environments are created on AWS, but it is possible, if\\nneeded, to configure our platform on another Kubernetes-compatible cloud (e.g.\\xa0OVH,\\nGCP, Azure, etc.).\\nUsers can create as many environments as they want in each\\nproject. Each environment is\\nhermetically sealed from the others.\\nIt is composed of:\\n\\na cluster: cloud computing resources based on a fully managed Kubernetes service. The ML pipelines created in an environment are executed on the associated cluster.\\na data store: cloud storage to save and retrieve any amount of data at any time. The results of the pipelines and metrics generated by the platform are also stored on the data store.\\n\\nIt is possible to tag environments with 3 types: development, test\\nand production.\\nThe main objectives of the environments are:\\n\\nBuilding and managing infrastructures in a few clicks without DevOps\\nskills.\\nEnsuring an efficient and automated use of computing resources, thanks\\nto Kubernetes.\\nEnabling large scale deployment of Python jobs without rewriting to a\\nmore production friendly language.\\nFine-tuning the size of your environments according to the needs of\\nyour projects.\\nEasily building dedicated environments for each phase of the project\\nto adopt software development best practices.\\n\\n\\nEnvironments\\n\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/introduction.html', 'title': 'Environments — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nEnvironments — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nEnvironments\\n\\n\\n\\n\\n\\n\\n\\n\\nEnvironments\\uf0c1\\nAn environment is the infrastructure used by the platform to store\\ndata and run computation.\\n\\nBy default, our environments are created on AWS, but it is possible, if\\nneeded, to configure our platform on another Kubernetes-compatible cloud (e.g.\\xa0OVH,\\nGCP, Azure, etc.).\\nUsers can create as many environments as they want in each\\nproject. Each environment is\\nhermetically sealed from the others.\\nIt is composed of:\\n\\na cluster: cloud computing resources based on a fully managed Kubernetes service. The ML pipelines created in an environment are executed on the associated cluster.\\na data store: cloud storage to save and retrieve any amount of data at any time. The results of the pipelines and metrics generated by the platform are also stored on the data store.\\n\\nIt is possible to tag environments with 3 types: development, test\\nand production.\\nThe main objectives of the environments are:\\n\\nBuilding and managing infrastructures in a few clicks without DevOps\\nskills.\\nEnsuring an efficient and automated use of computing resources, thanks\\nto Kubernetes.\\nEnabling large scale deployment of Python jobs without rewriting to a\\nmore production friendly language.\\nFine-tuning the size of your environments according to the needs of\\nyour projects.\\nEasily building dedicated environments for each phase of the project\\nto adopt software development best practices.\\n\\n\\nEnvironments\\n\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/introduction.html', 'title': 'Environments — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nCreate an environment — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nEnvironments\\nCreate an environment\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate an environment\\uf0c1\\nAn environment is the infrastructure used by the platform to store\\ndata and run computation. The first thing to do when you have created a\\nnew project is to create an environment composed of a cluster and a data store.\\n\\nDevelopment : For experimenting your projects and models on the\\nplatform.\\nTesting : For testing your projects just before the production\\nwith the same settings.\\nProduction : To use your pipelines in production and secure the\\naccess.\\n\\n\\nSummary\\uf0c1\\n\\nSet up an environment\\nManage an environment\\n\\n\\n\\nSet up an environment\\uf0c1\\nThe environment creation is available by email for the moment. In a next\\nversion, it will be possible to create, edit and delete an environment\\ndirectly on the platform UI.\\nFor the moment, you can send us your request by email to the address\\nyou have been given or on your dedicated Slack channel using this template:\\n——————————— My demand ——————————-\\n\\nChoose between :\\n- Create an environment\\n- Edit an environment\\n- Delete an environment\\n\\n——————————- Name of environment ——————————-\\n\\nPrecise the name of environment to create or the name of existing environment you want\\nto edit/delete\\n\\n———————————- Hardware ——————————-\\n\\nChoose environment size (see table below).\\n\\n———————————- Tag ———————————–\\n\\nChoose between this tag to set an environnement type :\\n- Dev\\n- Pre-prod\\n- Prod\\n\\n\\nEach environment is based on hardware infrastructure on the cloud.\\nIt’s possible to select different levels of size and computing power for this cluster\\naccording to your business use case and your usage.\\n\\n\\nSize\\nHardware\\nWorkers\\n\\n\\n\\nCPU size 1\\n2CPU 4GB\\n2\\n\\nCPU size 2\\n4CPU 16GB\\n2\\n\\nCPU size 3\\n8CPU 32GB\\n2\\n\\nCPU size 4\\n16CPU 64GB\\n2\\n\\nGPU size 1\\n1GPU (T4) 16GB (VRAM)\\n2\\n\\nGPU size 2\\n1GPU (A10G) 24GB (VRAM)\\n2\\n\\n\\n\\n\\n\\nManage an environment\\uf0c1\\nIf you need more size and computing power than expected, you can also change your\\nenvironment size once your environment is already created. All you\\nhave to do is simply send a Slack message on your dedicated channel\\nor an email to support@craft.ai with the name of the environment\\nand the new size you want.\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/createEnvironment.html', 'title': 'Create an environment — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nWork with environment variables — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nEnvironments\\nWork with environment variables\\n\\n\\n\\n\\n\\n\\n\\n\\nWork with environment variables\\uf0c1\\nAn environment is the infrastructure used by the platform to store\\ndata and run computation. Once created, you can start working in the\\nenvironments by creating pipelines and running them in the platform.\\nIn addition, you can create and save environment variables that\\nwill allow you to parameterize certain variables in order to call them\\nwhen running or deploying a pipeline.\\n\\n\\nFunction name\\nMethod\\nReturn type\\nDescription\\n\\n\\n\\ncreate_or_update_environment_variable\\nCraftAiSdk.create_or_update_environment_variable (environment_variable_name, environment_variable_value)\\ndict\\nTo create or update an environment variable available for all pipelines executions.\\n\\nlist_environment_variables\\nCraftAiSdk.list_environment_variables()\\nList of dict\\nGet a list of all environments variables in the current environment.\\n\\ndelete_environment_variable\\nCraftAiSdk.delete_environment_variable(environment_variable_name)\\ndict\\nDelete a specified environment variable.\\n\\n\\n\\n\\nSet up an environment variable\\uf0c1\\nAn environment variable is a value that can be passed to your step\\ncode. It is used to store information that may be needed by the\\noperating system or by applications that run on the platform (for example, by endpoints you deploy on the platform).\\n\\nCreate and update an environment variable\\uf0c1\\nTo create or update an environment variable available for all pipelines\\nexecutions.\\nCraftAiSdk.create_or_update_environment_variable(environment_variable_name,\\nenvironment_variable_value)\\n\\n\\n\\nParameters\\uf0c1\\n\\nenvironment_variable_name (str) – Name of the environment\\nvariable to create.\\nenvironment_variable_value (str) – Value of the environment\\nvariable to create.\\n\\n\\n\\nReturns\\uf0c1\\nA dict object containing the ID of environment variable (with keys “id”)\\n\\n\\n\\nGet the list of environment variables\\uf0c1\\nGet the list of all environment variables in the current environment.\\nCraftAiSdk.list_environment_variables()\\n\\n\\n\\nParameter\\uf0c1\\nNo parameter\\n\\n\\nReturns\\uf0c1\\nList of dicts of environment variables (with keys “name” and\\n“value”)\\n\\n\\n\\nDelete an environment variable\\uf0c1\\nDelete a specified environment variable.\\nCraftAiSdk.delete_environment_variable(environment_variable_name)\\n\\n\\n\\nParameter\\uf0c1\\n\\nenvironment_variable_name (str) – Name of the environment\\nvariable to delete.\\n\\n\\n\\nReturns\\uf0c1\\nDict (with keys “name” and “value”) of the deleted environment variable\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/workOnEnvironment.html', 'title': 'Work with environment variables — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nSave my data on the store — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nEnvironments\\nSave my data on the store\\n\\n\\n\\n\\n\\n\\n\\n\\nSave my data on the store\\uf0c1\\nEach environment has its own storage to save and retrieve any amount\\nof data at any time, from anywhere on the web. This storage is an\\nAmazon S3 bucket that you can manage with the Python SDK of the platform.\\nYou can upload any kind of raw data on the store. Furthermore, the\\nresults of the pipelines and metrics generated by the platform can also\\nbe saved on the data store.\\nInfo: For the moment, it is not possible to copy data from an\\nenvironment to another. You have to download it and to re-upload it. In\\nfuture versions, you will be able to transfer your data.\\n\\n\\n\\n🆕 The graphical interface of the data store will arrive later on the\\nplatform.\\n\\n\\n\\n\\nSummary\\uf0c1\\n\\nGet files\\nGet file info\\nUpload a file\\nDownload a file\\nDelete a file\\n\\n\\n\\nFunction name\\nMethod\\nReturn type\\nDescription\\n\\n\\n\\nlist_data_store_objects\\nCraftAiSdk.list_data_store_objects()\\nlist of dict\\nGet the list of the objects stored in the data store.\\n\\nupload_data_store_object\\nCraftAiSdk.upload_data_store_object(filepath_or_buffer, object_path_in_datastore)\\nNone\\nUpload a file as an object into the data store.\\n\\ndownload_data_store_object\\nCraftAiSdk.download_data_store_object(object_path_in_datastore, filepath_or_buffer)\\nNone\\nDownload an object in the data store and save it into a file.\\n\\ndelete_data_store_object\\nCraftAiSdk.delete_data_store_object(object_path_in_datastore)\\ndict\\nDelete an object on the data store.\\n\\n\\n\\n\\n\\nGet files\\uf0c1\\n\\nFunction definition\\uf0c1\\nGet the list of the objects stored in the data store.\\nCraftAiSdk.list_data_store_objects()\\n\\n\\n\\nParameter\\uf0c1\\nNo parameter\\n\\n\\nReturns\\uf0c1\\nList of objects (dict) in the data store, each object being represented as dict with :\\n\\n“path” (str): Location of the object in the data store.\\n“last_modified” (str): The creation date or last modification date in ISO format.\\n“size” (str): The size of the object with units of digital storage measurement (MB, GB, …).\\n\\n\\n\\n\\n\\nGet file info\\uf0c1\\n\\nFunction definition\\uf0c1\\nGet information about a single object in the data store.\\nCraftAiSdk.get_data_store_object_information(object_path_in_datastore)\\n\\n\\n\\nParameter\\uf0c1\\n\\nobject_path_in_datastore (str) – Location of the object in the data store.\\n\\n\\n\\nReturns\\uf0c1\\nObject information, with the following keys:\\n\\n“path” (str): Location of the object in the data store.\\n“last_modified” (str): The creation date or last modification date in ISO format.\\n“size” (str): The size of the object with units of digital storage measurement (MB, GB, …).\\n\\n\\n\\n\\n\\nUpload a file\\uf0c1\\n\\nFunction definition\\uf0c1\\nUpload a file as an object into the data store.\\nCraftAiSdk.upload_data_store_object(filepath_or_buffer, object_path_in_datastore)\\n\\n\\n\\nParameters\\uf0c1\\n\\nfilepath_or_buffer (str, or file-like object) – String, path to\\nthe file to be uploaded ; or file-like object implementing a read()\\nmethod (e.g.\\xa0via buildin open function). The file object must be\\nopened in binary mode, not text mode.\\nobject_path_in_datastore (str) – Destination of the uploaded\\nfile.\\n\\n\\n\\nReturns\\uf0c1\\nThis function returns None.\\n\\n\\n\\n\\nDownload a file\\uf0c1\\n\\nFunction definition\\uf0c1\\nDownload an object in the data store and save it into a file.\\nCraftAiSdk.download_data_store_object(object_path_in_datastore, filepath_or_buffer)\\n\\n\\n\\nParameters\\uf0c1\\n\\nobject_path_in_datastore (str) – Location of the object to\\ndownload from the data store.\\nfilepath_or_buffer (str or file-like object) – String, file path\\nto save the file to ; or a file-like object implementing a write()\\nmethod, (e.g.\\xa0via builtin open function). The file object must be\\nopened in binary mode, not text mode.\\n\\n\\n\\nReturns\\uf0c1\\nThis function return an None.\\n\\n\\n\\n\\nDelete a file\\uf0c1\\n\\nFunction definition\\uf0c1\\nDelete an object on the data store.\\nCraftAiSdk.delete_data_store_object(object_path_in_datastore)\\n\\n\\n\\nParameters\\uf0c1\\n\\nobject_path_in_datastore (str) – Location of the object to delete\\nin the data store.\\n\\n\\n\\nReturns\\uf0c1\\nDeleted object represented as dict (with key “path”).\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/saveOnDataStore.html', 'title': 'Save my data on the store — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nStep & Pipeline — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nStep & Pipeline\\n\\n\\n\\n\\n\\n\\n\\n\\nStep & Pipeline\\uf0c1\\nA pipeline is a machine learning workflow, consisting of one or more\\nsteps, to deploy containerised code. Like a regular function, a\\nstep is defined by the input it ingests, the code it runs, and\\nthe output it returns. You can then create a full pipeline formed\\nwith a computed acyclic graph (DAG) by specifying the output of one step\\nas the input of another step.\\n\\n💡 The pipelines are written in Python with SDK calls for an easy\\nauthoring experience and executed on Kubernetes for scalability. The\\npipelines’ functioning are based on the open-source library Argo and\\nthe steps are based on code stored on Github / GitLab. Each step deploy\\nis containerised with Docker.\\nThe main objectives of the steps and pipelines are :\\n\\nOrchestrating end-to-end ML workflows\\nIncreasing reusability of Data Science components from a project to\\nanother\\nCollaboratively managing, tracking and viewing pipeline definitions.\\nDeploying in production in few clicks with several methods (endpoint,\\nCRON, manual, …)\\nEnabling large scale production of Python code without refactoring to\\na more production friendly language\\nEnsuring an efficient use of compute resources, thanks to\\nKubernetes\\n\\nExample :\\n🖊️ Training pipeline\\n\\nData preparation and preprocessing: In this stage, raw data is\\ncollected, cleaned, and transformed into a format that is suitable\\nfor training a machine learning model. This may involve tasks such as\\nfiltering out missing or invalid data points, normalizing numerical\\nvalues, and encoding categorical variables.\\nModel training: In this stage, a machine learning model is trained on\\na prepared dataset. This may involve selecting a model type, tuning\\nhyperparameters, and training the model using an optimization\\nalgorithm.\\nModel evaluation: Once the model has been trained, it is important to\\nevaluate its performance to determine how well it generalizes to new\\ndata. This may involve tasks such as splitting the dataset into a\\ntraining set and a test set, evaluating the model’s performance on\\nthe test set, and comparing the results to a baseline model.\\n\\n🖊️ Inference pipeline\\n\\nModel loading: In this stage, a trained machine learning model is\\nloaded from storage and prepared for use. This may involve tasks such\\nas loading the model’s weights and any associated dependencies.\\nData preparation: In this stage, incoming data is cleaned and\\ntransformed into a format that is suitable for the model. This may\\ninvolve tasks such as normalizing numerical values and encoding\\ncategorical variables.\\nInference: In this final stage, the model is used to make predictions\\non the prepared data. This may involve tasks such as passing the data\\nthrough the model and processing the output to generate a final\\nprediction.\\n\\n\\nStep & pipeline\\n\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/introduction.html', 'title': 'Step & Pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nStep & Pipeline — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nStep & Pipeline\\n\\n\\n\\n\\n\\n\\n\\n\\nStep & Pipeline\\uf0c1\\nA pipeline is a machine learning workflow, consisting of one or more\\nsteps, to deploy containerised code. Like a regular function, a\\nstep is defined by the input it ingests, the code it runs, and\\nthe output it returns. You can then create a full pipeline formed\\nwith a computed acyclic graph (DAG) by specifying the output of one step\\nas the input of another step.\\n\\n💡 The pipelines are written in Python with SDK calls for an easy\\nauthoring experience and executed on Kubernetes for scalability. The\\npipelines’ functioning are based on the open-source library Argo and\\nthe steps are based on code stored on Github / GitLab. Each step deploy\\nis containerised with Docker.\\nThe main objectives of the steps and pipelines are :\\n\\nOrchestrating end-to-end ML workflows\\nIncreasing reusability of Data Science components from a project to\\nanother\\nCollaboratively managing, tracking and viewing pipeline definitions.\\nDeploying in production in few clicks with several methods (endpoint,\\nCRON, manual, …)\\nEnabling large scale production of Python code without refactoring to\\na more production friendly language\\nEnsuring an efficient use of compute resources, thanks to\\nKubernetes\\n\\nExample :\\n🖊️ Training pipeline\\n\\nData preparation and preprocessing: In this stage, raw data is\\ncollected, cleaned, and transformed into a format that is suitable\\nfor training a machine learning model. This may involve tasks such as\\nfiltering out missing or invalid data points, normalizing numerical\\nvalues, and encoding categorical variables.\\nModel training: In this stage, a machine learning model is trained on\\na prepared dataset. This may involve selecting a model type, tuning\\nhyperparameters, and training the model using an optimization\\nalgorithm.\\nModel evaluation: Once the model has been trained, it is important to\\nevaluate its performance to determine how well it generalizes to new\\ndata. This may involve tasks such as splitting the dataset into a\\ntraining set and a test set, evaluating the model’s performance on\\nthe test set, and comparing the results to a baseline model.\\n\\n🖊️ Inference pipeline\\n\\nModel loading: In this stage, a trained machine learning model is\\nloaded from storage and prepared for use. This may involve tasks such\\nas loading the model’s weights and any associated dependencies.\\nData preparation: In this stage, incoming data is cleaned and\\ntransformed into a format that is suitable for the model. This may\\ninvolve tasks such as normalizing numerical values and encoding\\ncategorical variables.\\nInference: In this final stage, the model is used to make predictions\\non the prepared data. This may involve tasks such as passing the data\\nthrough the model and processing the output to generate a final\\nprediction.\\n\\n\\nStep & pipeline\\n\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/introduction.html', 'title': 'Step & Pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nCreate a step — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nStep & Pipeline\\nCreate a step\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate a step\\uf0c1\\nA step is an atomic component defined by its input and output\\nparameters and by the processing it applies. Steps are the building\\nblocks of Pipelines. In practice, a step is a function with inputs and\\noutputs coded in Python. They are assembled with each other to create a\\ncomplete ML pipeline. The Python code (only available language for\\nthe moment) used by the step is stored on a Git repository.\\nAn input of a step is an object you can use inside the code.\\nAn output of a step is defined from the results of the step\\nfunction.\\nYou will be able to connect inputs & outputs of a step with another step\\nto compose a complete ML pipeline by using a directed acyclic graph\\n(DAG).\\nEach step is considered as a specific container that is executed on\\nKubernetes.\\nThe steps are stored in a specific environment, and only people with\\naccess to this environment can read and write the steps.\\n\\n\\nSummary\\uf0c1\\n\\nPrepare your code on Git repository\\nDefine step inputs and outputs\\nCreate a step\\n\\n\\n\\nFunction name\\nMethod\\nReturn type\\nDescription\\n\\n\\n\\nInput\\nInput(name, data_type=”string”, description=””, is_required=False, default_value=None)\\nInput SDK Object\\nCreate an Input object to give at create_steps() function for step a step input.\\n\\nOutput\\nOutput(name, data_type=”string”, description=””)\\nOutput SDK Object\\nCreate an Output object to give at create_steps() function for step a step output.\\n\\ncreate_step\\ncreate_step(step_name, function_path, function_name, repository_branch=None, description=None, timeout_s=180, container_config=None, inputs=None, outputs=None)\\nlist of dict[str, str]\\nCreate pipeline steps from a source code located on a remote repository.\\n\\n\\n\\n\\n\\nPrepare your code a Git repository\\uf0c1\\nPrerequisites: Before the creation of your first step, make sure you\\nhave already done this :\\n\\nSetup Project & Environment\\nGit repository link to the project\\n\\n❓ Why do you need to put your code on a Git repository ?\\nThis simplifies the access to the source code by the Craft AI platform.\\nIndeed, the platform will be able to directly fetch your code from the repository,\\nwithout the need for you to send it directly each time you change it,\\nyou just have to push it to your GitHub / GitLab repository.\\nCurrently, you can create a step via the Python SDK and not with\\ngraphical interface. But, after the creation, you will be able to see\\nthe step on the UI platform.\\nIf it’s not already done, put the code of the step into a GitHub / GitLab repository linked to the platform.\\nThe file with the entry function of your step can be anywhere in your Git repository.\\nExample tree file\\nin repo :\\n| requirements.txt\\n| src\\n   | my_entry_function_step.py\\n...\\n\\n\\nExample my_entry_function_step.py :\\nimport numpy as np\\n# and other import\\n\\ndef entryStep(dataX_input, dataY_input) :\\n\\n    # Some machine learning code\\n\\n    return result_output\\n\\n\\n\\n\\nDefine step inputs and outputs\\uf0c1\\nA step may need to receive some information or give some result (just\\nlike a function). To do that, we use Input and Output object. These\\nobjects allow defining the properties of the input or output that will\\nbe expected in the step. The input and output objects thus created must\\nbe given as a parameter of the step creation. Each input is defined as\\nan Input object and, each Output is defined as an Output object,\\nthrough a class available in the SDK.\\n\\nInput object definition\\uf0c1\\nfrom craft_ai_sdk.io import Input\\n\\nInput(\\n   name=\"*your_input_name*\",\\n   data_type=\"*your_io_data_type*\",\\n   description=\"\",\\n   is_required=True\\n   default_value=\"*default_value*\"\\n)\\n\\n\\n\\nParameters\\uf0c1\\n\\nname just a name for identifying the input later.\\ndata_type, one of the following possible types:\\n\\nfile: reference to binary data, equivalent to a file’s\\ncontent. If the input/output is not available, an empty stream.\\njson: JSON-serializable Python object. The following sub-types\\nare provided for more precise type checking, but they are all JSON\\nstring\\nnumber\\narray of JSON\\nboolean\\nIf the input/output is not available, None in Python\\n\\n\\n\\ndefault_value (optional) - If the parameter is empty, this value\\nwill be set by default. If a deployment receives an empty parameter\\nand already put a default value in the input, the default value of\\ndeployment will be keep.\\nis_required (optional, True by default) - Push an error is\\nthe input is empty.\\ndescription (optional) - This parameter precise what it’s\\nexpected in this input. It’s not read by the machine, it’s like a\\ncomment.\\n\\n\\n\\nReturn\\uf0c1\\nNo return\\n\\n\\n\\nOutput object definition\\uf0c1\\nfrom craft_ai_sdk.io import Output\\n\\nOutput(\\n   name=\"*your_input_name*\",\\n   data_type=\"*your_io_data_type*\",\\n   description=\"\",\\n)\\n\\n\\n\\nParameters\\uf0c1\\n\\nname just a name for identifying the input later.\\ndata_type, one of the following possible types:\\n\\nfile: reference to binary data, equivalent to a file’s\\ncontent. If the input/output is not available, an empty stream.\\njson: JSON-serializable Python object. The following sub-types\\nare provided for more precise type checking, but they are all JSON\\nstring\\nnumber\\narray of JSON\\nboolean\\n\\nIf the input/output is not available, None in Python\\n\\ndescription (optional) - This parameter precise what it’s\\nexpected in this input. It’s not read by the machine, it’s like a\\ncomment.\\n\\n\\n\\nReturn\\uf0c1\\nNo return\\n\\nNote\\nYou can use craft_ai_sdk.INPUT_OUTPUT_TYPES to get all possible types in Input and Output objects.\\nList of all possible types :\\n\\nARRAY = “array”\\nBOOLEAN = “boolean”\\nFILE = “file”\\nJSON = “json”\\nNUMBER = “number”\\nSTRING = “string”\\n\\nExample :\\nfrom craft_ai_sdk.io import Input, INPUT_OUTPUT_TYPES\\n\\nInput(\\n   name=\"inputName\",\\n   data_type=INPUT_OUTPUT_TYPES.JSON,\\n)\\n\\n\\n\\n\\n\\n\\nExample for input and output\\uf0c1\\nInput(\\n    name=\"inputName\",\\n    data_type=\"string\",\\n    description=\"A parameter for step input\",\\n    is_required=True,\\n    default_value=\"default_content_here\"\\n)\\n\\nOutput(\\n    name=\"inputName\",\\n    data_type=\"string\",\\n    description=\"A parameter for step input\",\\n)\\n\\n\\n\\nWarning\\nThe size of the I/O must not exceed 0.06MB (except for file type).\\n\\n\\n\\n\\nCreate a step\\uf0c1\\n\\nFunction definition\\uf0c1\\nCreate pipeline steps from a source code located on a remote repository.\\nsdk.create_step(\\n    function_path=\"src/my_reusable_funtion.py\",\\n    function_name=\"my_function\",\\n    inputs=[Input(...)],\\n    outputs=[Output(...)],\\n    [name=\"*your-custom-step-name*\"], # by default its the function name\\n    [description=\"*text with limit*\"],\\n    [repository_branch=\"*your-git-branch* or *your-git-tag*\"],\\n    [timeout_s=180]\\n    [container_config = {\\n        [language=\"python:3.8-slim\"],\\n        [repository_url=\"*your-git-url*\"],\\n        [repository_deploy_key=\"*your-private_key*\"],\\n        [requirements_path=\"*your-path-to-requirements*\"],\\n        [included_folders=[\"*your-list-of-path-to-sources*\"]],\\n        [system_dependencies=[\"package_1\", \"package_2\"]],\\n        [dockerfile_path=\"*your-dockerfile-path*\"],\\n}],\\n)\\n\\n\\n\\nParameters\\uf0c1\\n\\nfunction_path (str) – Path to access to the file who had the entry\\nfunction of the step.\\nfunction_name (str) – Function name of entry function step.\\ninputs (list<Input>) – List of step inputs.\\noutputs (list<Output>) – List of step outputs.\\nname (str) – Step name. By default, it’s the function name. The\\nname must be unique inside an environment and without special\\ncharacter ( - _ & / ? …)\\ndescription (str, optional) – Description of the step, it’s no use by\\nthe code, it’s only for user.\\nrepository_branch (str, optional) – Branch name for Git\\nrepository. Defaults to None.\\ntimeout_s (int, optional) – Maximum time to wait for the step to be created.\\n3min by default, and must be at least 2min.\\ncontainer_config (dict, optional) – Dict Python object where each key\\ncan override default parameter values for this step defined at\\nproject level.\\n\\nlanguage (str, optional) – Language of programming used inside the\\nstep. Defaults to python:3.8-slim. Versions python:3.9-slim and\\npython:3.10-slim are also available for CPU.\\nFor GPU enviromnent, python-cuda:3.8-11.3, python-cuda:3.9-11.3\\nand python-cuda:3.10-11.3 are available.\\nrepository_url (str, optional) – Remote repository URL.\\nrepository_deploy_key (str, optional) – Private SSH key related to\\nthe repository.\\nrequirements_path (str, optional) – Path to the file requirement\\nfor Python dependency.\\nincluded_folders (list, optional) – List of folders that need to be\\naccessible from step code.\\nsystem_dependencies (list, optional) – List of APT Linux packages\\nto install.\\ndockerfile_path (str, optional) – Path to a docker-file for having\\na custom config in step. (see the part after for more detail)\\n\\n\\n\\n\\nNote\\nThe repository_branch parameters as well as the container_config elements (except dockerfile_path) can take one of the STEP_PARAMETER object’s values in addition to theirs.\\nIn fact, STEP_PARAMETER allows us to specify at the step level whether we want to take the project’s values (default behavior) or define a null value:\\n\\nSTEP_PARAMETER.FALLBACK_PROJECT : Allows to take the value defined in the project parameters (default behavior if the field is not defined).\\nSTEP_PARAMETER.NULL : Allows to set the field to null value and not to take the value defined in the project.\\n\\nExample with a code step that does not need a requirement.txt and does not take the one defined in the project settings:\\nfrom craft_ai_sdk import STEP_PARAMETER\\n\\n# Code for init SDK here ...\\n\\nsdk.create_step(\\n  function_path=\"src/helloWorld.py\",\\n  function_name=\"helloWorld\",\\n  step_name=\"stepName\",\\n  container_config = {\\n      \"requirements_path\": STEP_PARAMETER.NULL,\\n   }\\n)\\n\\n\\n\\n\\nWarning\\nThe size of the embedded code from your repository must not exceed 5MB.\\nYou can select the part of your repository to import using the included_folders parameter.\\nIf the data you want to import is larger than 5MB, you can use the data store to store it and then import it into your step.\\n\\n\\nNote\\nWhen using a GPU environment, the language parameter must be python-cuda:3.X-11.3 with X equal to 8, 9 or 10 representing the version of python.\\nWithout this, the step will not be able to benefit from GPU computing using cuda.\\n\\n\\n\\nReturns\\uf0c1\\nThe return type is a dict with the following keys :\\n\\nparameters (dict): Information used to create the step with the following keys:\\n\\nstep_name (str): Name of the step.\\nfunction_path (str): Path to the file that contains the function.\\nfunction_name (str): Name of the function in that file.\\nrepository_branch (str): Branch name.\\ndescription (str): Description.\\ninputs (list of dict): List of inputs represented as a dict with the following keys:\\n\\nname (str): Input name.\\ndata_type (str): Input data type.\\nis_required (bool): Whether the input is required.\\ndefault_value (str): Input default value.\\n\\n\\noutputs (list of dict): List of outputs represented as a dict with the following keys:\\n\\nname (str): Output name.\\ndata_type (str): Output data type.\\ndescription (str): Output description.\\n\\n\\ncontainer_config (dict[str, str]): Some step configuration, with the following optional keys:\\n\\nlanguage (str): Language and version used for the step.\\nrepository_url (str): Remote repository url.\\nincluded_folders (list[str]): List of folders and files in the repository required for the step execution.\\nsystem_dependencies (list[str]): List of system dependencies.\\ndockerfile_path (str): Path to the Dockerfile.\\nrequirements_path (str): Path to the requirements.txt file.\\n\\n\\n\\n\\ncreation_info (dict): Information about the step creation:\\n\\ncreated_at (str): The creation date in ISO format.\\nupdated_at (str): The last update date in ISO format.\\ncommit_id (str): The commit id on which the step was built.\\nstatus (str): The step status, if the step creation process is under 2m40s (most of the time it is), is always Ready when this function returns.\\n\\n\\n\\n\\nNote\\nOnce our step is created, we need to create the pipeline that wraps\\nthe step. It is mandatory to create a pipeline once the step is created\\nto be able to use it later. This technical choice was made in\\nanticipation of future multistep functionality. This forces the use of a\\npipeline to contain the steps.\\n\\n\\n\\n\\nExample: Create step from scratch\\uf0c1\\nFunction usage\\nfrom craft_ai_sdk import Input, Output\\n\\ninput1 = Input(\\n    name=\"input1\",\\n    data_type=\"string\",\\n    description=\"A parameter named input1, its type is a string\",\\n    is_required=True,\\n)\\n\\ninput2 = Input(\\n    name=\"input2\",\\n    data_type=\"file\",\\n    description=\"A parameter named input2, its type is a file\"\\n)\\n\\ninput3 = Input(\\n    name=\"input3\",\\n    data_type=\"number\",\\n)\\n\\nprediction_output = Output(\\n    name=\"prediction\",\\n    data_type=\"file\",\\n    default_value=\"default,content,here\",\\n)\\n\\nstep = sdk.create_step(\\n        function_path=\"src/my_reusable_funtion.py\",\\n        function_name=\"my_function\",\\n    inputs_list=[input1, input2, input3],\\n        outputs_list=[prediction_output],\\n    description=\"Apply the model to the sea\",\\n        ## ...\\n)\\n\\n\\n\\nNote\\nIf you need to create a step with more specific configuration, you can do this with a custom dockerfile, more detail about here.\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nManage a step — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nStep & Pipeline\\nManage a step\\n\\n\\n\\n\\n\\n\\n\\n\\nManage a step\\uf0c1\\nSummary:\\n\\nFind and get information about steps\\nDelete steps\\n\\n\\n\\nFunction Name\\nMethod\\nReturn Type\\nDescription\\n\\n\\n\\nget_step\\nget_step (step_name)\\ndict\\nGet information about a step.\\n\\nlist_steps\\nlist_steps()\\nlist of dict\\nGet a list of available steps.\\n\\ndelete_step\\ndelete_step (step_name)\\ndict[str, str]\\nDelete one step.\\n\\n\\n\\n❓ For step update and deletion, you need the name (the name you provide\\nwhen creating the step) of the step you want to update/delete. You can\\nfind it with function list_steps() (see the previous part).\\n\\nFind and get information about steps\\uf0c1\\nTo get information about a step, we need its name in the environment.\\nYou can search its name in the list of step’s name of the environment.\\n\\nGet list of steps\\uf0c1\\n\\nFunction definition\\uf0c1\\nTo get all steps available in the current environment, you can get a\\nlist of step name with this function:\\nCraftAiSdk.list_steps()\\n\\n\\n\\n\\nReturns\\uf0c1\\nList of steps represented as dict with\\nthe following keys:\\n\\nname (str): Name of the step.\\nstatus (str): either Pending or Ready.\\ncreated_at (str): The creation date in ISO format.\\nupdated_at (str): The last update date in ISO format.\\nrepository_branch (str): The branch of the\\nrepository where the step was built.\\nrepository_url (str): The url of the repository\\nwhere the step was built.\\ncommit_id (str): The commit id on which the step was\\nbuilt.\\n\\n\\n\\n\\nGet information about one step\\uf0c1\\n\\nFunction definition\\uf0c1\\nGet all information (repository, dependency, …) about one step in the\\ncurrent environment with its name.\\nCraftAiSdk.get_step(step_name)\\n\\n\\n\\n\\nParameters\\uf0c1\\n\\nstep_name (str) – The name of the step to get.\\n\\n\\n\\nReturns\\uf0c1\\ndict: None if the step does not exist; otherwise\\nthe step information, with the following keys:\\n\\nparameters (dict): Information used to create the step with the following keys:\\n\\nstep_name (str): Name of the step.\\nfunction_path (str): Path to the file that contains the function.\\nfunction_name (str): Name of the function in that file.\\nrepository_branch (str): Branch name.\\ndescription (str): Description.\\ninputs (list of dict): List of inputs represented as a dict with the following keys:\\n\\nname (str): Input name.\\ndata_type (str): Input data type.\\nis_required (bool): Whether the input is required.\\ndefault_value (str): Input default value.\\n\\n\\noutputs (list of dict): List of outputs represented as a dict with the following keys:\\n\\nname (str): Output name.\\ndata_type (str): Output data type.\\ndescription (str): Output description.\\n\\n\\ncontainer_config (dict[str, str]): Some step configuration, with the following optional keys:\\n\\nlanguage (str): Language and version used for the step.\\nrepository_url (str): Remote repository url.\\nincluded_folders (list[str]): List of folders and files in the repository required for the step execution.\\nsystem_dependencies (list[str]): List of system dependencies.\\ndockerfile_path (str): Path to the Dockerfile.\\nrequirements_path (str): Path to the requirements.txt file.\\n\\n\\n\\n\\ncreation_info (dict): Information about the step creation:\\n\\ncreated_at (str): The creation date in ISO format.\\nupdated_at (str): The last update date in ISO format.\\ncommit_id (str): The commit id on which the step was built.\\nstatus (str): Either “Pending” or “Ready”.\\n\\n\\n\\n\\n\\n\\n\\nDelete steps\\uf0c1\\n\\nDelete steps function\\uf0c1\\n\\nFunction definition\\uf0c1\\nDelete step in the environment with his name.\\nCraftAiSdk.delete_step(step_name, force_dependents_deletion=False)\\n\\n\\n\\n\\nParameters\\uf0c1\\n\\nstep_name (str) – Name of the step to delete as defined in the create step function.\\nforce_dependents_deletion (bool, optional) – if True the\\nassociated step’s dependencies will be deleted too (pipeline,\\npipeline executions, deployments). Defaults to False.\\n\\n\\n\\nReturns\\uf0c1\\nDeleted step represented as dict (with key “name”). The return\\ntype is a dict [str, str].\\n\\nWarning\\nYou can’t delete a step that is used in a pipeline.\\nYou must delete the pipeline before or use the force_dependents_deletion parameter during step deletion.\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/manageStep.html', 'title': 'Manage a step — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nCompose a pipeline — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nStep & Pipeline\\nCompose a pipeline\\n\\n\\n\\n\\n\\n\\n\\n\\nCompose a pipeline\\uf0c1\\nA pipeline is a machine learning workflow, consisting of one or more\\nsteps, to deploy code using Docker containers. By specifying the\\noutput of one step as the input of another step, the user can create a\\nfull pipeline formed with a computed acyclic graph (DAG).\\nℹ️ Info: For the moment, the pipelines are only single-step. We are\\nactively working on the integration of multi-steps for the next versions\\nof the platform.\\nTo deploy ML code in production, you need to go through a pipeline. So\\nyou have to go through a single-step pipeline in any case to perform an\\nendpoint or another type of deployment.\\nSummary:\\n\\nCompose a mono-step pipeline\\nDelete a pipeline\\nFind and get pipeline information\\n\\n\\n\\nFunction name\\nMethod\\nReturn type\\nDescription\\n\\n\\n\\ncreate_pipeline\\ncreate_pipeline(pipeline_name, step_name)\\ndict[str, str]\\nCreate a pipeline containing a single step.\\n\\nget_pipeline\\nget_pipeline(pipeline_name)\\ndict\\nGet a single pipeline if it exists.\\n\\ndelete_pipeline\\ndelete_pipeline(pipeline_name, force_deployments_deletion=False)\\ndict\\nDelete a pipeline identified by its name and ID.\\n\\nlist_pipelines\\nlist_pipelines()\\nlist of dict\\nGet the list of all pipelines.\\n\\n\\n\\n\\nCreate a mono-step pipeline\\uf0c1\\nBefore creating a pipeline, the step creation must be finished.\\nYou can check this by checking that the step’s status are equal to Ready using the get_step() function.\\n\\nSDK function pipeline\\uf0c1\\n\\nFunction definition\\uf0c1\\npipeline = sdk.create_pipeline(\\n   pipeline_name=\"my_pipeline\",\\n   step_name=\"my_step\",\\n)\\n\\n\\n\\n\\nParameters\\uf0c1\\n\\npipeline_name (str) – Name of the pipeline to create.\\nstep_name (str) – Name of the step to include in the\\npipeline.\\n\\nNote\\nThe step should have the status “Ready” before being used to create the pipeline.\\n\\n\\n\\n\\n\\nReturns\\uf0c1\\nCreated pipeline represented as dict using this keys:\\n\\npipeline_name (str): Pipeline name.\\ncreated_at (str): Pipeline date of creation.\\nsteps (list[str]): List of step names.\\nopen_inputs (list[dist]): List of all input of step.\\n\\ninput_name (str): Name of the open input.\\nstep_name (str): Name of the step that provides the open input.\\ndata_type (str): Data type of the open input.\\ndescription (str): Description of the open input.\\ndefault_value (str): Default value of the open input.\\nis_required (bool): Whether the open input is required or not.\\n\\n\\nopen_outputs (list[dist]): List of all output of step.\\n\\noutput_name (str): Name of the open output.\\nstep_name (str): Name of the step that provides the open output.\\ndata_type (str): Data type of the open output.\\ndescription (str): Description of the open output.\\n\\n\\n\\n\\n\\n\\nInformation about pipeline store\\uf0c1\\nPipeline can have multiple inputs and outputs, or no one. In fact, it’s\\ndependent on the inputs and outputs of step inside. All input and output\\nwill be the same as the step inside. So, you have nothing to configure\\non pipeline creation for input and output.\\n\\n\\n\\nGet information pipeline\\uf0c1\\n\\nGet information about one pipeline\\uf0c1\\n\\nFunction definition\\uf0c1\\nGet all information about one pipeline, referred by its name.\\nCraftAiSdk.get_pipeline(pipeline_name)\\n\\n\\n\\n\\nParameters\\uf0c1\\n\\npipeline_name (str) – Name of the pipeline to get.\\n\\n\\n\\nReturns\\uf0c1\\nThe pipeline information in a dict (or None if the pipeline does not\\nexist), with the following keys:\\n\\npipeline_name (str): Pipeline name.\\ncreated_at (str): Pipeline date of creation.\\ncreated_by (str): ID of the user who created the deployment.\\nlast_execution_id (str): ID of the last execution of the pipeline.\\nsteps (list[str]): List of step names.\\ndeployments (list[str]): List of deployment names which are associated to the pipeline.\\nopen_inputs (list[dist]): List of all input of step.\\n\\ninput_name (str): Name of the open input.\\nstep_name (str): Name of the step that provides the open input.\\ndata_type (str): Data type of the open input.\\ndescription (str): Description of the open input.\\ndefault_value (str): Default value of the open input.\\nis_required (bool): Whether the open input is required or not.\\n\\n\\nopen_outputs (list[dist]): List of all output of step.\\n\\noutput_name (str): Name of the open output.\\nstep_name (str): Name of the step that provides the open output.\\ndata_type (str): Data type of the open output.\\ndescription (str): Description of the open output.\\n\\n\\n\\n\\n\\n\\nGet all pipelines\\uf0c1\\n\\nFunction definition\\uf0c1\\nGet the list of all pipelines on your environment.\\nCraftAiSdk.list_pipelines()\\n\\n\\n\\n\\nReturns\\uf0c1\\nList of pipelines represented as dict with keys :\\n\\n“pipeline_name” (str): Name of pipeline\\n“created_at” (str): Create date of the pipeline.\\n\\n\\n\\n\\n\\nDelete a pipeline\\uf0c1\\n\\nFunction definition\\uf0c1\\nDelete a pipeline from the current environment.\\nCraftAiSdk.delete_pipeline(pipeline_name, force_deployments_deletion=False)\\n\\n\\n\\nParameters\\uf0c1\\n\\npipeline_name (str) – Name of the pipeline.\\nforce_deployments_deletion (bool, optional) – if True, the associated\\nendpoints will be deleted too. Defaults to False.\\n\\n\\n\\nReturns\\uf0c1\\nThe deleted pipeline and associated deleted endpoints in a dict. The\\nreturned dict contains two keys:\\n\\n“pipeline”(dict): Deleted pipeline represented as dict (with keys\\n“id” and “name”).\\n“endpoints” (list): List of deleted endpoints represented as dict\\n(with keys “id” and “name”).\\n\\n\\nWarning\\nYou can’t delete a pipeline that is used in a Deployment\\n\\n\\n\\n\\n\\nRun a pipeline\\uf0c1\\nNow that your pipeline is created you can run it  directly from the SDK.\\nOr you can configure a deployment.\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/composePipeline.html', 'title': 'Compose a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nDeployment — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\n\\n\\n\\n\\n\\n\\n\\n\\nDeployment\\uf0c1\\nA deployment is a way to trigger an execution of a Machine Learning\\npipeline in a repeatable and automated way. Each pipeline can be\\nassociated with multiple deployments.\\nFor each deployment, you can use 2 distinct execution rules :\\n\\nby endpoint (web API)\\nby periodic trigger (CRON)\\n\\nIn addition, for each deployment, you will need to connect the pipeline\\ninputs and outputs with the desired sources and\\ndestinations. When one of the deployment conditions is met, the\\npipeline is executed by using the computing resources available in\\nits environment.\\nThe results of the execution (predictions, metrics, data, …) can be\\nstored in the data store of the environment and can be easily\\nretrieved by the users. You can find all the information about the\\nexecutions in the execution tracking section.\\n\\nIn addition to the deployment functionality, it is possible to run a\\npipeline directly without deploying it. This allows you to run your\\npipeline on the fly without having to create a specific deployment,\\nwhich is very useful during the experimentation phase.\\nThe main objectives of the deployments are :\\n\\nNo longer take 6 months to deploy ML models in production but a few\\nclicks!\\nAutomating the execution of the pipelines to save time for Data\\nScience teams\\nCreating secured web API to deliver pipeline results to external\\nusers without any DevOps skills\\nAutomatically triggering re-training pipelines when model\\nperformance drops\\nVisualizing pipeline executions, experiment tracking, and ML\\nartifacts\\n\\n\\nDeployment\\n\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/introduction.html', 'title': 'Deployment — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nDeployment — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\n\\n\\n\\n\\n\\n\\n\\n\\nDeployment\\uf0c1\\nA deployment is a way to trigger an execution of a Machine Learning\\npipeline in a repeatable and automated way. Each pipeline can be\\nassociated with multiple deployments.\\nFor each deployment, you can use 2 distinct execution rules :\\n\\nby endpoint (web API)\\nby periodic trigger (CRON)\\n\\nIn addition, for each deployment, you will need to connect the pipeline\\ninputs and outputs with the desired sources and\\ndestinations. When one of the deployment conditions is met, the\\npipeline is executed by using the computing resources available in\\nits environment.\\nThe results of the execution (predictions, metrics, data, …) can be\\nstored in the data store of the environment and can be easily\\nretrieved by the users. You can find all the information about the\\nexecutions in the execution tracking section.\\n\\nIn addition to the deployment functionality, it is possible to run a\\npipeline directly without deploying it. This allows you to run your\\npipeline on the fly without having to create a specific deployment,\\nwhich is very useful during the experimentation phase.\\nThe main objectives of the deployments are :\\n\\nNo longer take 6 months to deploy ML models in production but a few\\nclicks!\\nAutomating the execution of the pipelines to save time for Data\\nScience teams\\nCreating secured web API to deliver pipeline results to external\\nusers without any DevOps skills\\nAutomatically triggering re-training pipelines when model\\nperformance drops\\nVisualizing pipeline executions, experiment tracking, and ML\\nartifacts\\n\\n\\nDeployment\\n\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/introduction.html', 'title': 'Deployment — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nChoose an execution rule — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\nChoose an execution rule\\n\\n\\n\\n\\n\\n\\n\\n\\nChoose an execution rule\\uf0c1\\nA deployment is a way to run a Machine Learning pipeline in a\\nrepeatable and automated way.\\nFor each deployment, you can configure an execution rule:\\n\\nby endpoint (web API) : the pipeline will be executed by a call\\nto a web API. In addition, this API will allow, if necessary, to\\nretrieve data as input and deliver the result of the pipeline as\\noutput. Access to the API can be securely communicated to external\\nusers.\\nby periodic trigger (CRON) : rules can be configured to trigger\\nthe pipeline periodically.\\n\\n\\nSummary\\uf0c1\\n\\nDeploy with execution rule: Endpoint\\nDeploy with execution rule: Periodic\\n\\n\\n\\nFunction name\\nMethod\\nReturn type\\nDescription\\n\\n\\n\\ncreate_deployment\\ncreate_deployment(name, pipeline_name, execution_rule, outputs_mapping=[], inputs_mapping=[], description)\\nDict\\nFunction that deploys a pipeline by creating a deployment which allows a user to trigger the pipeline execution\\n\\n\\n\\n\\n\\nDeploy with execution rule: Endpoint\\uf0c1\\n\\nDefinition function\\uf0c1\\nTo create an auto-mapping deployment where all inputs and outputs are\\nbased on API calls, you can use the create_deployment function. To\\ncreate a deployment with manual mapping, you can use the\\ncreate_deployment function with the additional parameters\\ninputs_mapping to specify the precise mapping between input and\\nsource.\\n\\nWarning\\nOutputs mapping always needs to be precised. Auto-mapping is only\\navailable for inputs.\\n\\n\\nParameters\\uf0c1\\n\\ndeployment_name (str) – Name of endpoint chosen by the user to\\nrefer to the endpoint\\npipeline_name (str) – Name of pipeline that will be run by the\\ndeployment / endpoint\\nexecution_rule (str) - Execution rule of the deployment. Must be\\n“endpoint” or “periodic”. For convenience, members of the\\nenumeration DEPLOYMENT_EXECUTION_RULES could be used too.\\ndescription (str, optional) – Text description of usage of\\npipeline for user only\\noutputs_mapping (List) - List of all OutputDestination objects\\nwith information for each output mapping.\\ninputs_mapping (List, optional) - List of input mappings, to map\\npipeline inputs to different sources (such as constant values,\\nendpoint inputs, data store or environment variables). See\\nInputSource for more details. For\\nendpoint rules, if an input of the step in the pipeline is not\\nexplicitly mapped, it will be automatically mapped to an endpoint\\ninput with the same name.\\ndescription (str, optional) – Description of the deployment.\\n\\n\\n\\nReturns\\uf0c1\\nInformation about the deployment just create in a dict Python format. In\\nthis data, you will have :\\n\\nname - Name of the deployment\\nendpoint_token - Token of the endpoint used to trigger the\\ndeployment. Note that this token is only returned if\\nexecution_rule is “endpoint”.\\ninputs_mapping - Information about the mapping between the\\nsources of the deployment and the inputs of the pipeline\\n\\nstep_input_name - Name of the input in the pipeline\\nendpoint_input_name - Name of the source in the deployment\\nconstant_value - Default value for the input if the source\\nis missing\\nenvironment_variable_name - The name of an environment\\nvariable\\nis_null - A flag indicating that the input should not be\\nprovided any value at execution time\\n\\n\\noutputs_mapping - Information about the mapping between the\\ndestinations of the deployment and the outputs of the pipeline\\n\\nstep_output_name - Name of the output in the pipeline\\nendpoint_output_name - Name of the destination in the\\ndeployment\\nis_null - This flag indicates that the output will be\\ndeleted after execution.\\n\\n\\npipeline - Information about the pipeline that will be run by\\nthe deployment\\n\\nname - Name of the pipeline\\n\\n\\n\\n\\n\\n\\nExample\\uf0c1\\n\\nExample auto mapping\\uf0c1\\nsdk.create_deployment(\\n   deployment_name=\"my_deployment\",\\n   pipeline_name=\"my_pipeline\",\\n    execution_rule=\"endpoint\",\\n   outputs_mapping=[],\\n   inputs_mapping=[],\\n)\\n\\n\\n\\n\\nExample manual mapping\\uf0c1\\nsdk.create_deployment(\\n   deployment_name=\"my_deployment\",\\n   pipeline_name=\"my_pipeline\",\\n    execution_rule=\"endpoint\",\\n    inputs_mapping=[\\n                seagull_endpoint_input,\\n                big_whale_input,\\n                salt_constant_input,\\n        ],\\n   outputs_mapping=[prediction_endpoint_ouput],\\n)\\n\\n\\n\\n\\nExample of return object\\uf0c1\\n{\\n    \\'id\\': \\'29d0c371-49ec-4071-a879-10cdab00fda4\\', \\n    \\'name\\': \\'my_deployment\\', \\n    \\'endpoint_token\\': \\'EWwIii [...] kjdD5Q\\', \\n    \\'inputs_mapping\\': \\n    [{\\n        \\'step_input_name\\': \\'seagull\\', \\n        \\'endpoint_input_name\\': \\'sourceData\\'\\n    },\\n    {\\n        \\'step_input_name\\': \\'big_whale\\', \\n        \\'constant_value\\': 5\\n    },], \\n    \\'outputs_mapping\\': \\n    [{\\n        \\'step_output_name\\': \\'resultMulti\\', \\n        \\'endpoint_output_name\\': \\'resultMulti\\'\\n    }], \\n    \\'pipeline\\': {\\n        \\'name\\': \\'my_pipeline\\'\\n    }\\n}\\n\\n\\n\\n\\n\\n\\nDeploy with execution rule: Periodic\\uf0c1\\n\\nDefinition function\\uf0c1\\nTo create an auto-mapping deployment where all inputs and outputs are\\nbased on periodicity, you can use the create_deployment function. To\\ncreate a deployment with manual mapping, you can use the\\ncreate_deployment function with the additional parameters\\ninputs_mapping to specify the precise mapping between input and\\nsource.\\n\\nWarning\\nInput and output mapping must always be precise. Auto mapping isn’t\\navailable for periodic deployment.\\n\\n\\nParameters\\uf0c1\\n\\ndeployment_name (str) – Name of the deployment chosen\\npipeline_name (str) – Name of pipeline that will be run by the\\ndeployment\\ndescription (str, optional) – Text description of usage of\\npipeline for user only.\\nexecution_rule (str) - Execution rule of the deployment. Must be\\n“endpoint” or “periodic”. For convenience, members of the\\nenumeration DEPLOYMENT_EXECUTION_RULES could be used too.\\nschedule (str, optional) - Schedule of the deployment. Only required if execution_rule is “periodic”.\\nMust be a valid: cron expression.\\nThe deployment will be executed periodically according to this\\nschedule. The schedule must follow this format:\\n<minute> <hour> <day of month> <month> <day of week>. Note\\nthat the schedule is in UTC time zone. “*” means all possible\\nvalues. Here are some examples:\\n\\n\"0 0 * * *\" will execute the deployment every day at midnight.\\n\"0 0 5 * *\" will execute the deployment every 5th day of the month at midnight.\\n\\n\\ninputs_mapping (List of instances of [InputSource], optional) - List of input mappings, to map pipeline inputs to different :\\nsources (such as constant values, endpoint inputs, or\\nenvironment variables). See InputSource for more details.\\nFor endpoint rules, if an input\\nof the step in the pipeline is not explicitly mapped, it will be\\nautomatically mapped to an endpoint input with the same name.\\nFor periodic rules, all inputs of the step in the pipeline must\\nbe explicitly mapped.\\noutputs_mapping (List of instances of [OutputDestination], optional) - List of output mappings, to map pipeline outputs to different :\\ndestinations. See OutputDestination for more details.\\nFor endpoint execution rules, if an output\\nof the step in the pipeline is not explicitly mapped, it will be\\nautomatically mapped to an endpoint input with the same name.\\nFor other rules, all outputs of the step in the pipeline must\\nbe explicitly mapped.\\ndescription (str, optional) – Description of the deployment.\\n\\n\\n\\nReturns\\uf0c1\\nInformation about the deployment just create in a dict Python format.\\n\\nid - Unique identifier of the deployment\\nname - Name of the deployment\\nschedule - Schedule of the deployment. Note that this schedule\\nis only returned if execution_rule is “periodic”.\\nhuman_readable_schedule - Human readable schedule of the\\ndeployment. Note that this schedule is only returned if\\nexecution_rule is “periodic”.\\ninputs_mapping - Information about the mapping between the\\nsources of the deployment and the inputs of the pipeline\\n\\nstep_input_name - Name of the input in the pipeline\\nendpoint_input_name - Name of the source in the deployment\\nconstant_value - Default value for the input if the source\\nis missing\\nenvironment_variable_name - The name of an environment\\nvariable\\nis_null - A flag indicating that the input should not be\\nprovided any value at execution time\\n\\n\\noutputs_mapping - Information about the mapping between the\\ndestinations of the deployment and the outputs of the pipeline\\n\\nstep_output_name - Name of the output in the pipeline\\nendpoint_output_name - Name of the destination in the\\ndeployment\\nis_null - This flag indicates that the output will be\\ndeleted after execution.\\n\\n\\npipeline - Information about the pipeline that will be run by\\nthe deployment\\n\\nname - Name of the pipeline\\n\\n\\n\\n\\n\\n\\nExample\\uf0c1\\nSet up deployment to be triggered automatically every 14 days.\\nsdk.create_deployment(\\n   deployment_name=\"my_deployment\",\\n   pipeline_name=\"my_pipeline\",\\n   execution_rule=\"periodic\",\\n   schedule=\"0 14 * * *\"\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/chooseExecutionRule.html', 'title': 'Choose an execution rule — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nDefine the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\nDefine the pipeline sources and destinations\\n\\n\\n\\n\\n\\n\\n\\n\\nDefine the pipeline sources and destinations\\uf0c1\\nA deployment is a way to run a Machine Learning pipeline in a\\nrepeatable and automated way. First, you have to choose one of the 2\\ndeployments methods.\\nThen, you need to connect the pipeline inputs and outputs with\\nthe desired sources and destinations.\\nSources : This is the origin of the data that you want to connect to\\nthe pipeline inputs. The data can come from the data store, from\\nenvironment variables, from constants or from the endpoint (if this\\ndeployment method has been chosen).\\nDestinations : This is the data drop point that you want to connect\\nto the pipeline outputs. The data can go to the data store or to the\\nendpoint (if this deployment method has been chosen).\\n\\n\\nSummary\\uf0c1\\n\\nGeneral function of I/O mapping\\nCreate input mapping\\nCreate output mapping\\n\\n\\n\\nObjects name\\nConstructor\\nReturn type\\nDescription\\n\\n\\n\\nInputSource\\nInputSource(step_input_name, required=False, default=None)\\nInputSource Object\\nCreate a mapping source object for deployment.\\n\\nOutputDestination\\nOutputDestination(step_output_name, endpoint_output_name, required=False, default=None)\\nOutputDestination Object\\nCreate a mapping destination object for deployment.\\n\\n\\n\\n\\n\\nGeneral function of the I/O mapping\\uf0c1\\n\\nMapping rules\\uf0c1\\nWhen you start a new deployment, the data flow is configured with a\\nmapping. You can create this mapping in two ways: auto mapping (only\\navailable with endpoint trigger) or manual mapping in the SDK or UI.\\nAuto mapping automatically maps all inputs to endpoint variables for\\nsources. If you need a different mapping or another trigger, you must\\nmap your inputs and outputs manually.\\nAuto mapping Example\\n\\nFor each input / output, you had defined a data type in step. This data\\ntype will be the same as the mapped step input / output. You need to map\\nthe good source and destination with the good data type :\\n\\nEndpoint (input and output) → Variable (string, number, …) and\\nfile\\nData store → Only file\\nConstant → Only variable (string, number, …)\\nEnvironment variable → Only variable (string, number, …)\\nNone → Variable (string, number, …) and file\\n\\n\\n\\nLimitations\\uf0c1\\nThe inputs (as outputs) of a step mapped through the endpoint (default\\nmapping) can consist of only a single file or multiple variables. Thus,\\nif you use deployment by triggering an endpoint, you cannot have\\nmultiple files as inputs or outputs due to a technical limitation of the\\nAPI calls.\\nTo have multiple files as the inputs or outputs of an endpoint, you can\\ncompress the files into a single file (for example, with the\\n[tar]{.title-ref} command) to be sent in the API call.\\n\\nExamples: You can do\\uf0c1\\n\\n\\nWarning\\nThe data store is not yet available for the input/output mapping, but\\nit’s coming soon, so stay tuned.\\n\\n\\n\\n\\n\\nExamples: You can’t do\\uf0c1\\n\\n\\n\\n\\n\\n\\n\\nCreation input mapping\\uf0c1\\n\\nImport Dependencies\\uf0c1\\nBefore creating a mapping between the input/output of a pipeline and the\\nsources/destinations of an endpoint, you need to import the InputSource\\nand OutputDestination objects from the SDK.\\nfrom craft_ai_sdk.io import InputSource, OutputDestination\\n\\n\\n\\n\\nFunction Definition\\uf0c1\\nFor each input of your pipeline, in manual mapping, you need to create a\\nmapping object that will be given to the create_deployment() function.\\nboat_endpoint_input = InputSource(\\n    step_input_name=\"apply_model\",\\n\\n    ## Choose from one of these parameters\\n    endpoint_input_name=\"boat\", ## For mapping with an endpoint\\n    environment_variable_name=None, ## For mapping with an environment variable\\n    constant_value=None, ## For mapping with a constant value\\n    is_null=True ## For mapping with None\\n\\n    is_required=True,\\n    default_value=\"empty\",\\n)\\n\\n\\n\\nParameters\\uf0c1\\n\\nstep_input_name (str): name of the input at the step level to\\nwhich the deployment input will be linked to\\n\\nDifferent possible sources:\\n\\nendpoint_input_name (str, optional): name of the input at the\\nendpoint level to which the step input will be linked to\\nenvironment_variable_name (str, optional): name of the environment\\nvariable to which the step input will be linked to\\nconstant_value (Any, optional): a constant value to which the step\\ninput will be linked to\\n\\nOther parameters:\\n\\nis_null (True, optional): if specified, the input will not take\\nany value at execution time\\ndefault_value (Any, optional): this parameter can only be\\nspecified if the deployment is an endpoint. In this case, if nothing\\nis passed at the endpoint level, the step input will take the\\ndefault_value\\nis_required (bool, optional): this parameter can only be specified\\nif the deployment is an endpoint.If set to True, the corresponding\\nendpoint input should be provided at execution time.\\n\\n\\n\\nReturn\\uf0c1\\nAn InputSource object that can be used in the deployment creation in the\\ndictionary format.\\n\\n\\n\\nAdditional parameters for source definition\\uf0c1\\nBy default, the source is configured as an endpoint parameter, but you\\ncan configure a different source for your mapping. For each source, you\\nhave to add 1 parameter to :\\n\\nDefine the type of source with the name of parameter added\\nPrecise element about the source\\n\\nWe will list all parameters you can have in your input mapping.\\n\\nWarning\\nYou can only add 1 parameter of source definition. By default, it’s\\nalways endpoint source that is configured.\\n\\n\\nEndpoint source\\nParameter name : endpoint_input_name\\nSource from : Outside through the endpoint\\nValue to put in parameter : name of the input received by the\\nendpoint in the body of the HTTP call\\nExample :\\nendpoint_input = InputSource(\\n    step_input_name=\"seagull\",\\n    endpoint_input_name=\"seagull\",\\n    required=False, #optional \\n    default=\"Eureka\", #optional\\n)\\n\\n\\n\\nData store source\\nParameter name : datastore_path\\nSource from : file content from the data store\\nValue to put in parameter : path to a data store file\\nExample :\\ndata_store_input = InputSource(\\n    step_input_name=\"trainingData\",\\n    datastore_path=\"path/to/trainingData.csv\",\\n)\\n\\n\\nExample step code to read file :\\ndef stepFileIO (trainingData) :\\n   print (trainingData)\\n\\n   with open(trainingData[\"path\"]) as f:\\n      contents = f.readlines()\\n      print (contents)\\n\\n\\n\\nConstant source\\nParameter name : constant_value\\nSource from : static value\\nValue to put in parameter : direct value\\nExample :\\nconstant_input = InputSource(\\n  step_input_name=\"salt\",\\n    constant_value=3,\\n)\\n\\n\\n\\nEnvironment variable\\nParameter name : environment_variable_name\\nSource from : the variables set at the level of an Environment\\nin the platform\\nValue to put in parameter : name of the environment variable\\nExample :\\nenv_var_input = InputSource(\\n  step_input_name=\"fish\",\\n    environment_variable_name=\"nameOfEnvVar\",\\n)\\n\\n\\n\\nNone value\\nParameter name : no_value\\nDestination to : void\\nValue to put in parameter : True\\nExample :\\nnull_input = InputSource(\\n  step_input_name=\"fish\",\\n    is_null=True\\n)\\n\\n\\n\\n\\n\\n\\n\\nCreate output mapping\\uf0c1\\n\\nImport dependency\\uf0c1\\nBefore creating mapping between input / output of pipeline and sources /\\ndestination of endpoint, you have to import InputSource and\\nOutputDestination objects from SDK.\\nfrom craft_ai_sdk.io import InputSource, OutputDestination\\n\\n\\n\\n\\nFunction definition\\uf0c1\\nFor each output of your pipeline, in manual mapping, you have to create\\nan object, that will be given to create_deployment() function.\\nendpoint_output = OutputDestination(\\n    step_output_name=\"pred_0\",\\n\\n    ## Choose from one of these parameters\\n    endpoint_output_name=\"pred_0\", ## For mapping with an endpoint\\n    is_null=True ## For mapping with None\\n)\\n\\n\\n\\nParameters\\uf0c1\\n\\nstep_output_name (str) - the specific output in the step\\nendpoint_output_name (str, optional) – Name of the endpoint\\noutput to which the output is mapped.\\nis_null (True, optional) – If specified, the output is not\\nexposed as a deployment output.\\n\\n\\n\\nReturn\\uf0c1\\nAn OutputSource object who can be used in the deployment creation.\\n\\n\\n\\nAdditional parameter for destination definition\\uf0c1\\nBy default, the destination is configured as an endpoint parameter, but\\ncan configure a different source for your mapping. For each source, you\\nhave to add 1 parameter to :\\n\\nDefine the type of destination with the name of parameter added\\nPrecise element about the destination\\n\\nWe will list all parameters you can have in your output mapping.\\n\\nWarning\\nYou have to add just 1 parameter of destination definition. If a\\nparameter destination is missing, the function will generate an error\\n(as opposed to input mapping).\\n\\n\\nEndpoint destination\\nParameter name : endpoint_output_name\\nDestination to : Outside through the endpoint\\nValue to put in parameter : name of the output received by the\\nendpoint in the body of the HTTP call\\nExample :\\nprediction_deployment_ouput = OutputDestination(\\n    step_output_name=\"prediction\",\\n    endpoint_output_name=\"beautiful_prediction\",\\n)\\n\\n\\n\\nData store destination\\nParameter name : datastore_path\\nDestination to : write a file into the data store\\nValue to put in parameter : path to a data store folder\\nExample :\\nprediction_deployment_ouput = OutputDestination(\\n    step_output_name=\"history_prediction\",\\n    datastore_path=\"path/to/history/folder.csv\",\\n)\\n\\n\\nExample step code to send file :\\n\\ndef stepFileIO () :\\n\\n   text_file = open(\\'history_prediction.txt\\', \\'wb\\')  # Open the file in binary mode\\n   text_file.write(\"Result of step send in file output :) \".encode(\\'utf-8\\'))  # Encode the string to bytes\\n   text_file.close()\\n\\n   fileOjb = {\"history_prediction\" : {\"path\": \"history_prediction.txt\"}}\\n\\n   return fileOjb \\n\\n\\n\\nDynamic path :\\nYou can also specify a dynamic path for the file to be uploaded by using\\none of the following patterns in your datastore path:\\n\\n{execution_id}: The execution id of the deployment.\\n{date}: The date of the execution in truncated ISO 8601 (YYYYMMDD) format.\\n{date_time}: The date of the execution in ISO 8601 (YYYYM-MDD_hhmmss) format.\\n\\nExample with a dynamic path :\\nprediction_deployment_ouput = OutputDestination(\\n    step_output_name=\"history_prediction\",\\n    datastore_path=\"path/to/history/exec_{execution_id}.csv\",\\n)\\n\\n\\n\\nVoid destination\\nParameter name : no_destination\\nDestination to : void\\nValue to put in parameter : True\\nExample :\\nprediction_deployment_ouput = OutputDestination(\\n    step_output_name=\"history_prediction\",\\n    is_null=True,\\n)\\n\\n\\n\\n\\n\\n\\n\\n4. Generate new endpoint token\\uf0c1\\nIf you need to alter the endpoint token for an endpoint, you can\\ngenerate a new one with the following SDK function.\\nsdk.generate_new_endpoint_token(endpoint_name=\"*your-endpoint-name*\")\\n\\n\\n\\nWarning\\nThis will permanently deactivate the previous token.\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html', 'title': 'Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nExecute a pipeline — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\nExecute a pipeline\\n\\n\\n\\n\\n\\n\\n\\n\\nExecute a pipeline\\uf0c1\\nAn execution of a pipeline creates an execution on the platform.\\nEach execution is associated with a pipeline with the definition of the\\nvalues of its inputs and outputs. The execution triggers the\\nexecution of the pipeline on one or more Kubernetes containers using the\\ncomputational resources available on the environment. All the\\nresults and artifacts of the execution can be retrieved in the\\nExecution Tracking tab.\\nThere are two ways to execute a pipeline:\\n\\nby creating a deployment: the execution will then depend on the\\nselected execution rule and will be performed when the execution\\ncondition is met (call for an endpoint, periodicity for a CRON,\\netc…)\\nby running it instantly with the sdk: It is then necessary to\\nindicate the values for each input of the pipeline.\\n\\n\\nSummary\\uf0c1\\n\\nRun a pipeline\\nTrigger a deployment by endpoint with SDK Craft AI\\nTrigger a deployment by endpoint with request\\nGet result of a past execution\\n\\n\\n\\nFunction name\\nMethod\\nReturn type\\nDescription\\n\\n\\n\\nrun_pipeline\\nrun_pipeline(pipeline_name, inputs=None, inputs_mapping=None, outputs_mapping=None)\\ndict\\nExecutes the pipeline on the platform.\\n\\nretrieve_endpoint_results\\nretrieve_endpoint_results(endpoint_name, execution_id, endpoint_token)\\ndict\\nGet result of endpoint execution.\\n\\n\\n\\n\\n\\nRun a pipeline\\uf0c1\\nA run is an execution of a pipeline on the platform. SDK\\nfunction that runs a pipeline to create an execution.\\nrun_pipeline(pipeline_name, inputs=None, inputs_mapping=None, outputs_mapping=None)\\n\\n\\nParameters\\n\\npipeline_name (str) – Name of an existing pipeline.\\ninputs (dict, optional) - Dictionary of inputs to pass to the\\npipeline with input names as dict keys and corresponding values as\\ndict values. For files, the value should be the path to the file or\\na file content as an instance of io.IOBase. Defaults to None.\\ninputs_mapping (list of instances of [InputSource]{.title-ref}) -\\nList of input mappings, to map pipeline inputs to different sources\\n(such as environment variables). See [InputSource]{.title-ref} for\\nmore details.\\noutputs_mapping (list of instances of\\n[OutputDestination]{.title-ref}) - List of output mappings, to map\\npipeline outputs to different destinations (such as datastore). See\\n[OutputDestination]{.title-ref} for more details.\\n\\nReturns\\nCreated pipeline execution represented as dict with execution_id and outputs as keys.\\nThe output values will be in the output object represented as dict with output_names as keys and corresponding values as values.\\nExample of return object :\\n{\\n  \"execution_id\": \"my-pipeline-8iud6\",\\n  \"outputs\": {\\n      \"output_number\": 0.117,\\n      \"output_text\": \"This is working fine\",\\n   }\\n}\\n\\n\\n\\n\\nTrigger a deployment with execution rule by endpoint with SDK Craft AI\\uf0c1\\nSDK function that triggers the deployment of our pipeline.\\nsdk.trigger_endpoint(endpoint_name, endpoint_token, inputs={},\\nwait_for_results=True)\\n\\n\\nParameters\\n\\nendpoint_name (str) – Name of the endpoint.\\nendpoint_token (str) – Token to access endpoint.\\ninputs (dict) - Inputs value for endpoint call.\\nwait_for_results (bool, optional) – Automatically call\\nretrieve_endpoint_results (True by default)\\n\\nReturns\\nCreated pipeline execution represented as dict.\\n\\n\\nTrigger a deployment with execution rule by endpoint with request\\uf0c1\\nFor trigger a deployment who is set up with an endpoint, you can also\\nsend request with your element defined in the pipeline input.\\nExamples in Python for variable :\\nimport requests\\n\\nr = requests.post(\\n    \"https://your_environment_url/my_endpoint\",\\n    json={\\n        \"input1\": \"value1\",\\n        \"input2\": [1,2,3]\\n        \"input3\": False\\n    },\\n    headers={\"Authorization\": \"EndpointToken \" + ENDPOINT_TOKEN }\\n)\\n\\n\\nExamples in Python for file (not available with auto mapping) :\\nimport requests\\n\\nr = requests.post(\\n    \"https://your_environment_url/my_multistep_endpoint\",\\n    files={\"data\": open(\"my_file.txt\", \"rb\")},\\n    headers={\"Authorization\": \"EndpointToken \" + ENDPOINT_TOKEN }\\n)\\n\\n\\n\\nNote\\nWe have explained in this documentation how to trigger the endpoint with\\nPython, but you can obviously send a request from any tool (curl,\\npostman, JavaScript, …).\\n\\n\\nWarning\\nInputs and outputs have size limits. This limit is 0.06MB for cumulative inputs and also 0.06MB for cumulative outputs. This input/output size limit is available for all trigger/deployment types (run, endpoint or CRON). This limit applies regardless of the source or destination of the input/output.\\nOnly file inputs/outputs are not affected by this limit. We recommend that you use this method when transferring large amounts of data.\\n\\n\\n\\nGet result of a past execution\\uf0c1\\nGet the results of an endpoint execution.\\nCraftAiSdk.retrieve_endpoint_results(endpoint_name, execution_id, endpoint_token)\\n\\n\\nParameters\\n\\nendpoint_name (str) - Name of the endpoint.\\nexecution_id (str) - Name of the execution returned by trigger_endpoint.\\nendpoint_token (str) - Token to access endpoint.\\n\\nReturns\\nCreated pipeline execution represented as dict with the following keys:\\n\\noutputs (dict): Dictionary of outputs of the pipeline with output names as keys and corresponding values as values.\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/executePipeline.html', 'title': 'Execute a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nFollow the deployment executions — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\nFollow the deployment executions\\n\\n\\n\\n\\n\\n\\n\\n\\nFollow the deployment executions\\uf0c1\\nA deployment\\n\\nis a way to run a Machine Learning pipeline in a\\nrepeatable and automated way. Once it is created, you can find the setup\\nof your deployments in the pipeline store. In addition, you can find\\nall the information about the executions of the deployments in the\\nexecution tracking.\\n\\n\\nWarning\\nDates provided by the Web UI and SDK are always expressed in Coordinated Universal Time (UTC).\\n\\n\\nSummary\\uf0c1\\n\\nGet information about a deployment\\nDelete a deployment or an execution\\nFollow the execution tracking\\n\\n\\n\\nFunction name\\nMethod\\nReturn type\\nDescription\\n\\n\\n\\nlist_deployments\\nlist_deployments()\\nlist of dict\\nGet the list of all deployments.\\n\\nget_deployment\\nget_deployment(deployment_name)\\ndict\\nGet information of a deployment.\\n\\ndelete_deployment\\ndelete_deployment(deployment_name)\\ndict\\nDelete a deployment identified by its name.\\n\\nget_pipeline_execution\\nget_pipeline_execution(execution_id)\\ndict\\nGet the status of one pipeline execution identified by its name.\\n\\ndelete_pipeline_execution\\ndelete_pipeline_execution(execution_id)\\ndict\\nDelete pipeline execution.\\n\\nget_pipeline_execution_input\\nget_pipeline_execution_input(execution_id, input_name)\\ndict\\nGet information about an input of an execution.\\n\\nget_pipeline_execution_output\\nget_pipeline_execution_output(execution_id, output_name)\\ndict\\nGet information about an output of an execution.\\n\\n\\n\\n\\n\\nGet information about a deployment\\uf0c1\\n\\nList of deployments\\uf0c1\\nGet the list of all deployments.\\nCraftAiSdk.list_deployments()\\n\\n\\nReturns\\nList of deployments represented as dict with the following keys:\\n\\nname (str): Name of the deployment.\\npipeline_name (str): Name of the pipeline associated to the deployment.\\nversion (str): Version of the pipeline associated to the deployment.\\nexecution_count (int): Number of times the deployment has been executed.\\ntype (str): Type of the deployment. Can be endpoint, run or periodic.\\n\\n\\n\\nGet deployment information\\uf0c1\\nGet information of a deployment.\\nCraftAiSdk.get_deployment(deployment_name)\\n\\n\\nParameters\\n\\ndeployment_name (str) – Name of the deployment.\\n\\nReturns\\nDeployment information represented as dict (with keys id, name and\\npipeline).\\n\\nname (str): Name of the deployment.\\npipeline (dict): Pipeline associated to the deployment represented as dict with the following keys:\\n\\nname (str): Name of the pipeline.\\n\\n\\ninputs_mapping (list of dict): List of inputs mapping represented as dict with the following keys:\\n\\nstep_input_name (str): Name of the step input.\\ndata_type (str): Data type of the step input.\\ndescription (str): Description of the step input.\\nconstant_value (str): Constant value of the step input. Note that this key is only returned if the step input is mapped to a constant value.\\nenvironment_variable_name (str): Name of the environment variable. Note that this key is only returned if the step input is mapped to an environment variable.\\nendpoint_input_name (str): Name of the endpoint input. Note that this key is only returned if the step input is mapped to an endpoint input.\\nis_null (bool): Whether the step input is mapped to null. Note that this key is only returned if the step input is mapped to null.\\ndatastore_path (str): Datastore path of the step input. Note that this key is only returned if the step input is mapped to the datastore.\\nis_required (bool): Whether the step input is required. Note that this key is only returned if the step input is required.\\ndefault_value (str): Default value of the step input. Note that this key is only returned if the step input has a default value.\\n\\n\\noutputs_mapping (list of dict): List of outputs mapping represented as dict with the following keys:\\n\\nstep_output_name (str): Name of the step output.\\ndata_type (str): Data type of the step output.\\ndescription (str): Description of the step output.\\nendpoint_output_name (str): Name of the endpoint output. Note that this key is only returned if the step output is mapped to an endpoint output.\\nis_null (bool): Whether the step output is mapped to null. Note that this key is only returned if the step output is mapped to null.\\ndatastore_path (str): Datastore path of the step output. Note that this key is only returned if the step output is mapped to the datastore.\\n\\n\\nendpoint_token (str): Token of the endpoint. Note that this key is only returned if the deployment is an endpoint.\\nschedule (str): Schedule of the deployment. Note that this key is only returned if the deployment is a periodic deployment.\\nhuman_readable_schedule (str): Human readable schedule of the deployment. Note that this key is only returned if the deployment is a periodic deployment.\\ncreated_at (str): Date of creation of the deployment.\\ncreated_by (str): ID of the user who created the deployment.\\nupdated_at (str): Date of last update of the deployment.\\nupdated_by (str): ID of the user who last updated the deployment.\\nlast_execution_id (str): ID of the last execution of the deployment.\\nactive (bool): Whether the deployment is active.\\ndescription (str): Description of the deployment.\\nexecution_rule (str): Execution rule of the deployment.\\n\\nReturn type\\ndict\\n\\n\\n\\nDelete a deployment or an execution\\uf0c1\\n\\n\\nDelete a deployment\\uf0c1\\nDelete a deployment identified by its name.\\nCraftAiSdk.delete_deployment(deployment_name)\\n\\n\\nParameters\\n\\ndeployment_name (str) – Name of the deployment.\\n\\nReturns\\nDeleted deployment represented as dict (with keys id, name). The\\nreturn data type is dict.\\n\\nWarning\\nBe careful, deleting a deployment will delete all its executions.\\n\\n\\nDelete an execution\\uf0c1\\nDelete one pipeline execution identified by its execution_id.\\nCraftAiSdk.delete_pipeline_execution(execution_id)\\n\\n\\nParameters\\n\\nexecution_id (str) - Name of the pipeline execution.\\n\\nReturns\\nDeleted pipeline execution represented as dict with the following keys:\\n\\nexecution_id (str): Name of the pipeline execution deleted.\\n\\n\\n\\n\\nFollow the execution tracking\\uf0c1\\n\\nGet execution list\\uf0c1\\nGet the status of one pipeline execution identified by its name.\\nCraftAiSdk.get_pipeline_execution(execution_id)\\n\\n\\nParameters\\n\\nexecution_id (str) - ID of the pipeline execution.\\n\\nReturns\\nInformation on the pipeline execution with id execution_id represented as dict.\\n\\nexecution_id (str): Name of the pipeline execution.\\nstatus (str): Status of the pipeline execution.\\ncreated_at (str): Date of creation of the pipeline\\ncreated_by (str): ID of the user who created the pipeline execution. In the case of a pipeline run, this is the user who triggered the run. In the case of an execution via a deployment, this is the user who created the deployment.\\nend_date (str): Date of completion of the pipeline execution.\\npipeline_name (str): Name of the pipeline used for the execution.\\ndeployment_name (str): Name of the deployment used for the execution.\\nsteps (list of obj): List of the step executions represented as dict with the following keys:\\n\\nname (str): Name of the step.\\nstatus (str): Status of the step.\\nstart_date (str): Date of start of the step execution.\\nend_date (str): Date of completion of the step execution.\\ncommit_id (str): Id of the commit used to build the step.\\nrepository_url (str): Url of the repository used to build the step.\\nrepository_branch (str): Branch of the repository used to build the step.\\n\\n\\ninputs (list of dict): List of inputs represented as a dict with the following keys:\\n\\nstep_input_name (str): Name of the input.\\n`data_type (str): Data type of the input.\\nsource (str): Source of type of the input. Can be environment_variable, datastore, constant, is_null endpoint or run.\\nendpoint_input_name (str): Name of the input in the endpoint execution if source is endpoint.\\nconstant_value (str): Value of the constant if source is constant.\\nenvironment_variable_name (str): Name of the environment variable if source is environment_variable.\\nis_null (bool): True if source is is_null.\\nvalue: Value of the input.\\n\\n\\noutputs (list of dict): List of outputs represented as a dict with the following keys:\\n\\nstep_output_name (str): Name of the output.\\n`data_type (str): Data type of the output.\\ndestination (str): Destination of type of the output. Can be datastore, is_null endpoint or run.\\nendpoint_output_name (str): Name of the output in the endpoint execution if destination is endpoint.\\nis_null (bool): True if destination is is_null.\\nvalue: Value of the output.\\n\\n\\n\\n\\n\\nGet execution logs\\uf0c1\\nGet the logs of an executed pipeline identified by its name.\\nCraftAiSdk.CraftAiSdk.get_pipeline_execution_logs(*pipeline_name*, *execution_id*,\\nfrom_datetime=None, to_datetime=None, limit=None)\\n\\n\\nParameters\\n\\npipeline_name (str) – Name of an existing pipeline.\\nexecution_id (str) – ID of the pipeline execution.\\nfrom_datetime (datetime.time, optional) – Datetime from which the\\nlogs are collected.\\nto_datetime (datetime.time, optional) – Datetime until which the\\nlogs are collected.\\nlimit (int, optional) – Maximum number of logs that are\\ncollected.\\n\\nReturns\\nList of collected logs represented as dict (with keys message,\\ntimestamp and stream). The return type is a list.\\n\\n\\nGet Input and Output of an execution\\uf0c1\\n\\nInput\\uf0c1\\nGet the input value of an executed pipeline identified by its execution_id.\\nCraftAiSdk.get_pipeline_execution_input(execution_id, input_name)\\n\\n\\nParameters\\n\\nexecution_id (str) - ID of the pipeline execution.\\ninput_name (str) - Name of the input.\\n\\nReturns\\nInformation on the input represented as a dict with the following keys :\\n\\nstep_input_name (str): Name of the input.\\ndata_type (str): Data type of the input.\\nsource (str): Source of type of the input. Can be environment_variable, datastore, constant, is_null endpoint or run.\\nendpoint_input_name (str): Name of the input in the endpoint execution if source is endpoint.\\nconstant_value (str): Value of the constant if source is constant.\\nenvironment_variable_name (str): Name of the environment variable if source is environment_variable.\\nis_null (bool): True if source is is_null.\\nvalue: Value of the input.\\n\\n\\n\\nOutput\\uf0c1\\nGet the output value of an executed pipeline identified by its execution_id.\\nCraftAiSdk.get_pipeline_execution_output(execution_id, output_name)\\n\\n\\nParameters\\n\\nexecution_id (str) - ID of the pipeline execution.\\noutput_name (str) - Name of the output.\\n\\nReturns\\nInformation on the output represented as a dict with the following keys :\\n\\nstep_output_name (str): Name of the output.\\ndata_type (str): Data type of the output.\\ndestination (str): Destination of type of the output. Can be datastore, is_null endpointorrun`.\\nendpoint_output_name (str): Name of the output in the endpoint ex-\\necution if destination is endpoint.\\nis_null (bool): True if destination is is_null.\\nvalue: Value of the output.\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nMetrics — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\nMetrics\\n\\n\\n\\n\\n\\n\\n\\n\\nMetrics\\uf0c1\\nIn the context of MLOps, tracking and monitoring metrics is critical for\\nassessing the performance and progress of machine learning pipelines.\\nThe CraftAiSdk platform provides a comprehensive set of features for\\ndefining and recording metrics at each pipeline execution.\\nWith measurement capabilities, you can efficiently track and retrieve\\nthe metrics associated with each execution in your machine learning\\npipelines. This enables you to valuable insights and make informed\\ndecisions about your models and deployments.\\n\\nPipeline Metrics\\uf0c1\\nThe record_metric_value function allows you to create or update a\\npipeline metric within a step code. This function allows you to store\\nthe name and corresponding value of a particular metric.\\nYou do not need to declare anything outside of the step, you can just\\nuse record_metrics_value() in your step code. Remember, if you want to\\nuse the SDK in your step code, you don’t need to specify your\\nenvironment URL or token in the builder parameters.\\nAfter the execution is finished, you can find all your metric values in\\nthe web interface on the Execution page and on the Metrics tab.\\n\\nUpload Metrics\\uf0c1\\nCurrently, pipeline metrics can only have one numeric value and one name\\nfor each execution metric. If multiple metrics are entered with\\nidentical names, only the last metric will be retained.\\n\\nWarning\\nThis function can only be used in the source code of the step running on\\nthe platform. When used outside a code step, it doesn’t send metrics\\nand displays a warning message.\\n\\nCraftAiSdk.record_metric_value(name, value)\\n\\n\\nParameters\\n\\nname (str) - The name of the metric to store.\\nvalue (float) - The value of the metric to store.\\n\\nReturns\\nList of execution metrics as dict (with keys “name”, “value”,\\n“created_at”, “execution_id”, “deployment_name”,\\n“pipeline_name”).\\nExample\\nHere is a very simple example of step code that sends only 2 different\\nmetrics.\\n\\nNote\\nDon’t forget to import the craft-ai-sdk package in the step code and to\\nlist the library in your requirement.txt to install it on the step\\nexecution context.\\n\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndef metricsStep () :\\n\\n    sdk = CraftAiSdk()\\n\\n    # Some code \\n\\n    sdk.record_metric_value(\"accuracy\", 0.1409)\\n    sdk.record_metric_value(\"loss\", 1/3)\\n\\n    print (\"Metrics are sent\")\\n\\n\\n\\n\\nGet metrics\\uf0c1\\nThe get_metrics function retrieves a list of pipeline metrics. You can\\nfilter the metrics based on the name, pipeline name, deployment name, or\\nexecution ID. It’s important to note that only one of the parameters\\n(name, pipeline_name, deployment_name, execution_id) can be set at a\\ntime.\\nCraftAiSdk.get_metrics(name=None, pipeline_name=None, deployment_name=None, execution_id=None)\\n\\n\\nParameters\\n\\nname (str, optional) - The name of the metric to retrieve.\\npipeline_name (str, optional) - Filter metrics by pipeline. If not\\nspecified, all pipelines will be considered.\\ndeployment_name (str, optional) - Filter metrics by deployment. If\\nnot specified, all deployments will be considered.\\nexecution_id (str, optional) - Filter metrics by execution. If not\\nspecified, all executions will be considered.\\n\\nReturns\\nThe function returns a list of execution metrics as dictionaries. Each\\nmetric entry contains the following keys: “name”, “value”,\\n“created_at”, “execution_id”, “deployment_name”,\\n“pipeline_name”.\\n\\n\\n\\nList Metrics\\uf0c1\\nThe Craft AI platform provides robust features for defining and\\nrecording list metrics during pipeline execution. This functionality\\nallows you to store the name and corresponding list of values for a\\nspecific metric.\\nTo create or update a list metric within a step code, you can utilize\\nthe record_list_metric_values() function. Afterwards, you\\ncan retrieve your metrics outside the step using the\\nget_list_metrics() function. Additionally, you can access\\nall your metric values in the web interface via the Metrics tab on the\\nExecution page.\\nSimilar to pipeline metrics, list metrics can only consist of a list of\\nnumbers (integer or float).\\n\\nUpload list metrics\\uf0c1\\nThe record_list_metric_values() function enables you to\\nadd values to a metric list by specifying the name of the metric list\\nand the corresponding values. There is no need to declare anything\\noutside of the step; simply use\\nrecord_list_metric_values() in your step code, as you\\nwould for pipeline metrics.\\nIt’s important to note that when using the\\nrecord_list_metric_values() function, it can only be\\nutilized within the source code of the step running on the platform.\\nWhen uploading list metrics, you have the option to either specify a\\nPython list directly or upload values individually, specifying the same\\nmetric name (which will automatically accumulate into a list).\\nHere is an example of step code that sends two different lists metrics:\\n\\nWarning\\nThis function can only be used in the source code of the step running on\\nthe platform. When used outside of a code step, it doesn’t send metrics\\nand displays a warning message.\\n\\nCraftAiSdk.record_list_metric_values(name, values)\\n\\n\\nParameters\\n\\nname (str) - Name of the metric list to add values.\\nvalues (list of float or float) - Values of the metric list to\\nadd.\\n\\nReturns\\nThis function returns nothing (None).\\nExample\\nHere is a very simple example of step code that sends only 2 different\\nlists metrics.\\n\\nNote\\nDon’t forget to import the craft-ai-sdk package in the step code and to\\nlist the library in your requirement.txt to install it on the step\\nexecution context.\\n\\nfrom craft_ai_sdk import CraftAiSdk\\nimport math \\n\\ndef metricsStep():\\n    sdk = CraftAiSdk()\\n\\n    # Some code \\n\\n    # Just one list upload\\n    sdk.record_list_metric_values(\"accuracy_list\", [0.89, 0.92, 0.95])\\n\\n    # Tow list upload, the lists will be concatenated in loss_list list metrics \\n    sdk.record_list_metric_values(\"loss_list\", [1.4, 1.2])\\n    sdk.record_list_metric_values(\"loss_list\", [1.1, 1.0])\\n\\n    # Upload multiple values that will be concatenated into 1 metrics list *logx* with all values\\n    for i in range (1, 50) : \\n\\n        sdk.record_list_metric_values(\"logx\", math.log(i))\\n\\n    print(\"List metrics are sent\")\\n\\n\\n\\nWarning\\nA pipeline metrics and a list metrics can have the same name in the same\\nexecution. A metrics list is limited to a maximum of 50,000 values per\\nexecution.\\n\\n\\n\\nGet list metrics\\uf0c1\\nTo retrieve a list of metric lists, you can use the\\nget_list_metrics() function. This function allows you to\\nfilter the metric lists based on the name, pipeline name, deployment\\nname, or execution ID.\\nIt’s important to note that only one of the parameters\\n(name, pipeline_name,\\ndeployment_name, execution_id) can be set at\\na time.\\nCraftAiSdk.get_list_metrics(name=None, pipeline_name=None, deployment_name=None, execution_id=None)\\n\\n\\nParameters\\n\\nname (str, optional) - Name of the metric list to retrieve.\\npipeline_name (str, optional) - Filter metric lists by pipeline,\\ndefaults to all the pipelines.\\ndeployment_name (str, optional) - Filter metric lists by\\ndeployment, defaults to all the deployments.\\nexecution_id (str, optional) - Filter metric lists by execution,\\ndefaults to all the executions.\\n\\nReturns\\nThe function returns a list of execution metrics as dictionaries. Each\\nmetric entry contains the following keys: “name”, “value”,\\n“created_at”, “execution_id”, “deployment_name”,\\n“pipeline_name”.\\nHere is an example of how to use the get_list_metrics()\\nfunction:\\nlist_metrics = CraftAiSdk.get_list_metrics(name=\"accuracy_list\", pipeline_name=\"my_pipeline\")\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/metrics.html', 'title': 'Metrics — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nCraft AI SDK documentation — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nCraft AI SDK documentation\\n\\n\\n\\n\\n\\n\\n\\n\\nCraft AI SDK documentation\\uf0c1\\nTo check the version of the installed craft-ai-sdk, run the following command in a console:\\npip show craft-ai-sdk\\n\\n\\nOr in a Python shell:\\nimport craft_ai_sdk\\ncraft_ai_sdk.__version__\\n\\n\\n\\nCurrent\\uf0c1\\nsdk_0.31.2.pdf\\n\\n\\nArchives\\uf0c1\\nsdk_0.31.1.pdf\\nsdk_0.31.0.pdf\\nsdk_0.30.0.pdf\\nsdk_0.29.0.pdf\\nsdk_0.28.0.pdf\\nsdk_0.27.1.pdf\\nsdk_0.27.0.pdf\\nsdk_0.25.0.pdf\\nsdk_0.24.1.pdf\\nsdk_0.24.0.pdf\\nsdk_0.23.3.pdf\\nsdk_0.23.2.pdf\\nsdk_0.23.1.pdf\\nsdk_0.23.0.pdf\\nsdk_0.22.0.pdf\\nsdk_0.21.1.pdf\\nsdk_0.21.0.pdf\\nsdk_0.20.0.pdf\\nsdk_0.19.0.pdf\\nsdk_0.18.0.pdf\\nsdk_0.17.0.pdf\\nsdk_0.16.0.pdf\\nsdk_0.15.0.pdf\\nsdk_0.14.1.pdf\\nsdk_0.14.0.pdf\\nsdk_0.13.2.pdf\\nsdk_0.13.1.pdf\\nsdk_0.13.0.pdf\\nsdk_0.12.0.pdf\\nsdk_0.11.0.pdf\\nsdk_0.10.1.pdf\\nsdk_0.10.0.pdf\\nsdk_0.9.1.pdf\\nsdk_0.9.0.pdf\\nsdk_0.8.1.pdf\\nsdk_0.8.0.pdf\\nsdk_0.7.0.pdf\\nsdk_0.6.0.pdf\\nsdk_0.5.0.pdf\\nsdk_0.4.0.pdf\\nsdk_0.3.0.pdf\\nsdk_0.2.0.pdf\\nsdk_0.1.6.pdf\\nsdk_0.1.5.pdf\\nsdk_0.1.4.pdf\\nsdk_0.1.3.pdf\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/histo_doc_sdk.html', 'title': 'Craft AI SDK documentation — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nCraft AI changelog — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\n\\n\\nCraft AI changelog\\uf0c1\\n\\nV1.1.6 - 17/11/2023\\uf0c1\\nSDK version : 0.31.2 \\nAPI version : 2023-11-13\\n\\nFixed bugs and improvements\\n\\n\\n\\nV1.1.5 - 08/11/2023\\uf0c1\\nSDK version : 0.31.1 \\nAPI version : 2023-11-05\\n\\nSearch bar added to projects page\\nAdded size limit for json entries in run_pipeline() SDK documentation\\nFixed bugs and improvements\\n\\n\\n\\nV1.1.4 - 02/11/2023\\uf0c1\\nSDK version : 0.31.0 \\nAPI version : 2023-10-30\\n\\nCompare execution for list metrics\\nAdd created_by , last_execution_id and deployments parameters to the return from get_pipeline()\\nDeployment and pipeline slider -> display will evolve again\\nFixed bugs and improvements\\n\\n\\n\\nV1.1.3 - 24/10/2023\\uf0c1\\nSDK version : 0.30.0 \\nAPI version : 2023-10-23-2\\n\\nAdded information to the return of get_deployment()\\nAdded the ability to add a description to a deployment\\nUpdate information bubble infra env + associated tooltips\\nCorrection of filter overflows on small tables in the front end\\nFixed bugs and improvements\\n\\n\\n\\nV1.1.2 - 19/10/2023\\uf0c1\\nSDK version : 0.29.0 \\nAPI version : 2023-10-16-2\\n\\nChange of the output format of the size parameter of the functions get_data_store_object_information and list_data_store_objects.\\nWe now have str with the storage size units for a better readability of the file sizes.\\nFixed bugs and improvements\\n\\n\\n\\nV1.1.1 - 13/10/2023\\uf0c1\\nSDK version : 0.28.0 \\nAPI version : 2023-10-11\\n\\nCorrection of blue screen in execution tracking when changing environment\\nRequirement.txt path displayed in execution tracking\\nAddition of get_user() function in SDK\\nAdded documentation on the maximum size of included_folders\\nFixed bugs and improvements\\n\\n\\n\\nV1.1.0 - 11/10/2023\\uf0c1\\nSDK version : 0.27.1 \\nAPI version : 2023-10-03\\n\\nGPU envs are available\\nStep creation is limited to 3min max (the timeout_s parameter can be modified if necessary)\\nAddition of pipeline duration in the experiment tracking\\nFront-end correction (date in UTC and change GitHub references to Git)\\nFixed bugs and improvements\\n\\n\\n\\nV1.0.28 - 03/10/2023\\uf0c1\\nSDK version : 0.27.0 \\nAPI version : 2023-09-25\\n\\nDownload button added to the execution comparison page\\nAdd inputs and outputs to the comparison execution page\\nWait for the step status to be ready before returning the result of create_step()\\nMaximum size of code imported into a step increased from 1 MB to 5 MB\\nAddition of a size limit for value-type I/O (JSON)\\nFixed bugs and improvements\\n\\n\\n\\nV1.0.27 - 27/09/2023\\uf0c1\\nSDK version : 0.25.0 \\nAPI version : 2023-09-14\\n\\nRemoval of the experimental mention on the tech doc for metrics and periodic deployment\\nFixed bugs and improvements\\n\\n\\n\\nV1.0.26 - 14/09/2023\\uf0c1\\nSDK version : 0.24.1 \\nAPI version : 2023-09-14\\n\\nRemoval of the experimental mention on the tech doc for metrics and periodic deployment\\nFixed bugs and improvements\\n\\n\\n\\nV1.0.25 - 10/09/2023\\uf0c1\\nSDK version : 0.24.0 \\nAPI version : 2023-09-06\\n\\nI/O files can be downloaded from the Execution Tracking page\\nSimple metrics can be viewed on the Execution Comparison page\\nYou can select the columns you want to display and filter/sort the metadata and simple metrics on the Execution Comparison page\\nThe repo, branch and Git commit can be viewed from the Execution Tracking page\\nA limit of 100 simple metrics and 25 list metrics per step has been introduced\\nWe now use the d3 lib to display graphs\\nIt is now possible to have environments with GPUs (standard or medium)\\nAdded the Monitoring page (which lets you track pipeline metrics over a period)\\nAdded the delete_pipeline_execution(execution_id) function, which can be used to delete an execution using its execution_id\\n\\n\\n\\nV1.0.24 - 29/08/2023\\uf0c1\\nSDK version : 0.23.2 \\nAPI version : 2023-08-29\\n\\nFront improvement\\nBug fixes and improvements\\n\\n\\n\\nV1.0.24 - 29/08/2023\\uf0c1\\nSDK version : 0.23.2 \\nAPI version : 2023-08-29\\n\\nFront improvement\\nBug fixes and improvements\\n\\n\\n\\nV1.0.23 - 09/08/2023\\uf0c1\\nSDK version : 0.23.1 \\nAPI version : 2023-08-01\\n\\nFront improvement\\nBug fixes and improvements\\n\\n\\n\\nV1.0.22 - 01/08/2023\\uf0c1\\nSDK version : 0.23.1 \\nAPI version : 2023-08-01\\n\\nFront improvement (ISO format for dates)\\nBug fixes and improvements\\n\\n\\n\\nV1.0.21 - 26/07/2023\\uf0c1\\nSDK version : 0.23.0 \\nAPI version : 2023-07-26\\n\\nV1 run comparison page added\\nCopy icon for the access path to the datastore in the front end\\nAdded parameter with created_by for the output of the list_pipeline_executions function in the SDK\\nBug fixes and improvements\\n\\n\\n\\nV1.0.20 - 18/07/2023\\uf0c1\\nSDK version : 0.22.0 \\nAPI version : 2023-07-18\\n\\nPossibility to delete a pipeline from front\\nBug fixes and improvements\\n\\n\\n\\nV1.0.19 - 06/07/2023\\uf0c1\\nSDK version : 0.21.1 \\nAPI version : 2023-07-06\\n\\nDynamic path for outputs to datastore\\nSome fixes on the list metrics front\\nInternal usage metrics for list metrics added\\nBug fixes and improvements\\n\\n\\n\\nV1.0.18 - 28/06/2023\\uf0c1\\nSDK version : 0.21.0 \\nAPI version : 2023-06-28\\n\\nFront-end graphics for list metrics\\nCommit id and branch name available in get_step and list_step\\nEnhanced SDK doc with\\n\\nAdditional usage examples\\nMore details on function returns\\nFlags for experimental features\\n\\n\\nBug fixes and improvements\\n\\n\\n\\nV1.0.17 - 20/06/2023\\uf0c1\\nSDK version : 0.20.0 \\nAPI version : 2023-06-2\\n\\nList metrics (without list metrics values in the frontend, only via SDK for now)\\nBug fixes and improvements\\n\\n\\n\\nV1.0.16 - 15/06/2023\\uf0c1\\nSDK version : 0.19.0 \\nAPI version : 2023-06-14\\n\\nOutput mapping for run\\nAllow other repository than GitHub to fetch step code (example : GitLab)\\nBug fixes and improvements\\n\\n\\n\\nV1.0.15 - 07/06/2023\\uf0c1\\nSDK version : 0.18.0 \\nAPI version : 2023-06-07\\n\\nPeriodic deployment\\nDatastore as source and destination for I/O files\\nFix for long runs with run_pipeline()\\nBug fixes and improvements\\n\\n\\n\\nV1.0.14 - 26/05/2023\\uf0c1\\nSDK version : 0.17.0 \\nAPI version : 2023-05-26\\n\\nMetrics pipeline (only for metrics with 1 value)\\nPipeline page in front\\nBug fixes and improvements\\n\\n\\n\\nV1.0.13 - 24/05/2023\\uf0c1\\nSDK version : 0.16.0 \\nAPI version : 2023-05-24\\n\\nCustom mappings for the execution pipeline are now available\\nAbility to place env variables in the requirement path when creating a step\\nBug fixes and improvements\\n\\n\\n\\nV1.0.12 - 10/05/2023\\uf0c1\\nSDK version : 0.15.0 \\nAPI version : 2023-05-04\\n\\nUsage metrics (For internal use at Craft AI only)\\nrun_pipeline without custom mapping\\nBug fixes and improvements\\n\\n\\n\\nV1.0.11 - 04/05/2023\\uf0c1\\nSDK version : 0.15.0 \\nAPI version : 2023-05-04\\n\\nBug fixes and improvements\\n\\n\\n\\nV1.0.10 - 26/04/2023\\uf0c1\\nSDK version : 0.14.1 \\nAPI version : 2023-04-26\\n\\nBug fixes and improvements\\n\\n\\n\\nV1.0.9 - 21/04/2023\\uf0c1\\nSDK version : 0.14.0 \\nAPI version : 2023-04-20\\n\\nBug fixes and improvements\\n\\n\\n\\nV1.0.8 - 11/04/2023\\uf0c1\\nSDK version : 0.13.2 \\nAPI version : 2023-04-22\\n\\nAdd environment page in the UI\\nBug fixes and improvements\\n\\n\\n\\nV1.0.7 - 28/03/2023\\uf0c1\\nSDK version : 0.13.1 \\nAPI version : 2023-03-22\\n\\nBug fixes and improvements\\n\\n\\n\\nV1.0.6 - 23/03/2023\\uf0c1\\nSDK version : 0.13.0 \\nAPI version : 2023-03-22\\n\\nAdd error messages on the build of steps\\nAdded object STEP_PARAMETER to explain step creation parameters between “fall back on project info” and “null”\\nAdded function get_data_store_object_information()\\nis_null source is now available for all input type\\nFix endpoint mapping with is_required parameter (which was mandatory)\\nVarious fixes and improvements\\n\\n\\n\\nV1.0.5 - 15/03/2023\\uf0c1\\nSDK version : 0.13.0 \\nAPI version : 2023-03-15\\n\\nNew front page for input/output in execution tracking\\nAdd ability to create a step with a custom dockerfile\\nAdd function list_pipelines() to get list of all pipelines\\nBug fixes\\n\\n\\n\\nV1.0.4 - 28/02/2023\\uf0c1\\nSDK version : 0.12.0 \\nAPI version : 2023-02-24\\n\\nAdded ability to trigger endpoint with file input/output in io feature\\nFixed input mapping ‘isRequired’ matches step input\\nFixed issue with refreshing environments and deployments lists in the frontend\\nFixed default value for included folders\\n\\n\\n\\nV1.0.3 - 17/02/2023\\uf0c1\\nSDK version : 0.11.0 \\nAPI version : 2023-02-17\\n\\nFix input mapping bug with constant source\\nAdd Google Tag Manager\\nRework login page\\nOther bug fixes\\n\\n\\n\\nV1.0.2 - 15/02/2023\\uf0c1\\nSDK version : 0.10.1 \\nAPI version : 2023-02-13\\n\\nRename force_step_deletion to force_dependents_deletion\\nBug fix\\n\\n\\n\\nV1.0.1 - 10/02/2023\\uf0c1\\nSDK version : 0.10.0 \\nAPI version : 2023-02-06\\n\\nProject information from GitHub is taken into account for step creation (and therefore optional at this stage)\\nImprove error messages for step creation\\nMessage for first login\\nFront improvement/correction\\nBug correction\\n\\n\\n\\nV1.0.0 - 27/01/2023\\uf0c1\\nSDK version : 0.9.0 \\nAPI version : 2023-01-26\\n\\nAdds separate environments for each project.\\nInput/output (IO) added to steps.\\nAdded IO mapping for the endpoint\\nAdd automatic redirection to wait for pipeline output.\\nSyntax change: force_endpoints_deletion replaced by force_deployments_deletion.\\nWhen executions are retrieved, two variables are added to the result:\\n\\nstart_date for the ;\\nend_date for pipelines and stages.\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/changeLog.html', 'title': 'Craft AI changelog — Craft AI MLOps platform documentation  documentation', 'language': 'en'}),\n",
       " Document(page_content='\\n\\n\\n\\n\\nAdministration — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nAdministration\\n\\n\\n\\n\\n\\n\\n\\n\\nAdministration\\uf0c1\\nThe MLOps - Craft AI Platform is composed of a graphical interface\\nallowing you to visualize, create, manage and monitor the objects\\nnecessary for the realization of your AI projects.\\nIt is composed of:\\n\\nHomepage: Visualize and create projects.\\nParameters: Manage the company’s account and users.\\nProject settings: Manage the configuration of a project.\\nEnvironments: See the environment(s) and their information within\\na project.\\nExecutions: See the execution(s) and their information within an\\nenvironment or a deployment.\\n\\n\\nNote: We strongly recommend Google Chrome browser to use the\\ngraphical interface of the platform.\\n\\nSummary:\\n\\nUsers\\nProjects\\nToken SDK\\n\\n\\nUsers\\uf0c1\\nSummary:\\n\\nManage user\\nLogin\\nGet user with ID\\n\\n\\nManage user\\uf0c1\\nThe management of user is available by email for the moment. In a next\\nversion, it will be possible to add, edit and delete a user directly on\\nthe platform UI.\\n\\nAdd a user\\uf0c1\\nSend a message to Craft AI with your request and the following\\ninformation:\\n\\nFirst and last name of the user\\nEmail of the user\\n\\n\\n\\n   🆕 Adding users will arrive later on the platform.\\n\\n\\n\\nAccess rights\\uf0c1\\nEach user has access to one or more defined projects.\\nEach user who has access to a project has access to all the information and actions in it.\\n\\n\\n   🆕 Advanced access rights will arrive later on the platform.\\n\\n\\n\\nDelete a user\\uf0c1\\nSend a message to Craft AI with your request and the following\\ninformation:\\n\\nFirst and last name of the user\\nEmail of the user\\n\\n\\n\\n   🆕 The deletion of users will arrive later on the platform.\\n\\n\\n\\n\\nLogin\\uf0c1\\nHere is the URL to access the platform:\\nhttps://mlops-platform.craft.ai\\n\\nFirst connection\\uf0c1\\nPrerequisites: The user must be added to the platform (Add a user).\\n\\nThe user connects to the platform: https://mlops-platform.craft.ai\\nThe connection will not work, but Craft AI will receive the request\\nand can add the user.\\nThe user receives an email/slack from Craft AI to inform him that he\\ncan log in.\\nThe user logs in with the same link and has access to the platform.\\n\\n\\n\\nForgot your password\\uf0c1\\n\\nIn the Login popup, click on Don’t remember your password?\\nEnter your email, you will receive an email to modify your password.\\n\\n\\n\\n\\nGet user with ID\\uf0c1\\n\\nFunction definition\\uf0c1\\nWhile using the SDK you may encounter outputs parameters containing a user when using specific functions. Each user is identified with a unique ID. That is the case for example in the sdk.get_pipeline_execution() function output with the parameter created_by. To match the user ID with the corresponding information (name and email) you can use the get_user() function.\\nCraftAiSdk.get_user(user_id)\\n\\n\\n\\n\\nParameters\\uf0c1\\n\\nuser_id (str) – The ID of the user.\\n\\n\\n\\nReturns\\uf0c1\\nThe user information in dict type, with the following keys:\\n\\nid (str): ID of the user.\\nname (str): Name of the user.\\nemail (str): Email of the user.\\n\\n\\n\\n\\n\\nProjects\\uf0c1\\nA project is a complete use case. From data import, through\\nexperimentation, testing and production, to performance monitoring over\\ntime.\\nUsers can create as many projects as they want. Each project is\\nhermetically sealed from the others. All users on the platform can\\naccess to all projects.\\nTo work on a project, it must include at least one environment. The\\nuser can create several environments in a project.\\nSummary:\\n\\nCreate a project\\nManange a prject\\n\\n\\nCreate a project\\uf0c1\\n\\nFrom the homepage, you can create a new project by clicking on the\\n“New project” button.\\nA project creation page opens in which you have to fill in the fields\\n:\\n\\n[Mandatory]Project name : Enter the name of your project (in\\nlowercase and with “-”), you will not be able to modify it later.\\n[Mandatory]Python version : Select the version of Python\\nthat will be applied to this project. It will be possible to\\nchoose a different version when creating each step.\\n[Mandatory]Repository URL : Enter the SSH URL of your repository.\\n\\nHow to get my repository URL\\nOn GitHub, from your repository, click on the “Code” button and\\nchoose SSH. The URL must start with “git@”.\\n\\nOn GitLab, from your repository, click on the “Clone” button and\\ncopy SSH URL. The URL must start with “git@”.\\n\\n\\n\\n[Mandatory]Deploy key : Enter your Github / GitLab private key.\\nHow to generate my deploy key :\\n\\nFor security reasons, to get access to your GitHub project, the\\nplatform uses a Deploy Key with the RSA SSH KEY standard. The\\ndeploy key is a special key that grants access to a specific\\nrepository; it is not the same as personal keys used commonly\\nby users to access their repositories, although they are both\\nSSH keys.\\nThe deploy key has two elements:\\n\\nThe public key, which must be set in the GitHub\\nadministration settings for the repository.\\nThe private key, which must be sent to the Craft AI MLOps\\nPlatform, so it can access the repository.\\n\\nPlease follow these steps:\\n\\nYou will need to generate an SSH key on your computer.\\n\\nOn Linux and macOS\\n\\nMove to a new directory and run the following command, by\\nreplacing *your-key-filename* by a name of\\nyour choosing.\\nssh-keygen -m PEM -t rsa -b 4096 -f *your-key-filename* -q -N \"\" -C \"\"\\nThis will generate two files: a file named\\n*your-key-filename* with the “private key” used\\nfor creating a step, and a file named\\n*your-key-filename*.pub with the “public key”\\nused to create the Deploy Key on GitHub. These files\\nshould not be included in the step’s directory. Only the\\ntype of SSH key generated by this command is accepted\\nwhen creating the Step.\\nExample on Linux / macOS :\\nssh-keygen -m PEM -t rsa -b 4096 -f *keyAccesPlatformCraftAI* -q -N \"\" -C \"\"\\n\\n\\n\\n\\nOn window\\n\\n\\nCheck that OpenSSH is installed and install it if it\\nis not the case. (Go in Settings > Apps & features >\\nOptional feature to get list of features and click on\\nadd feature to find and install OpenSSH)\\nPress the Windows key.\\nType cmd.\\nUnder Best Match, right-click Command Prompt.\\nClick Run as Administrator.\\nIf prompted, click Yes in\\nthe ``Do you want to allow this app to make changes to your device?``\\npop-up.\\nIn the command prompt, type the following : ssh-keygen\\nBy default, the system will save the keys to C:Usersyour_username/.ssh/id_rsa. You\\ncan use the default name, or you can choose more\\ndescriptive names. This can help distinguish between\\nkeys, if you are using multiple key pairs. To stick to\\nthe default option, press  Enter.\\nYou’ll be asked to enter a\\npassphrase. Hit Enter to skip this step.\\nThe system will generate the key pair, and display\\nthe key fingerprint and a randomart image.\\nOpen your file browser.\\nNavigate\\nto C:Usersyour_username/.ssh.\\nYou should see two files. The identification is saved\\nin the id_rsa file and the public key is\\nlabeled id_rsa.pub. This is your SSH key pair.\\n\\n\\n\\n\\n\\n\\nFor GitHub :\\n\\nHead to the homepage of your repository on GitHub.\\nGo to the Settings page.\\nOnce there, select the tab on the left named Deploy Keys\\nSelect Add deploy key on the Deploy Keys page.\\n\\n\\nInsert the name you want for your deploy key\\nCopy/paste the public key (content of\\n*your-key-filename*.pub) in the second text box.\\nClick on “Add key” (you don’t need to allow write\\naccess)\\n\\nFor GitLab :\\n\\nHead to the homepage of your repository on GitLab.\\nClick on Settings (left bar) then go to Repository\\nClick on Expand in the Deploy keys section\\nInsert the name you want for your deploy key.\\nCopy/paste the public key (content of\\n*your-key-filename*.pub) in the second text box.\\nClick on Add key (you don’t need to Grant write permissions to this key)\\n\\n\\n\\nDefault branch : Enter the Git branch you want as default\\nfor this project. If this field is empty, we will use the default\\nGit branch. It will be possible to choose a different default\\nbranch within an environment.\\n[Mandatory]Folders : Enter file(s) or folder(s) that will be\\naccessible by the platform. By default, type / to have access\\nto all the repositories. It will be possible to choose different\\nfiles or folders within an environment.\\nRequirements.txt : Enter the path to the requirements.txt file\\nwith the list of library to install automatically on this project.\\nIt will be possible to add a different file within an environment.\\nSystem dependencies : Enter the list of the APT and/or APK\\npackages that you want to install automatically on this project\\n(for Ubuntu distribution). It will be possible to add different\\npackages within an environment.\\n\\n\\nClick on “Create project”, you will see the project card displayed,\\nand you can enter in it to create a first environment.\\n\\nThere are no access rights at first, all users of the platform have\\naccess to all projects.\\n\\n\\nNote: You will be able to modify these elements (except the\\nproject name) in the Settings section of your project.\\n\\nNext step: Create an environment in this project.\\n\\n\\nManage a project\\uf0c1\\n\\nEdit a project\\uf0c1\\nWithin your project, click on Settings to view and edit your project\\ninformation.\\n\\nOn this page, you will find the information you set up when you created\\nthe project.\\nYou can change them, except the name of the project.\\n\\nWarning\\nDon’t forget to save and validate the confirmation slider\\nto make the changes effective.\\n\\nThe changes will apply to steps created after this modification. These\\nchanges may affect your steps and can make them non-functional.\\n\\n\\n\\nUsers in a project\\uf0c1\\nInitially, all users of the platform have access to all projects without\\nspecial access rights.\\n\\n\\n   🆕 The access rights per user within a project will arrive later on the\\n   platform.\\n\\n\\n\\nDelete a project\\uf0c1\\nInitially, you cannot delete a project.\\nSubmit an email request to delete a project from the platform.\\n\\n\\n   🆕 The deletion of a project will arrive later on the platform.\\n\\n\\n\\n\\n\\nAccess the SDK\\uf0c1\\nThe Craft AI MLOps Platform is composed of a Python SDK allowing you\\nto use, from your IDE, the functions developed by Craft AI to create\\nyour steps, pipelines and deployments.\\nSummary:\\n\\nGet token access\\nConnect to the SDK\\n\\n\\nGet token access\\uf0c1\\nIn the header, click on your name and go to the “Parameters” page.\\n\\nAt the bottom of the “Account” page, you will find your SDK token,\\nwhich will allow you to identify yourself to use the SDK.\\nYou can regenerate it, the old token will be obsolete, and you will\\nhave to identify yourself again to the SDK to access it.\\n\\nWarning\\nThe SDK token is strictly personal. You must keep it to yourself.\\n\\n\\n\\n\\nConnect to the SDK\\uf0c1\\nYou must install the SDK from a Python terminal, with the command :\\npip install craft-ai-sdk\\n\\n\\nRun the following commands in a Python terminal to initialize the\\nSDK.\\n\\nEnter the CRAFT_AI_ENVIRONMENT_URL, you can find on Get\\nenvironment URL.\\nIt should look like https://brisk-hawk-charcoal-zeta-42.mlops-platform.craft.ai.\\nIf you don’t have any environments in your project yet, you should\\nCreate an environment.\\n\\nEnter your personal CRAFT_AI_SDK_TOKEN, you can find it in “Parameters” on the platform web UI.\\nexport CRAFT_AI_ENVIRONMENT_URL=\"*your-env-URL*\"\\nexport CRAFT_AI_SDK_TOKEN=\"*your-token-acces*\"\\n\\n\\n\\nExecute the following Python code to set up SDK Python object.\\nThe SDK will automatically take into account your environment variables for the installation of the connection to the platform.\\nimport os\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk()\\n\\n\\nYou can also specify it directly in the constructor, although this method is not recommended.\\nimport os\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk(\\n    environment_url=\"*your-env-URL*\",\\n    sdk_token=\"*your-token-acces*\"\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en'})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(url_list)\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 100,\n",
    "    length_function = len,\n",
    "    add_start_index = True,)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Introduction — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nIntroduction', metadata={'source': 'https://mlops-platform-documentation.craft.ai/', 'title': 'Introduction — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nIntroduction\\n\\n\\n\\n\\n\\n\\n\\n\\nIntroduction\\uf0c1\\nYou are a Data Scientist struggling with data, code, and models in your projects.\\nYou are an ML Engineer who has trouble replicating pipelines and monitoring models in production.\\nYou are Head of a Data Science team, who has trouble shipping quickly models to production.\\nYou are a CTO who has difficulty to move Python code into large-scale production and manage the DevOps workload on an AI project.\\n🚀 Then, you are a potential user of Craft AI’s MLOps platform, which aims to accelerate the deployment and improve the management of your Machine Learning models in production.\\n\\nWhat is Craft AI?\\uf0c1\\nCraft AI is an MLOps platform for data science teams.\\nTo use Craft AI, the basic workflow with the platform is:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/', 'title': 'Introduction — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 825}), Document(page_content='Choose a configured environment on the cloud provider of your choice\\nCreate Machine Learning pipelines with your Python code\\nDeploy and execute the pipelines on environments running on Kubernetes\\nMonitor the performance of the models in production and the health of the infrastructure\\n\\nThe platform aims to be an end-to-end MLOps tool that brings together all the MLOps functionalities required for the successful implementation of an AI project. It is therefore composed of the following main features:\\n\\nEnvironments\\nMachine Learning Pipelines\\nDeployments', metadata={'source': 'https://mlops-platform-documentation.craft.ai/', 'title': 'Introduction — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1620}), Document(page_content='Environments\\nMachine Learning Pipelines\\nDeployments\\n\\n\\n\\n\\nMission\\uf0c1\\nOur mission is to democratize the use of trustworthy Artificial Intelligence on a day-to-day basis. How do we do this? By empowering Data Science teams to master their AI project from start to finish.  Our vision of AI is that every project of Data Science should be in production, responsible and profitable!\\nTo achieve this, we have developed a MLOps platform that allows anyone to put Python code into production, on a large scale, in a few clicks. We allow Data Scientist to deploy their models, choose their environments (development and production) and create pipelines to optimize their ML workflows.\\nHow does it work? We will contain your code in step to allow you to create ready to production ML and DL pipelines in an environment adapted to each project’s needs.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/', 'title': 'Introduction — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2125}), Document(page_content='History\\uf0c1\\nCraft AI is a French company founded in 2015. We originally developed AI solutions for the energy, industry, healthcare, education and retail sectors. Our unique ability to deploy thousands of ML models at large scale with a focus on being always explainable, energy frugal and fair, drove us to develop a MLOps platform as a front end to our expertise. Today, with our MLOps platform, we can share our expertise in large-scale model deployment, at large scale.\\n\\n\\nWhat is MLOps?\\uf0c1\\nMachine Learning Operations (MLOps), aims to provide an end-to-end development process to design, build and manage reproducible, testable, and evolvable ML-powered software.\\nBeing an emerging field, MLOps is rapidly gaining momentum amongst Data Scientists, ML Engineers and AI enthusiasts. Following this trend, MLOps differentiates the ML models management from traditional software engineering like DevOps and suggests the following MLOps capabilities:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/', 'title': 'Introduction — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2966}), Document(page_content='MLOps aims to unify the release/production cycle for ML and software application release.\\nMLOps enables automated testing of machine learning artifacts (e.g. data validation, ML model testing, and ML model integration testing)ML\\nMLOps enables the application of agile principles to machine learning projects.\\nMLOps enables supporting machine learning models and datasets to build these models as first-class citizens within CI/CD systems.\\nMLOps reduces technical debt across machine learning models.\\nMLOps must be a language-, framework-, platform-, and infrastructure-agnostic practice.\\n\\n\\n\\nBenefits of using Craft AI\\uf0c1\\nThe main benefits of the platform for the users are:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/', 'title': 'Introduction — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3912}), Document(page_content='Benefits of using Craft AI\\uf0c1\\nThe main benefits of the platform for the users are:\\n\\nNo longer taking 6 months to deploy ML models in production but only a few clicks!\\nAllowing a Data Science team to be autonomous on AI in production without DevOps skills.\\nEnabling large scale production of Python code without refactoring to Java or C.\\nAutomating the execution of the pipelines to save time for Data Science teams.\\nEnsuring an efficient use of computing resources and reduce the cloud bill.\\nImproving model performance over time by automatically triggering re-training pipelines when performance drops.\\n\\n\\n\\nSDK Python technical documentation\\uf0c1\\nsdk_0.31.2.pdf\\n\\n\\n\\n\\n\\n\\n\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/', 'title': 'Introduction — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4503}), Document(page_content='Administration — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nAdministration', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nAdministration\\n\\n\\n\\n\\n\\n\\n\\n\\nAdministration\\uf0c1\\nThe MLOps - Craft AI Platform is composed of a graphical interface\\nallowing you to visualize, create, manage and monitor the objects\\nnecessary for the realization of your AI projects.\\nIt is composed of:\\n\\nHomepage: Visualize and create projects.\\nParameters: Manage the company’s account and users.\\nProject settings: Manage the configuration of a project.\\nEnvironments: See the environment(s) and their information within\\na project.\\nExecutions: See the execution(s) and their information within an\\nenvironment or a deployment.\\n\\n\\nNote: We strongly recommend Google Chrome browser to use the\\ngraphical interface of the platform.\\n\\nSummary:\\n\\nUsers\\nProjects\\nToken SDK\\n\\n\\nUsers\\uf0c1\\nSummary:\\n\\nManage user\\nLogin\\nGet user with ID\\n\\n\\nManage user\\uf0c1\\nThe management of user is available by email for the moment. In a next\\nversion, it will be possible to add, edit and delete a user directly on\\nthe platform UI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 828}), Document(page_content='Add a user\\uf0c1\\nSend a message to Craft AI with your request and the following\\ninformation:\\n\\nFirst and last name of the user\\nEmail of the user\\n\\n\\n\\n   🆕 Adding users will arrive later on the platform.\\n\\n\\n\\nAccess rights\\uf0c1\\nEach user has access to one or more defined projects.\\nEach user who has access to a project has access to all the information and actions in it.\\n\\n\\n   🆕 Advanced access rights will arrive later on the platform.\\n\\n\\n\\nDelete a user\\uf0c1\\nSend a message to Craft AI with your request and the following\\ninformation:\\n\\nFirst and last name of the user\\nEmail of the user\\n\\n\\n\\n   🆕 The deletion of users will arrive later on the platform.\\n\\n\\n\\n\\nLogin\\uf0c1\\nHere is the URL to access the platform:\\nhttps://mlops-platform.craft.ai\\n\\nFirst connection\\uf0c1\\nPrerequisites: The user must be added to the platform (Add a user).', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1806}), Document(page_content='First connection\\uf0c1\\nPrerequisites: The user must be added to the platform (Add a user).\\n\\nThe user connects to the platform: https://mlops-platform.craft.ai\\nThe connection will not work, but Craft AI will receive the request\\nand can add the user.\\nThe user receives an email/slack from Craft AI to inform him that he\\ncan log in.\\nThe user logs in with the same link and has access to the platform.\\n\\n\\n\\nForgot your password\\uf0c1\\n\\nIn the Login popup, click on Don’t remember your password?\\nEnter your email, you will receive an email to modify your password.\\n\\n\\n\\n\\nGet user with ID\\uf0c1\\n\\nFunction definition\\uf0c1\\nWhile using the SDK you may encounter outputs parameters containing a user when using specific functions. Each user is identified with a unique ID. That is the case for example in the sdk.get_pipeline_execution() function output with the parameter created_by. To match the user ID with the corresponding information (name and email) you can use the get_user() function.\\nCraftAiSdk.get_user(user_id)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2523}), Document(page_content='Parameters\\uf0c1\\n\\nuser_id (str) – The ID of the user.\\n\\n\\n\\nReturns\\uf0c1\\nThe user information in dict type, with the following keys:\\n\\nid (str): ID of the user.\\nname (str): Name of the user.\\nemail (str): Email of the user.\\n\\n\\n\\n\\n\\nProjects\\uf0c1\\nA project is a complete use case. From data import, through\\nexperimentation, testing and production, to performance monitoring over\\ntime.\\nUsers can create as many projects as they want. Each project is\\nhermetically sealed from the others. All users on the platform can\\naccess to all projects.\\nTo work on a project, it must include at least one environment. The\\nuser can create several environments in a project.\\nSummary:\\n\\nCreate a project\\nManange a prject\\n\\n\\nCreate a project\\uf0c1\\n\\nFrom the homepage, you can create a new project by clicking on the\\n“New project” button.\\nA project creation page opens in which you have to fill in the fields\\n:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3517}), Document(page_content='[Mandatory]Project name : Enter the name of your project (in\\nlowercase and with “-”), you will not be able to modify it later.\\n[Mandatory]Python version : Select the version of Python\\nthat will be applied to this project. It will be possible to\\nchoose a different version when creating each step.\\n[Mandatory]Repository URL : Enter the SSH URL of your repository.\\n\\nHow to get my repository URL\\nOn GitHub, from your repository, click on the “Code” button and\\nchoose SSH. The URL must start with “git@”.\\n\\nOn GitLab, from your repository, click on the “Clone” button and\\ncopy SSH URL. The URL must start with “git@”.\\n\\n\\n\\n[Mandatory]Deploy key : Enter your Github / GitLab private key.\\nHow to generate my deploy key :', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4381}), Document(page_content='[Mandatory]Deploy key : Enter your Github / GitLab private key.\\nHow to generate my deploy key :\\n\\nFor security reasons, to get access to your GitHub project, the\\nplatform uses a Deploy Key with the RSA SSH KEY standard. The\\ndeploy key is a special key that grants access to a specific\\nrepository; it is not the same as personal keys used commonly\\nby users to access their repositories, although they are both\\nSSH keys.\\nThe deploy key has two elements:\\n\\nThe public key, which must be set in the GitHub\\nadministration settings for the repository.\\nThe private key, which must be sent to the Craft AI MLOps\\nPlatform, so it can access the repository.\\n\\nPlease follow these steps:\\n\\nYou will need to generate an SSH key on your computer.\\n\\nOn Linux and macOS', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4997}), Document(page_content='You will need to generate an SSH key on your computer.\\n\\nOn Linux and macOS\\n\\nMove to a new directory and run the following command, by\\nreplacing *your-key-filename* by a name of\\nyour choosing.\\nssh-keygen -m PEM -t rsa -b 4096 -f *your-key-filename* -q -N \"\" -C \"\"\\nThis will generate two files: a file named\\n*your-key-filename* with the “private key” used\\nfor creating a step, and a file named\\n*your-key-filename*.pub with the “public key”\\nused to create the Deploy Key on GitHub. These files\\nshould not be included in the step’s directory. Only the\\ntype of SSH key generated by this command is accepted\\nwhen creating the Step.\\nExample on Linux / macOS :\\nssh-keygen -m PEM -t rsa -b 4096 -f *keyAccesPlatformCraftAI* -q -N \"\" -C \"\"\\n\\n\\n\\n\\nOn window', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5671}), Document(page_content='Check that OpenSSH is installed and install it if it\\nis not the case. (Go in Settings > Apps & features >\\nOptional feature to get list of features and click on\\nadd feature to find and install OpenSSH)\\nPress the Windows key.\\nType cmd.\\nUnder Best Match, right-click Command Prompt.\\nClick Run as Administrator.\\nIf prompted, click Yes in\\nthe ``Do you want to allow this app to make changes to your device?``\\npop-up.\\nIn the command prompt, type the following : ssh-keygen\\nBy default, the system will save the keys to C:Usersyour_username/.ssh/id_rsa. You\\ncan use the default name, or you can choose more\\ndescriptive names. This can help distinguish between\\nkeys, if you are using multiple key pairs. To stick to\\nthe default option, press  Enter.\\nYou’ll be asked to enter a\\npassphrase. Hit Enter to skip this step.\\nThe system will generate the key pair, and display\\nthe key fingerprint and a randomart image.\\nOpen your file browser.\\nNavigate\\nto C:Usersyour_username/.ssh.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6417}), Document(page_content='Open your file browser.\\nNavigate\\nto C:Usersyour_username/.ssh.\\nYou should see two files. The identification is saved\\nin the id_rsa file and the public key is\\nlabeled id_rsa.pub. This is your SSH key pair.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7320}), Document(page_content='For GitHub :\\n\\nHead to the homepage of your repository on GitHub.\\nGo to the Settings page.\\nOnce there, select the tab on the left named Deploy Keys\\nSelect Add deploy key on the Deploy Keys page.\\n\\n\\nInsert the name you want for your deploy key\\nCopy/paste the public key (content of\\n*your-key-filename*.pub) in the second text box.\\nClick on “Add key” (you don’t need to allow write\\naccess)\\n\\nFor GitLab :\\n\\nHead to the homepage of your repository on GitLab.\\nClick on Settings (left bar) then go to Repository\\nClick on Expand in the Deploy keys section\\nInsert the name you want for your deploy key.\\nCopy/paste the public key (content of\\n*your-key-filename*.pub) in the second text box.\\nClick on Add key (you don’t need to Grant write permissions to this key)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7531}), Document(page_content='Default branch : Enter the Git branch you want as default\\nfor this project. If this field is empty, we will use the default\\nGit branch. It will be possible to choose a different default\\nbranch within an environment.\\n[Mandatory]Folders : Enter file(s) or folder(s) that will be\\naccessible by the platform. By default, type / to have access\\nto all the repositories. It will be possible to choose different\\nfiles or folders within an environment.\\nRequirements.txt : Enter the path to the requirements.txt file\\nwith the list of library to install automatically on this project.\\nIt will be possible to add a different file within an environment.\\nSystem dependencies : Enter the list of the APT and/or APK\\npackages that you want to install automatically on this project\\n(for Ubuntu distribution). It will be possible to add different\\npackages within an environment.\\n\\n\\nClick on “Create project”, you will see the project card displayed,\\nand you can enter in it to create a first environment.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8286}), Document(page_content='There are no access rights at first, all users of the platform have\\naccess to all projects.\\n\\n\\nNote: You will be able to modify these elements (except the\\nproject name) in the Settings section of your project.\\n\\nNext step: Create an environment in this project.\\n\\n\\nManage a project\\uf0c1\\n\\nEdit a project\\uf0c1\\nWithin your project, click on Settings to view and edit your project\\ninformation.\\n\\nOn this page, you will find the information you set up when you created\\nthe project.\\nYou can change them, except the name of the project.\\n\\nWarning\\nDon’t forget to save and validate the confirmation slider\\nto make the changes effective.\\n\\nThe changes will apply to steps created after this modification. These\\nchanges may affect your steps and can make them non-functional.\\n\\n\\n\\nUsers in a project\\uf0c1\\nInitially, all users of the platform have access to all projects without\\nspecial access rights.\\n\\n\\n   🆕 The access rights per user within a project will arrive later on the\\n   platform.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 9272}), Document(page_content='🆕 The access rights per user within a project will arrive later on the\\n   platform.\\n\\n\\n\\nDelete a project\\uf0c1\\nInitially, you cannot delete a project.\\nSubmit an email request to delete a project from the platform.\\n\\n\\n   🆕 The deletion of a project will arrive later on the platform.\\n\\n\\n\\n\\n\\nAccess the SDK\\uf0c1\\nThe Craft AI MLOps Platform is composed of a Python SDK allowing you\\nto use, from your IDE, the functions developed by Craft AI to create\\nyour steps, pipelines and deployments.\\nSummary:\\n\\nGet token access\\nConnect to the SDK\\n\\n\\nGet token access\\uf0c1\\nIn the header, click on your name and go to the “Parameters” page.\\n\\nAt the bottom of the “Account” page, you will find your SDK token,\\nwhich will allow you to identify yourself to use the SDK.\\nYou can regenerate it, the old token will be obsolete, and you will\\nhave to identify yourself again to the SDK to access it.\\n\\nWarning\\nThe SDK token is strictly personal. You must keep it to yourself.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 10148}), Document(page_content='Warning\\nThe SDK token is strictly personal. You must keep it to yourself.\\n\\n\\n\\n\\nConnect to the SDK\\uf0c1\\nYou must install the SDK from a Python terminal, with the command :\\npip install craft-ai-sdk\\n\\n\\nRun the following commands in a Python terminal to initialize the\\nSDK.\\n\\nEnter the CRAFT_AI_ENVIRONMENT_URL, you can find on Get\\nenvironment URL.\\nIt should look like https://brisk-hawk-charcoal-zeta-42.mlops-platform.craft.ai.\\nIf you don’t have any environments in your project yet, you should\\nCreate an environment.\\n\\nEnter your personal CRAFT_AI_SDK_TOKEN, you can find it in “Parameters” on the platform web UI.\\nexport CRAFT_AI_ENVIRONMENT_URL=\"*your-env-URL*\"\\nexport CRAFT_AI_SDK_TOKEN=\"*your-token-acces*\"\\n\\n\\n\\nExecute the following Python code to set up SDK Python object.\\nThe SDK will automatically take into account your environment variables for the installation of the connection to the platform.\\nimport os\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk()', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 11007}), Document(page_content='sdk = CraftAiSdk()\\n\\n\\nYou can also specify it directly in the constructor, although this method is not recommended.\\nimport os\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk(\\n    environment_url=\"*your-env-URL*\",\\n    sdk_token=\"*your-token-acces*\"\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 11950}), Document(page_content='Get Started — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nGet Started', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/Setup.html', 'title': 'Get Started — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nGet Started\\n\\n\\n\\n\\n\\n\\n\\n\\nGet Started\\uf0c1\\nThe goal of this use case is to accompany you in understanding the mechanics of the platform with a simple and basic machine learning application.\\nThe final goal of this get started will be to be able to generate\\npredictions on the Craft AI platform. The model used to generate predictions\\nwill be the iris one, but you can use any Python code.\\nTo achieve this goal, we will go through several parts:\\n\\n\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\n\\nPart 0 : Setup\\uf0c1\\nThis page is about the setup of the platform in your Python code and in\\nthe Craft AI platform UI.\\nThere are 2 ways to access the platform:\\n\\nWith the Python SDK, in line of code\\nWith the web interface, in a browser\\n\\nPrerequisites', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/Setup.html', 'title': 'Get Started — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 825}), Document(page_content='With the Python SDK, in line of code\\nWith the web interface, in a browser\\n\\nPrerequisites\\n\\nPython 3.8 or higher is required to be installed on your\\ncomputer.\\nWe strongly recommend the use of Google Chrome browser to use the\\nUI of the platform.\\n\\n\\n\\n*Pipelines have one step for the moment. Multi-step pipelines will be\\navailable soon.\\nWe’ve already created for you :\\n\\nA project\\nA repository GitHub linked to the project\\nAn environment setup in the project with datastore and workers\\n\\n\\nSet up the Python SDK\\uf0c1\\nThe Python SDK can be installed with\\npip from a terminal.\\npip install craft-ai-sdk\\n\\n\\nFor this use case, you also need NumPy if you don’t have already\\ninstalled it on your machine.\\npip install numpy\\n\\n\\n\\n\\nInitialization of Python SDK\\uf0c1\\nRun the following commands in a Python terminal to initialize the SDK:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/Setup.html', 'title': 'Get Started — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1657}), Document(page_content='Set the value of CRAFT_AI_ENVIRONMENT_URL into a local environment\\nvariable with your environment URL (you should get it from the Environment page on the UI. Environnement URL is NOT the HTML address of the UI).\\nSet the value of CRAFT_AI_SDK_TOKEN into a local\\nenvironment variable (On the UI, click on your name in the top right corner and then go to the “Parameters” page. There you can generate and find your SDK token at the bottom. The SDK token is strictly personal. You must keep it to yourself.)\\nA good practice would be to create a script per environment that\\ncontains the setup of these 2 local environment variables. For example,\\nyou could create an .env-test-platform-craft-ai file.\\n\\n#!/bin/bash\\n\\nexport CRAFT_AI_ENVIRONMENT_URL=\"*your-env-URL*\"\\nexport CRAFT_AI_ACCESS_TOKEN=\"*your-token-acces*\"\\n\\n\\n\\n\\nHere, we use the source command to define the values of the\\nvariables written to the file.\\nsource .env-test-platform-craft-ai', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/Setup.html', 'title': 'Get Started — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2466}), Document(page_content='Execute the following Python code to set up SDK Python object.\\nimport os\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk(\\n    environment_url=os.environ.get(\"CRAFT_AI_ENVIRONMENT_URL\"),\\n    sdk_token=os.environ.get(\"CRAFT_AI_ACCESS_TOKEN\")\\n)\\n\\n\\n🎉 Well done! You’re ready to execute your first code on the platform!\\n\\n\\n\\n\\nWhat’s next ?\\uf0c1\\nNow that we have configured the platform, we can create our first\\nobjects and run a “Hello World” on the platform.\\nNext step: Part 1: Deploy a simple pipeline\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/Setup.html', 'title': 'Get Started — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3407}), Document(page_content='Get Started — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nGet Started', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/Setup.html', 'title': 'Get Started — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nGet Started\\n\\n\\n\\n\\n\\n\\n\\n\\nGet Started\\uf0c1\\nThe goal of this use case is to accompany you in understanding the mechanics of the platform with a simple and basic machine learning application.\\nThe final goal of this get started will be to be able to generate\\npredictions on the Craft AI platform. The model used to generate predictions\\nwill be the iris one, but you can use any Python code.\\nTo achieve this goal, we will go through several parts:\\n\\n\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\n\\nPart 0 : Setup\\uf0c1\\nThis page is about the setup of the platform in your Python code and in\\nthe Craft AI platform UI.\\nThere are 2 ways to access the platform:\\n\\nWith the Python SDK, in line of code\\nWith the web interface, in a browser\\n\\nPrerequisites', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/Setup.html', 'title': 'Get Started — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 825}), Document(page_content='With the Python SDK, in line of code\\nWith the web interface, in a browser\\n\\nPrerequisites\\n\\nPython 3.8 or higher is required to be installed on your\\ncomputer.\\nWe strongly recommend the use of Google Chrome browser to use the\\nUI of the platform.\\n\\n\\n\\n*Pipelines have one step for the moment. Multi-step pipelines will be\\navailable soon.\\nWe’ve already created for you :\\n\\nA project\\nA repository GitHub linked to the project\\nAn environment setup in the project with datastore and workers\\n\\n\\nSet up the Python SDK\\uf0c1\\nThe Python SDK can be installed with\\npip from a terminal.\\npip install craft-ai-sdk\\n\\n\\nFor this use case, you also need NumPy if you don’t have already\\ninstalled it on your machine.\\npip install numpy\\n\\n\\n\\n\\nInitialization of Python SDK\\uf0c1\\nRun the following commands in a Python terminal to initialize the SDK:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/Setup.html', 'title': 'Get Started — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1657}), Document(page_content='Set the value of CRAFT_AI_ENVIRONMENT_URL into a local environment\\nvariable with your environment URL (you should get it from the Environment page on the UI. Environnement URL is NOT the HTML address of the UI).\\nSet the value of CRAFT_AI_SDK_TOKEN into a local\\nenvironment variable (On the UI, click on your name in the top right corner and then go to the “Parameters” page. There you can generate and find your SDK token at the bottom. The SDK token is strictly personal. You must keep it to yourself.)\\nA good practice would be to create a script per environment that\\ncontains the setup of these 2 local environment variables. For example,\\nyou could create an .env-test-platform-craft-ai file.\\n\\n#!/bin/bash\\n\\nexport CRAFT_AI_ENVIRONMENT_URL=\"*your-env-URL*\"\\nexport CRAFT_AI_ACCESS_TOKEN=\"*your-token-acces*\"\\n\\n\\n\\n\\nHere, we use the source command to define the values of the\\nvariables written to the file.\\nsource .env-test-platform-craft-ai', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/Setup.html', 'title': 'Get Started — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2466}), Document(page_content='Execute the following Python code to set up SDK Python object.\\nimport os\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk(\\n    environment_url=os.environ.get(\"CRAFT_AI_ENVIRONMENT_URL\"),\\n    sdk_token=os.environ.get(\"CRAFT_AI_ACCESS_TOKEN\")\\n)\\n\\n\\n🎉 Well done! You’re ready to execute your first code on the platform!\\n\\n\\n\\n\\nWhat’s next ?\\uf0c1\\nNow that we have configured the platform, we can create our first\\nobjects and run a “Hello World” on the platform.\\nNext step: Part 1: Deploy a simple pipeline\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/Setup.html', 'title': 'Get Started — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3407}), Document(page_content='Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nGet Started\\nPart 1: Execute a simple pipeline', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='home\\nGet Started\\nPart 1: Execute a simple pipeline\\n\\n\\n\\n\\n\\n\\n\\n\\nPart 1: Execute a simple pipeline\\uf0c1\\nThe main goal of the Craft AI platform is to allow to deploy easily your\\nmachine learning pipelines. In this first part, you will learn how you\\ncan execute some simple code on the platform.\\n\\nOverview of the first use case\\uf0c1\\nIn this part we will use the platform to build a simple “hello\\nworld” application by showing you how to execute a basic Python code\\nthat prints “Hello world” and displays the number of days until 2025.\\nYou will learn how to:\\n\\nPackage your application code into a step on the platform\\nEmbed it in a pipeline\\nExecute it on the platform\\nCheck the logs of the executions on the web interface', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 890}), Document(page_content='Create a step with the SDK\\uf0c1\\nThe first thing to do to build an application on the Craft AI platform\\nis to create a step.\\nA Step is the equivalent of a Python function in the Craft AI platform.\\nLike a regular function, a step is defined by the inputs it\\ningests, the code it runs, and the outputs it returns. For this\\n“hello world” use case, we are focusing on the code part so we will\\nignore inputs and outputs for now.\\nA step is created from any function located in the source code on your\\nrepository, using the create_step() method of thesdk object.\\nIt is very important to understand that the platform can only create\\nsteps from the code present in your GitHub / Gitlab repository in the branch\\nspecified during setup. If you have some uncommitted changes, they won’t\\nbe taken into account at step creation.\\nWe’ve added this code to your repository in the get_started folder:\\nimport datetime\\n\\ndef helloWorld() -> None:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1599}), Document(page_content=\"def helloWorld() -> None:\\n\\n    now = datetime.datetime.now()\\n    difference = datetime.datetime(2025, 1, 1) - now\\n\\n    print(f'Hello world ! Number of days to 2025 : {difference}')\\n\\n\\nOur helloworld function is located\\nin src/part-1-helloWorld.py so we can create our first step on the platform\\nas follow:\\nsdk.create_step(\\n    step_name='part-1-hello-world',\\n    function_path='src/part-1-helloWorld.py',\\n    function_name='helloWorld'\\n)\\n\\n\\nIts main arguments are:\\n\\nThe step_name is the name of the step that will be created. This\\nis the identifier you will use later to refer to this step.\\nThe function_path argument is the path of the Python module\\ncontaining the function that you want to execute for this step. This\\npath must be relative to the root of the git repository.\\nThe function_name argument is the name of the function that you\\nwant to execute for this step.\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2494}), Document(page_content=\"The above code should give you the following output:\\n>>> Please wait while step is being created. This may take a while...\\n>>> Steps creation succeeded\\n>>> {'name': 'part-1-hello-world'}\\n\\n\\nYou can view the list of steps that you created in the platform with the\\nlist_steps() function of the SDK.\\nstep_list = sdk.list_steps()\\nprint(step_list)\\n\\n\\n>>> [{'name': 'part-1-hello-world',\\n>>>   'created_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>>   'updated_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>>   'status': 'Ready',\\n>>>   'repository_branch': 'main',\\n>>>   'commit_id': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx',\\n>>>   'repository_url': 'git@github.com:xxxxx/xxxxx.git'}]\\n\\n\\nYou can see your step and its status of creation at Ready.\\nYou can also get the information of a specific step with the\\nget_step() function of the SDK.\\nstep_info = sdk.get_step('part-1-hello-world')\\nprint(step_info)\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3365}), Document(page_content=\">>> {\\n>>>   'parameters': {\\n>>>     'step_name': 'part-1-hello-world',\\n>>>     'function_path': 'src/part-1-helloWorld.py',\\n>>>     'function_name': 'helloWorld',\\n>>>     'description': None,\\n>>>     'container_config': {\\n>>>       'repository_branch': 'main',\\n>>>       'repository_url': 'git@github.com:xxxxx/xxxxx.git',\\n>>>       'repository_deploy_key': 'xxx ... xxx',\\n>>>       'dockerfile_path': None\\n>>>     },\\n>>>     'inputs': [],\\n>>>     'outputs': []\\n>>>   },\\n>>>   'creation_info': {\\n>>>     'commit_id': 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx',\\n>>>     'created_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>>     'updated_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>>     'status': 'Ready'\\n>>>   }\\n>>> }\\n\\n\\n🎉 Now your step has been created. You can now create your Pipeline (and\\nafter that, you’ll execute it on the platform).\\n\\n\\nCreate a pipeline with the SDK\\uf0c1\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4243}), Document(page_content=\"Create a pipeline with the SDK\\uf0c1\\n\\nThe step part-1-hello-world containing our helloWorld code is\\nnow created in the platform and ready to be used in a pipeline that\\nwe will then be executed.\\nA pipeline is a machine learning workflow, consisting of one or\\nmore steps, that can be easily deployed on the Craft AI platform.\\nThis way, you can create a full pipeline formed with a directed acyclic graph (DAG) by\\nspecifying the output of one step as the input of another step.\\nIn the future, it will be possible to assemble multiple steps into a complex machine\\nlearning pipeline. For now, the platform only allows single step\\npipelines.\\nTo create a pipeline consisting of the previous step, you must use the\\ncreate_pipeline() function of the SDK.\\nsdk.create_pipeline(\\n    pipeline_name='part-1-hello-world',\\n    step_name='part-1-hello-world',\\n)\\n\\n\\nThis function has two arguments:\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5068}), Document(page_content=\"This function has two arguments:\\n\\nThe pipeline_name is the name of the pipeline you have just\\ncreated. As for the step_name you will then refer to the pipeline\\nusing this name\\nThe step_name is the name of the step used in the pipeline.\\n\\nAfter executing this function, you should see the following output :\\n>>> Pipeline creation succeeded\\n>>> {'pipeline_name': 'part-1-hello-world',\\n>>> 'created_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>> 'steps': ['part-1-hello-world'],\\n>>> 'open_inputs': [],\\n>>> 'open_outputs': []}\\n\\n\\n🎉 Now that our pipeline is created (around our step), we want to execute\\nit. To do this, we will run the pipeline with the sdk function, run_pipeline(), and it will execute the code contained in the step.\\n\\n\\nExecute your pipeline (run)\\uf0c1\\nYou can execute  a pipeline on the platform directly with the run_pipeline() function.\\nThis function has two arguments:\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5910}), Document(page_content=\"The name of the existing pipeline to execute (pipeline_name)\\nOptional (only if you have inputs): a dict of inputs to pass to the pipeline with input names as dict keys and corresponding values as dict values.\\n\\nsdk.run_pipeline(pipeline_name='part-1-hello-world')\\n\\n\\n>>> The pipeline execution may take a while, you can check its status and get information on the Executions page of the front-end.\\n>>> Its execution ID is 'part-1-hello-world-xxxxx'.\\n>>> Pipeline execution results retrieval succeeded\\n>>> Pipeline execution startup succeeded\\n\\n\\n🎉 Now, you have created a step for the helloWorld\\nfunction, included it in a pipeline and execute it on the platform! Our hello world\\napplication is built and ready to be executed again!\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6781}), Document(page_content=\"Get information about an execution\\uf0c1\\nNow, we have executed the pipeline.\\nThe return of the function allows us to see that the pipeline has been successfully executed, however\\nit does not give us the logs of the\\nexecution (we can receive outputs with the return of the run pipeline, but\\nwe did not put any here).\\nIt is possible to have the returns of the executions with the SDK, let’s\\nsee how it works.\\nOnce your pipeline is executed, you can now see the\\npipeline executions with the sdk.list_pipeline_executions()\\ncommand.\\nsdk.list_pipeline_executions(\\n    pipeline_name='part-1-hello-world'\\n)\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7512}), Document(page_content=\">>> [{'execution_id': 'part-1-hello-world-XXXX',\\n>>> 'status': 'Succeeded',\\n>>> 'created_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>> 'end_date': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>> 'created_by': 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxx',\\n>>> 'pipeline_name': 'part-1-hello-world',\\n>>> 'deployment_id': 'xxxxxxxx-xxxx-xxxx-xxx-xxxxxxxx',\\n>>> 'steps':\\n>>>     [{'name': 'part-1-hello-world',\\n>>>       'status': 'Succeeded',\\n>>>       'end_date': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n>>>       'start_date': 'xxxx-xx-xxTxx:xx:xx.xxxZ'}]}]\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8108}), Document(page_content=\"Furthermore, you can get the specific logs of an execution with the\\nsdk.get_pipeline_execution_logs() command. You will have to fill\\nin the execution ID, which can be found with the previous command. The\\nlogs are given to us line by line in JSON, however, we can display them\\nclearly with the command in the print() below. This way, we will\\nalso be able to see the error messages of the step code through this if\\nthe execution of the code encounters any.\\npipeline_executions = sdk.list_pipeline_executions(\\n    pipeline_name='part-1-hello-world'\\n)\\n\\nlogs = sdk.get_pipeline_execution_logs(\\n    pipeline_name='part-1-hello-world',\\n      execution_id=pipeline_executions[-1]['execution_id'] # [-1] to get the last execution\\n)\\n\\nprint('\\\\n'.join(log['message'] for log in logs))\\n\\n\\n>>> Please wait while logs are being downloaded. This may take a while…\\n>>> Hello world ! Number of days to 2024 : xxx\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8620}), Document(page_content='To be able to find more easily the list of executions as well as the\\ninformation and associated logs, you can use the user interface, as\\nfollows:\\n\\nConnect to\\nhttps://mlops-platform.craft.ai\\nClick on your project:\\n\\n\\n\\n\\nClick on the Execution page and on “Select an execution”: this displays the list of\\nenvironments:\\n\\n\\n\\n\\nSelect your environment to get the list of runs and deployments:\\n\\n\\n\\n\\nFinally, click on a run name to get its executions:\\n\\n\\n\\n\\nYou have the “General” tab to get general information about your\\nexecution and the “Logs” tab where you can see and download the execution\\nlogs:\\n\\n\\n\\n\\n\\n🎉 You can now get your execution’s logs.\\n\\nWhat we have learned\\uf0c1\\nIn this part we learned how to easily build, deploy and use a simple\\napplication with the Craft AI platform with the following workflow:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 9516}), Document(page_content='These 3 main steps are the fundamental workflow to work with the\\nplatform and we will see them over and over throughout this tutorial.\\nNow that we know how to run our code on the platform, it is time to\\ncreate more complex steps to have a real ML use case.\\nNext step : Part 2: Execute a simple ML model\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_1.html', 'title': 'Part 1: Execute a simple pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 10312}), Document(page_content='Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nGet Started\\nPart 2: Execute a simple ML model\\n\\n\\n\\n\\n\\n\\n\\n\\nPart 2: Execute a simple ML model\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='home\\nGet Started\\nPart 2: Execute a simple ML model\\n\\n\\n\\n\\n\\n\\n\\n\\nPart 2: Execute a simple ML model\\uf0c1\\n\\nIntroduction\\uf0c1\\nThe previous part showed the main concepts of the platform and how to use the basics of the SDK.\\nWith what you already know you are able to execute really simple pipelines.\\nBut in order to build more realistic applications, using more complex code on your data with dependencies such\\nas Python libraries, it is needed to learn more advanced functionnalities and especially how to configure\\nthe execution context of a step and how to retrieve data stored on the platform.\\nThis page will present the same commands as the previous ones going through more available\\nfunctionalities offered by the platform, with a real Machine Learning use case.\\nWe will improve this Machine Learning application later in Part 3 and Part 4.\\nYou can find all the code used in this part and its structure here\\nPrerequisites', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 890}), Document(page_content='Python 3.8 or higher is required to be installed on your\\ncomputer.\\nHave done the previous Part of this tutorial (Part 1: Execute a simple pipeline).\\n\\n\\n\\nFirst Machine Learning use case\\uf0c1\\nHere we will build a pipeline to train and store a simple ML model with the iris\\ndataset.\\n\\nOverview of the use case\\uf0c1\\nThe iris dataset describes four features (petal length, petal width,\\nsepal length, sepal width) from three different types of irises (Setosa,\\nVeriscolour, Virginica).\\n\\nThe goal of our application is to classify flower type based on the\\nprevious four features. In this part we will start our new use case\\nby retrieving the iris dataset from the Data Store (that we will introduce just below), building a pipeline to train a simple ML model on the\\ndataset and store it on the Data Store.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1801}), Document(page_content='Storing Data on the Platform\\uf0c1\\nThe Data Store is a file storage on which you can upload and download unlimited files\\nand organize them as you want using the SDK.\\nAll your steps can download and upload files from and to the Data Store.\\nPushing the iris dataset to the Data Store\\nIn our case the first thing we want to do is to upload the iris dataset to the Data Store.\\nYou can do so with the upload_data_store_object function from the SDK like so:\\nfrom io import BytesIO\\nfrom sklearn import datasets\\nimport pandas as pd\\n\\niris = datasets.load_iris(as_frame=True)\\niris_df = pd.concat([iris.data, iris.target], axis=1)\\n\\nfile_buffer = BytesIO(iris_df.to_parquet())\\nsdk.upload_data_store_object(\\n   filepath_or_buffer=file_buffer,\\n   object_path_in_datastore=\"get_started/dataset/iris.parquet\"\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2592}), Document(page_content='The argument filepath_or_buffer can be a string or a file-like object.\\nIf a string, it is the path to the file to be uploaded,\\nif a file-like object you have to pass an IO object (something you don’t write to the disk\\nbut stay in the memory). Here we choose to use a BytesIO object.\\n\\n\\nThe code we want to execute\\uf0c1\\nWe will use the following code that trains a sklearn KNN classifier on\\nthe iris dataset from the Data Store and put the trained model on the Data Store.\\nimport joblib\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndef trainIris():\\n\\n   sdk = CraftAiSdk()\\n\\n   sdk.download_data_store_object(\\n      object_path_in_datastore=\"get_started/dataset/iris.parquet\",\\n      filepath_or_buffer=\"iris.parquet\",\\n   )\\n   dataset_df = pd.read_parquet(\"iris.parquet\")\\n\\n   X = dataset_df.loc[:, dataset_df.columns != \"target\"].values\\n   y = dataset_df.loc[:, \"target\"].values', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3384}), Document(page_content='np.random.seed(0)\\n   indices = np.random.permutation(len(X))\\n\\n   n_train_samples = int(0.8 * len(X))\\n   train_indices = indices[:n_train_samples]\\n   val_indices = indices[n_train_samples:]\\n\\n   X_train = X[train_indices]\\n   y_train = y[train_indices]\\n   X_val = X[val_indices]\\n   y_val = y[val_indices]\\n\\n   knn = KNeighborsClassifier()\\n   knn.fit(X_train, y_train)\\n\\n   mean_accuracy = knn.score(X_val, y_val)\\n   print(\"Mean accuracy:\", mean_accuracy)\\n\\n   joblib.dump(knn, \"iris_knn_model.joblib\")\\n\\n   sdk.upload_data_store_object(\\n      \"iris_knn_model.joblib\", \"get_started/models/iris_knn_model.joblib\"\\n   )\\n\\n\\n\\n\\nCleaning object\\uf0c1\\nBefore we really start building our new use case, we might want to\\nclean the platform from the objects we created in Part\\n1.\\nTo do this, we need to use the functions associated with each object,\\nhere the pipeline and the step.\\n\\nWarning\\nThese objects have dependencies on each other, we have\\nto delete them in a certain order. First the pipeline,\\nand then the step.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4340}), Document(page_content='sdk.delete_pipeline(pipeline_name=\"part-1-hello-world\")\\n\\nsdk.delete_step(step_name=\"part-1-hello-world\")\\n\\n\\n\\n\\n\\nIn depth step configuration\\uf0c1\\nIn the rest of this part we will follow the same workflow as in the\\nprevious one:\\n\\nNow it is time to use the create_step() method of craft-ai-sdk\\nobject to create step like before. This time though, we will see how we\\ncan define a bit more the step and its execution context. We are going\\nto focus on two parameters.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5336}), Document(page_content='Python libraries\\uf0c1\\nAs you might have noticed, the code above uses external Python libraries\\n(craft_ai_sdk, joblib, numpy, pandas and scikit learn). In the previous step we conveniently built an\\napplication that didn’t require any external dependency but this time if\\nwe want this code to work on the platform we have to inform it that this\\nstep requires some Python libraries to run properly.\\nTo do so we create a requirements.txt file,\\ncontaining the list of Python libraries used in our step function:\\ncraft_ai_sdk==xx.xx.xx\\njoblib==xx.xx.xx\\nnumpy==xx.xx.xx\\nscikit_learn==xx.xx.xx\\npandas==xx.xx.xx\\npyarrow==xx.xx.xx\\n\\n\\nIn this case we place it at the root of the repo, but you can put it wherever you want.\\n\\nWarning\\nAs for the code, the platform only sees what’s on your repository so don’t\\nforget to push your requirement file on your Git repository.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5793}), Document(page_content='You can now specify the path by default of this file in the Libraries & Packages section of your project\\nsettings using the web interface and all the steps created in this project will have these libraries\\ninstalled and ready to be used.\\n\\n\\nIncluded folder\\uf0c1\\nWhen creating your step, you may not need to include all the files of\\nyour project repositories in your step. You can specify the file(s) and\\nfolder(s) to include from the GitHub / GitLab repository to prevent the step from\\naccessing all code available in the repository by default, using the\\nincluded_folders option.\\nOnce again you can set this option in your project settings page in the\\nweb interface in the Folder field.\\n🎉 Now all the steps created in this project will have the relevant\\nlibraries installed and only the necessary files will be included', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6646}), Document(page_content='Create a step\\uf0c1\\nWhen you created your project in the platform (cf Part 0), you have set up different parameters\\n(like the repository URL, the deploy key or the Python version you are using) and we also set up new parameters in\\nthe previous section.\\nBy default, your step will apply those parameters during its creation (that is why you didn’t need to add any parameters\\nwhen creating your step in Part 1).\\nHowever, sometimes you want to define them only at the step level and keep the previous ones at the project level.\\nThis is the role of the create_step() function’s parameter container_config . You can pass as a dictionary the\\nset of parameters you want to use for the step creation. It allows you to be really specific in the execution\\ncontext of your step.\\n💡 For example, if you need to build a step embedding a function from\\nanother repository, you can specify the new Git repository URL and\\ndeploy key at the step level using the container_config parameter.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7465}), Document(page_content='deploy key at the step level using the container_config parameter.\\nYour project parameters will remain unchanged.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8364}), Document(page_content='Warning\\nThe execution context of a step is non persistent.\\nIt means that as soon as the execution is done, everything written in\\nmemory and on disk during the execution will be deleted. Therefore, a\\nstep is not the right place to store data or any information. You should use\\nthe Data Store instead.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8479}), Document(page_content='Here we will specify the requirements_path and the\\nincluded_folders.\\nNote: You can specify them in the user interface at the project\\nlevel as well. Just remember that they will be used by default if you\\ndon’t specify others at the step level.\\nIn order for our requirements.txt file to be taken into account, we\\nmust also add it to the container_config parameter with the\\nrequirements_path\\nsdk.create_step(\\n    step_name=\"part-2-iristrain\",\\n    function_path=\"src/part-2-irisModel.py\",\\n    function_name=\"trainIris\",\\n    description=\"This function creates a classifier model for iris\",\\n    container_config = {\\n        \"requirements_path\" : \"requirements.txt\", #put here the path to the requirements.txt\\n        \"included_folders\" : [\"src\"]\\n    }\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8780}), Document(page_content='It may also be useful to describe precisely the steps created to be able\\nto understand their purpose afterward. To do so, you can fill in the\\ndescription parameter during the step creation.\\nℹ️ To go further with step creation If you want to create a step\\nbased on the code of a repository not indicated in the project (or just\\non another branch), you can also specify it during the step creation.\\nYou can find all the function arguments you can modify here.\\n🎉 Now your step has been created. You can now create your Pipeline.\\nFrom here, we reproduce the same steps as before with the creation of\\nthe pipeline and we execute it.\\n\\n\\nCreate a pipeline\\uf0c1\\nCreate a pipeline with the create_pipeline() method of the SDK.\\nsdk.create_pipeline(\\n    pipeline_name=\"part-2-iristrain\",\\n        step_name=\"part-2-iristrain\"\\n)\\n\\n\\n\\n\\nExecute your Pipeline (run) and get the execution logs\\uf0c1\\nNow you can execute your pipeline as in Part\\n1.\\nsdk.run_pipeline(pipeline_name=\"part-2-iristrain\")', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 9530}), Document(page_content='To be able to find more easily the list of executions as the\\ninformation and associated logs, you can use the user interface, like in the previous part.\\nThe output is a list of iris categories :\\n>> [2 0 2 0 2 2 0 0 2 0 0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2\\n1 1 1 2 2 2 1 0 1 2 2 0 1 1 2 1 0 0 0 2 1 2 0]\\n\\n\\n🎉 You can now execute a more realistic Machine Learning pipeline.\\nNow that we can have more complex code in our steps and we know how to\\nparametrize the execution context of our steps, we would like to be able to\\ngive it input elements to vary the result and receive the result easily.\\nFor this, we can use the input/output feature offered by the platform.\\nNext step: Part 3: Execute with input and output\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_2.html', 'title': 'Part 2: Execute a simple ML model — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 10502}), Document(page_content='Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nGet Started\\nPart 3: Execute a ML use case with inputs and outputs', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='home\\nGet Started\\nPart 3: Execute a ML use case with inputs and outputs\\n\\n\\n\\n\\n\\n\\n\\n\\nPart 3: Execute a ML use case with inputs and outputs\\uf0c1\\n\\nIntroduction\\uf0c1\\nIn Part 2, we have built and run our first ML pipeline to retrieve\\ndata from the data store, train a model and store it on the data store.\\nWhat if we want to use our pipeline to perform predictions with this\\ntrained model on new data ? Currently, we can not pass new data to our\\nmodel.\\n⇒ We need to add an Input to our pipeline.\\nMoreover, what if we want to provide these predictions to a final user?\\n⇒ We need to add an Output to our pipeline.\\nThis part will show you how to do this with the Craft AI platform:\\n\\nWe will first create the code of the predictIris() function\\nso that it can receive data and return predictions.\\nThen, we will see how to create a step, a pipeline and run it on the platform\\nwith input data and return the corresponding predictions\\nas an output.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 910}), Document(page_content='By the end of this part, we will have built a runable pipeline that allows\\nto get the predictions of the iris species on new data with a\\nsimple execution:\\nPrerequisites\\n\\nPython 3.8 or higher is required to be installed on your\\ncomputer.\\nHave done the previous parts of this tutorial ( Part 0.\\nSetup,\\nPart 1: Execute a simple pipeline\\nand Part 2: Execute a simple ML model\\n).\\n\\n\\n\\nMachine Learning use case with I/O\\uf0c1\\nHere we will build a pipeline to retrieve the trained model stored in the last part\\nand make a prediction on new data.\\n\\nOverview of the use case\\uf0c1\\n\\n\\n\\nThe code we want to execute\\uf0c1\\nFirst we have to implement our code to compute predictions with a stored model on\\nthe data store on any (correctly prepared) data given as input instead of computing\\npredictions on a test set.\\nHence, our file src/part-3-iris-predict.py is as follows:\\nfrom io import BytesIO\\nfrom craft_ai_sdk import CraftAiSdk\\nimport joblib\\nimport pandas as pd\\n\\n\\ndef predictIris(input_data: dict, input_model_path:str):', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1834}), Document(page_content='def predictIris(input_data: dict, input_model_path:str):\\n\\n   sdk = CraftAiSdk()\\n\\n   f = BytesIO()\\n   sdk.download_data_store_object(input_model_path, f)\\n   model = joblib.load(f)\\n\\n   input_dataframe = pd.DataFrame.from_dict(input_data, orient=\"index\")\\n   predictions = model.predict(input_dataframe)\\n\\n   final_predictions = predictions.tolist()\\n\\n   return {\"predictions\": final_predictions}\\n\\n\\nIn this code:\\n\\nWe add the argument input_data. Here, we choose it to be a\\ndictionary like the one below:\\n{\\n    1: {\\n        \\'sepal length (cm)\\': 6.7,\\n      \\'sepal width (cm)\\': 3.3,\\n      \\'petal length (cm)\\': 5.7,\\n      \\'petal width (cm)\\': 2.1\\n    },\\n  2: {\\n      \\'sepal length (cm)\\': 4.5,\\n      \\'sepal width (cm)\\': 2.3,\\n      \\'petal length (cm)\\': 1.3,\\n      \\'petal width (cm)\\': 0.3\\n  },\\n}\\n\\n\\nIt contains the data on which we want to compute predictions.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2772}), Document(page_content='We retrieve our trained model with the download_data_store_object() function of the sdk by passing the model path.\\nAt the end, we convert our input_data dictionary into a Pandas\\ndataframe, and we compute predictions with our trained model.\\nAs you can see, the function now returns a Python dict with one\\nfield called “predictions” that contains the predictions value. The\\nplatform only accepts step function with one return value of type\\n``dict``. Each item of this dict will be an output of the step and\\nthe key associated with each item will be the name of this output on\\nthe platform.\\nMoreover, you can see that we converted our result from a numpy\\nndarray to a list. That is because the values of the inputs and\\noutputs are restricted to native Python types such as int, float,\\nbool, string, list and dict with elements of those types. More\\nprecisely anything that is json-serializable. Later, the platform\\nmight handle more complex input and output types such as numpy array', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3619}), Document(page_content='might handle more complex input and output types such as numpy array\\nor even pandas dataframe.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4530}), Document(page_content='Dont forget to update your requirements.txt file,\\ncontaining the list of Python libraries used in our step function:\\njoblib==xx.xx.xx\\npandas==xx.xx.xx\\ncraft_ai_sdk==xx.xx.xx\\n\\n\\n\\nWarning\\nSince we added a new functiion, we must add and commit our changes with\\nGit and push them to GitHub so that the platform can take them into\\naccount!\\n\\n\\n\\n\\nStep creation with Input and Output\\uf0c1\\nNow, let’s create our step on the platform. Here, since we have inputs and an output, our step is the combination of three elements: an\\ninput, an output and the Python function above. We will first declare\\nthe inputs and the output. Then, we will use the function\\nsdk.create_step() as in Part\\n2\\nto create the whole step.\\n\\n\\nDeclare Input and Output of our new step\\uf0c1\\nTo manage inputs and outputs of a step, the platform requires you to\\ndeclare them using the Input and Output classes from the\\nSDK.\\nFor our Iris application, the inputs and outputs declaration would\\nlook like this:\\nfrom craft_ai_sdk.io import Input, Output', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4626}), Document(page_content='prediction_input = Input(\\n   name=\"input_data\",\\n   data_type=\"json\"\\n)\\n\\nmodel_input = Input(\\n   name=\"input_model_path\",\\n   data_type=\"string\"\\n)\\n\\nprediction_output = Output(\\n   name=\"predictions\",\\n   data_type=\"json\"\\n)\\n\\n\\nBoth objects have two main attributes:\\n\\nThe name of the Input or Output\\n\\nFor the inputs it corresponds to the names of the arguments of\\nyour step’s function. In our case name=\"input_data\" and \"input_model_path\", as in\\nthe first line of function :\\ndef predictIris(input_data: dict, input_model_path:str):\\n\\n\\n\\nFor the output it must be a key in the dictionary returned by\\nyour step’s function. In our case, name=\"predictions\"as in\\nthe last line of function :\\nreturn {\"predictions\": final_predictions}\\n\\n\\n\\n\\n\\nThe data_type describing the type of data it can accept. It\\ncan be one of: string, number, boolean, json, array, file.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5623}), Document(page_content='For the inputs we want a dictionary and a string as we specified, which\\ncorresponds to data_type=\"json\" and data_type=\"string\".\\nFor the output, we return a dictionary which corresponds to\\ndata_type=\"json\".\\n\\n\\n\\nNow, we have everything we need to create, as\\nbefore,\\nthe step and the pipeline corresponding to our\\npredictIris() function.\\n\\n\\nCreate step\\uf0c1\\nNow as in Part 2, it is time to create our step on the platform using\\nthe sdk.create_step() function, but this time we specify our inputs\\nand output:\\nsdk.create_step(\\n   step_name=\"part-3-irisio\",\\n   function_path=\"src/part-3-iris-predict.py\",\\n   function_name=\"predictIris\",\\n   description=\"This function retrieves the trained model and classifies the input data by returning the prediction.\",\\n   inputs=[prediction_input, model_input],\\n   outputs=[prediction_output],\\n   container_config={\\n     \"included_folders\": [\"src\"],\\n     \"requirements_path\": \"requirements.txt\",\\n   },\\n)\\n\\n\\nThis is exclatly like in part 2 except for two parameters :', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6466}), Document(page_content='This is exclatly like in part 2 except for two parameters :\\n\\ninputs containing the list of Input objects we declared above\\n(here, prediction_input and model_input).\\noutputs containing the list of Output objects we declared\\nabove (here, prediction_output).\\n\\nWhen step creation is finished, you obtain an output describing your\\nstep (including its inputs and outputs) as below:\\n>> Step \"part-3-irisio\" created\\n  Inputs:\\n    - input_data (json)\\n    - input_model_path (string)\\n  Outputs:\\n    - predictions (json)\\n>> Steps creation succeeded\\n>> {\\'name\\': \\'part-3-irisio\\',\\n \\'inputs\\': [{\\'name\\': \\'input_data\\', \\'data_type\\': \\'json\\'}, {\\'name\\': \\'input_model_path\\', \\'data_type\\': \\'string\\'}],\\n \\'outputs\\': [{\\'name\\': \\'predictions\\', \\'data_type\\': \\'json\\'}]}\\n\\n\\nNow that our step is created in the platform, we can embed it in a\\npipeline and run it.\\n\\n\\n\\nCreate and run your pipeline\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7397}), Document(page_content='Create and run your pipeline\\uf0c1\\n\\nCreate pipeline\\uf0c1\\nLet’s create our pipeline here with sdk.create_pipeline() as in\\nPart\\n2:\\nsdk.create_pipeline(\\n   pipeline_name=\"part-3-irisio\",\\n   step_name=\"part-3-irisio\",\\n)\\n\\n\\nYou quickly obtain this output, which describes the pipeline, its step\\nand its inputs and outputs:\\n>> Pipeline creation succeeded\\n>> {\\'pipeline_name\\': \\'part-3-irisio\\',\\n \\'created_at\\': \\'xxxx-xx-xxTxx:xx:xx.xxxZ\\',\\n \\'steps\\': [\\'part-3-irisio\\'],\\n \\'open_inputs\\': [{\\'input_name\\': \\'input_data\\',\\n   \\'step_name\\': \\'part-3-irisio\\',\\n   \\'data_type\\': \\'json\\'}, {\\'input_name\\': \\'input_model_path\\',\\n   \\'step_name\\': \\'part-3-irisio\\',\\n   \\'data_type\\': \\'string\\'}],\\n \\'open_outputs\\': [{\\'output_name\\': \\'predictions\\',\\n   \\'step_name\\': \\'part-3-irisio\\',\\n   \\'data_type\\': \\'json\\'}]}\\n\\n\\n🎉 You’ve created your first step & pipeline with inputs and\\noutputs!\\nLet’s run this pipeline.\\n\\n\\n\\nRun the pipeline with new input data\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8228}), Document(page_content='Run the pipeline with new input data\\uf0c1\\n\\nPrepare input data\\uf0c1\\nNow, our pipeline needs data as input (formatted as we said\\nabove ⬆️). Let’s prepare it, simply by choosing some of the rows of iris\\ndataset we did not use when training our model:\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn import datasets\\n\\nnp.random.seed(0)\\nindices = np.random.permutation(150)\\niris_X, iris_y = datasets.load_iris(return_X_y=True, as_frame=True)\\niris_X_test = iris_X.loc[indices[90:120],:]\\n\\nnew_data = iris_X_test.to_dict(orient=\"index\")\\n\\n\\nLet’s check the data we created:\\nprint(new_data)\\n\\n\\nWe get the following output:\\n>> 124: {\\'sepal length (cm)\\': 6.7,\\n\\'sepal width (cm)\\': 3.3,\\n\\'petal length (cm)\\': 5.7,\\n\\'petal width (cm)\\': 2.1\\n},\\n41: {\\'sepal length (cm)\\': 4.5\\n...', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 9084}), Document(page_content='Finally, we need to encapsulate this dictionary in another one\\nwhose key is \"input_data\" (the name of the input of our step,\\ni.e.\\xa0the name of the argument of our step’s function).\\nWe define also the path to our trained model on the data store with the value\\nassociated to the key \"input_model_path\".\\ninputs = {\\n    \"input_data\": new_data,\\n    \"input_model_path\": \"get_started/models/iris_knn_model.joblib\"\\n}\\n\\n\\nIn particular, when your step has several inputs, this dictionary should\\nhave as many keys as the number of inputs the step have.\\n\\n\\nExecute the pipeline (RUN)\\uf0c1\\nFinally, we can execute our pipeline with the data we’ve just prepared by\\ncalling the run_pipeline() function almost as in Part 2 and passing our dictionary inputs\\nto the inputs arguments of the function:\\noutput_predictions = sdk.run_pipeline(\\n                        pipeline_name=\"part-3-irisio\",\\n                        inputs=inputs)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 9839}), Document(page_content='Finally, our output can be obtained like this:\\nprint(output_predictions[\"outputs\"][\\'predictions\\'])\\n\\n\\nThis gives the output we want (with the predictions!):\\n>> {\\'predictions\\': [0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2]}\\n\\n\\nMoreover, you can check the logs on the\\nUI, by clicking on the Executions\\ntab of your environment, selecting your pipeline and choosing the last\\nexecution.\\n🎉 Congratulations! You have run a pipeline to which we can pass\\nnew data, the path to our trained model and get predictions.\\nNext step: Part 4: Deploy a ML use case with inputs and outputs\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_3.html', 'title': 'Part 3: Execute a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 10749}), Document(page_content='Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nGet Started\\nPart 4: Deploy a ML use case with inputs and outputs', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='home\\nGet Started\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\n\\n\\n\\n\\n\\n\\nPart 4: Deploy a ML use case with inputs and outputs\\uf0c1\\n\\nIntroduction\\uf0c1\\nIn Part 3, we have built and run our second ML pipeline to retrieve our trained model from the data store,\\nprovide some new data to it as input and retrieve the result as an output of our pipeline execution.\\nWhat if we want to let an external user execute our predict pipeline?\\nOr if we want to schedule the execution of the pipeline that trains our model periodically?\\n⇒ We need to deploy one pipeline via an endpoint and another one with a scheduled execution.\\nThis part will show you how to do this with the Craft AI platform:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 909}), Document(page_content='We will first update the code of the predictIris() function\\nso that it can retrieve directly from the data store the trained model\\nand returns the predictions as a json to the user.\\nWe will also update the code of the trainIris() function\\nso that it re trains the model on a specific dataset (that could be often updated)\\nand uploads the trained model directly to the datastore.\\nThen, we will see how to create a step and a pipeline that we will deploy\\non the platform in two different ways, and that could be executed periodicly\\nand by a call.\\n\\nPrerequisites\\n\\nPython 3.8 or higher is required to be installed on your\\ncomputer.\\nHave done the previous parts of this tutorial ( Part 0.\\nSetup,\\nPart 1: Execute a simple pipeline,\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs).', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1587}), Document(page_content='Machine Learning application with I/O\\uf0c1\\nHere we will build an application based on what we did on the last part. We will expose our service\\nto external users and schedule periodic executions.\\n\\nOverview of the use case\\uf0c1\\n\\nTo get the predictions via and endpoint:\\n\\n\\n\\nTo retrain the model periodicly (we will focus on this case later):\\n\\n\\n\\n\\nThe code we want to execute\\uf0c1\\nWe will first focus on the construction of the endpoint the final user will be able to target.\\nFirst we have to update our code to retrieve directly the model from the data store\\nwithout any call to the sdk in the code and to return a file on the data store with the predictions inside.\\nHence, our file src/part-4-iris-predict.py is as follows:\\nimport joblib\\nimport pandas as pd\\nimport json\\n\\ndef predictIris(input_data: dict, input_model:dict):\\n\\n   model = joblib.load(input_model[\\'path\\'])\\n\\n   input_dataframe = pd.DataFrame.from_dict(input_data, orient=\"index\")\\n   predictions = model.predict(input_dataframe)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2406}), Document(page_content='return {\"predictions\": predictions.tolist()}\\n\\n\\nWhat changed are only how we get the trained model.\\nmodel = joblib.load(input_model[\\'path\\'])\\n\\n\\ninput_model is a dictionary in which the key path refers to the file’s path where is located the file\\non the step environnement.\\nThis input is a file data type.\\nDon’t forget to update your requirements.txt file,\\ncontaining the list of Python libraries used in our step function:\\njoblib==xx.xx.xx\\npandas==xx.xx.xx\\n\\n\\n\\nWarning\\nAs for the code, the platform only sees what’s on your repository so don’t\\nforget to push your requirements file on your Git repository.\\n\\n\\n\\n\\nStep creation with Input and Output\\uf0c1\\nAs we did in part 3, we will first declare\\nthe inputs and the output. Then, we will use the function\\nsdk.create_step() to create the whole step.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3385}), Document(page_content='Declare Input and Output of our new step\\uf0c1\\nThe only difference now is the data type we will assign\\nto input_model.\\nThis is now a file that we want to retrieve from the data store.\\nTo do so, we define the inputs and output like below:\\nfrom craft_ai_sdk.io import Input, Output\\n\\nprediction_input = Input(\\n   name=\"input_data\",\\n   data_type=\"json\"\\n)\\n\\nmodel_input = Input(\\n   name=\"input_model\",\\n   data_type=\"file\"\\n)\\n\\nprediction_output = Output(\\n   name=\"predictions\",\\n   data_type=\"json\"\\n)\\n\\n\\nWe have just seen the code of the step has been adapted to handle file objects.\\nNow, we have everything we need to create, as\\nbefore,\\nthe step and the pipeline corresponding to our\\npredictIris() function.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4176}), Document(page_content='Create your step\\uf0c1\\nNow as in Part 3, it is time to create our step on the platform using\\nthe sdk.create_step() function, with our inputs\\nand output:\\nsdk.create_step(\\n   step_name=\"part-4-iris-deployment\",\\n   function_path=\"src/part-4-iris-predict.py\",\\n   function_name=\"predictIris\",\\n   description=\"This function retrieves the trained model and classifies the input data by returning the prediction.\",\\n   inputs=[prediction_input, model_input],\\n   outputs=[prediction_output],\\n   container_config={\\n     \"included_folders\": [\"src\"],\\n     \"requirements_path\": \"requirements.txt\",\\n   },\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4872}), Document(page_content='When the step creation is finished, you obtain an output describing your\\nstep (including its inputs and outputs) as below:\\n>> Step \"part-4-iris-deployment\" created\\n  Inputs:\\n    - input_data (json)\\n    - input_model (file)\\n  Outputs:\\n    - predictions (json)\\n>> Steps creation succeeded\\n>> {\\'name\\': \\'part-4-iris-deployment\\',\\n \\'inputs\\': [{\\'name\\': \\'input_data\\', \\'data_type\\': \\'json\\'}, {\\'name\\': \\'input_model\\', \\'data_type\\': \\'file\\'}],\\n \\'outputs\\': [{\\'name\\': \\'predictions\\', \\'data_type\\': \\'json\\'}]}\\n\\n\\nNow that our step is created in the platform, we can embed it in a\\npipeline and deploy it.\\n\\n\\nCreate your pipeline\\uf0c1\\nLet’s create our pipeline here with sdk.create_pipeline() as in\\nPart\\n3:\\nsdk.create_pipeline(\\n   pipeline_name=\"part-4-iris-deployment\",\\n   step_name=\"part-4-iris-deployment\",\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5461}), Document(page_content=\"You quickly obtain this output, which describes the pipeline, its step\\nand its inputs and outputs:\\n>> Pipeline creation succeeded\\n>> {'pipeline_name': 'part-4-iris-deployment',\\n 'created_at': 'xxxx-xx-xxTxx:xx:xx.xxxZ',\\n 'steps': ['part-4-iris-deployment'],\\n 'open_inputs': [{'input_name': 'input_data',\\n   'step_name': 'part-4-iris-deployment',\\n   'data_type': 'json'}, {'input_name': 'input_model',\\n   'step_name': 'part-4-iris-deployment',\\n   'data_type': 'file'}],\\n 'open_outputs': [{'output_name': 'predictions',\\n   'step_name': 'part-4-iris-deployment',\\n   'data_type': 'json'}]}\\n\\n\\n🎉 You’ve created your second step & pipeline with inputs and\\noutput!\\n\\n\\n\\nCreate your deployments with input and output mappings\\uf0c1\\nHere, we want to be able to execute the pipeline, either by launching the execution with an url link or\\nat a certain time, but not by a run anymore.\\nLet’s try the first case.\\nWe want the user to be able to:\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6246}), Document(page_content='send the input data directly to the application via an url link\\nretrieve the results directly from the endpoint\\n\\nWe want also to specify the path to the stored model on the data store,\\nso that the service will take this model directly from the data store.\\nThe user won’t be the one selecting the model used, it’s only on the technical side.\\n\\nCreate the endpoint with IO mappings\\uf0c1\\nAn endpoint is a publicly accessible URL that launches the execution of the Pipeline.\\nWithout the platform, you would need to write an api with a library like Flask, Fast API or Django and deploy it on a server that you would have to maintain.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7170}), Document(page_content='IO Mappings\\uf0c1\\nWhen you start a new deployment, the data flow has to be configured with a mapping, that you can create with the sdk.\\nFor our endpoint, we have to define the IO mappings defined on the schema above, like this:\\ninputs_mapping_endpoint = [\\n   InputSource(\\n      step_input_name=\"input_model\",\\n      datastore_path=\"get_started/models/iris_knn_model.joblib\"\\n      ),\\n   InputSource(\\n      step_input_name=\"input_data\",\\n      endpoint_input_name=\"input_data\"\\n      )\\n]\\n\\noutput_mapping_endpoint = [\\n   OutputDestination(\\n      step_output_name=\"predictions\",\\n      endpoint_output_name=\"iris_type\")\\n]', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7797}), Document(page_content='Create the endpoint\\uf0c1\\nWith the platform you can create an endpoint with a simple call to the sdk.create_deployment() function of the SDK,\\nby choosing the endpoint for the argument execution_rule.\\nYou also have to specify a deployment_name, used to refer to the created endpoint and that is further used in its URL.\\nendpoint = sdk.create_deployment(\\n   execution_rule=\"endpoint\",\\n   pipeline_name=\"part-4-iris-deployment\",\\n   deployment_name=\"part-4-iris-endpoint\",\\n   inputs_mapping=inputs_mapping_endpoint,\\n   outputs_mapping=output_mapping_endpoint\\n)\\n\\n\\n\\n\\nTarget the endpoint\\uf0c1\\nPrepare the input data\\nNow, our endpoint needs data as input, like we did for last part:\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn import datasets\\n\\nnp.random.seed(0)\\nindices = np.random.permutation(150)\\niris_X, iris_y = datasets.load_iris(return_X_y=True, as_frame=True)\\niris_X_test = iris_X.loc[indices[90:120],:]\\n\\nnew_data = iris_X_test.to_dict(orient=\"index\")', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8410}), Document(page_content='new_data = iris_X_test.to_dict(orient=\"index\")\\n\\n\\nWe need to encapsulate this dictionary in another one\\nwhose key is \"input_data\" (the name of the input of our step,\\ni.e.\\xa0the name of the argument of our step’s function).\\nWe don’t need to define the path to our trained model because it is already defined with the output mapping we have just done.\\ninputs = {\\n    \"input_data\": new_data\\n}\\n\\n\\nCall the endpoint with the input data\\nendpoint_url = sdk.base_environment_url  + \"/endpoints/\" + endpoint[\"name\"]\\nendpoint_token = endpoint[\"endpoint_token\"]\\nrequest = requests.post(endpoint_url, headers={\"Authorization\": f\"EndpointToken {endpoint_token}\"}, json=inputs)\\nrequest.json()\\n\\n\\nThe HTTP code 200 indicates that the request has been taken into account. In case of an error, we can expect an error code starting with 4XX or 5XX.\\nIt is a way to execute your deployment. But, obviously, you can execute it in any other way (curl command in bash, Postman…).', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 9313}), Document(page_content=\"Warning\\nAs the request is based on the POST method, note that you can’t directly target your endpoint and recieve the output by entering it in your web navigator.\\n\\nLet’s check we can get the predictions as output of the endpoint:\\nprint(request.json()['outputs']['iris_type'])\\n\\n\\nMoreover, you can check the logs on the\\nUI, by clicking on the Executions\\ntab of your environment, selecting your pipeline and choosing the last\\nexecution.\\n🎉 You’ve created your first deployment and you’ve just called it!\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 10266}), Document(page_content='Retrain the model periodically\\uf0c1\\nLet’s imagine that our dataset is frequently updated, for instance we get new labeled\\niris data every day. In this case we might want to retrain our model by triggering our\\ntraining pipeline part4-iris-train every day.\\nThe platform can do this automatically using the periodic execution rule in our deployment.\\nA periodic execution rule allows to schedule a pipeline execution at a certain time.\\nFor example, every Monday at a certain time, every month, every 5 minutes etc.\\nThe inputs and output have to be defined, with a constant value or a data store mappings.\\nFirst we will update our trainIris function so that it produces a file output containing our model,\\nthat we will then map to the datastore.\\nYou can check the entire updated version of this function in src/part-4-iris-predict.py.\\nThe only change is done at the return of the function:\\n.. code:: python\\n\\nreturn {“model”: {“path”: “iris_knn_model.joblib”}}', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 10768}), Document(page_content='return {“model”: {“path”: “iris_knn_model.joblib”}}\\n\\nWe can then create the step and pipeline as we are used to.\\ntrain_output = Output(\\n   name=\"model\",\\n   data_type=\"file\"\\n)\\n\\nsdk.create_step(\\n   step_name=\"part-4-iristrain\",\\n   function_path=\"src/part-4-iris-predict.py\",\\n   function_name=\"trainIris\",\\n   outputs=[train_output],\\n)\\n\\nsdk.create_pipeline(\\n   pipeline_name=\"part-4-iristrain\",\\n   step_name=\"part-4-iristrain\"\\n)\\n\\n\\nNow let’s create a deployment that executes our pipeline every 5 minutes. In our case, we will map the prediction output\\n(which is our only I/O) to the datastore on the same path that is used in the pediction endpoint deployment.\\nThis way our prediction pipeline will automatically use the latest version of our model for predictions.\\nNote that the schedule argument takes the CRON syntax (examples here: https://crontab.guru/).', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 11667}), Document(page_content='Adapt IO mapping\\nLet’s create a deployment that will schedule our pipeline to be executed every 5 minutes with the same\\nIO mappings as in the endpoint, except that the new data is a constant input and not something we define during the execution.\\noutput_mapping_periodic = OutputDestination(\\n   step_output_name=\"model\",\\n   datastore_path=\"get_started/models/iris_knn_model.joblib\"\\n)\\n\\n\\nCreate periodic deployment\\nAnd now, below is how we create a deployment that will schedule our pipeline execution:\\nperiodic = sdk.create_deployment(\\n   execution_rule=\"periodic\",\\n   pipeline_name=\"part-4-iristrain\",\\n   deployment_name=\"part-4-iristrain\",\\n   schedule=\"*/5 * * * *\",\\n   outputs_mapping=[output_mapping_periodic],\\n\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 12524}), Document(page_content=')\\n\\n\\nOur training pipeline will now be executed every 5 minutes, updating our model with the potential new data.\\nThe predict pipeline will then use this updated model automatically.\\nYou can check that you actually have a new execution every 5 minutes using the sdk or via the web interface.\\n🎉 Congrats! You’ve created your second deployment and planned it to run every 5 minutes!\\n\\n\\n\\nConclusion\\uf0c1\\n🎉 After this Get Started, you have learned how to use the basic functionalities of the platform!\\nYou know now the entire workflow to create a pipeline and deploy it.\\n\\nYou are now able to:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 13239}), Document(page_content='You are now able to:\\n\\nDeploy your code through a pipeline in a few lines of code, run it whenever you want and have the logs to analyze the execution.\\nUse the Data Store on the platform to upload and download files, models, images, etc.\\nExecute your pipeline via an endpoint that is accessible from outside with a secured token, or via a periodic execution.\\nMake your inputs flexible: set constant values to avoid users to fill in, let users enter inputs values via the endpoint directly, or use the data store to retrieve or put objects.\\n\\n\\nNote\\n🎉 If you want to go further\\nOne concept has not been explained to you: the metrics.\\nIf you want to go further and discover this feature, you can read the associated documentation.\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 13800}), Document(page_content='User workflow — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nUser workflow', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/introduction.html', 'title': 'User workflow — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nUser workflow\\n\\n\\n\\n\\n\\n\\n\\n\\nUser workflow\\uf0c1\\nWelcome to the comprehensive documentation on the user workflow for accessing and deploying\\nyour applications on our platform. This guide will walk you through the essential steps\\nrequired to connect to the platform, access your data, and deploy your models effectively.\\nWhether you are new to the platform or an experienced user, this documentation will provide\\nyou with a step-by-step approach to make the most of our services.\\n\\nUser workflow\\n\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/introduction.html', 'title': 'User workflow — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 827}), Document(page_content='User workflow — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nUser workflow', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/introduction.html', 'title': 'User workflow — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nUser workflow\\n\\n\\n\\n\\n\\n\\n\\n\\nUser workflow\\uf0c1\\nWelcome to the comprehensive documentation on the user workflow for accessing and deploying\\nyour applications on our platform. This guide will walk you through the essential steps\\nrequired to connect to the platform, access your data, and deploy your models effectively.\\nWhether you are new to the platform or an experienced user, this documentation will provide\\nyou with a step-by-step approach to make the most of our services.\\n\\nUser workflow\\n\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/introduction.html', 'title': 'User workflow — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 827}), Document(page_content='Accessing your data — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nUser workflow\\nAccessing your data', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nUser workflow\\nAccessing your data\\n\\n\\n\\n\\n\\n\\n\\n\\nAccessing your data\\uf0c1\\nWhen executing your code within Pipelines on the Craft AI Platform, one crucial aspect is the ability to retrieve and store data. In this context, we will explore two approaches to acquiring data during our Pipeline Executions:\\n\\nUsing the Data Store provided by the Craft AI Platform, which offers a convenient way to store and retrieve objects on the environment.\\nConnecting to external data sources like you are used to. By accessing your own organization database, cloud external database, some open source data, data available via FTP or some data storage such as the classic ones offered by AWS, Azure, or Google Cloud Platform, we can expand the range of data available for our Pipeline Executions.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 833}), Document(page_content='By combining the capabilities of both the Data Store and external data sources, we can ensure a reliable and efficient data retrieval system for our executions. We will take a closer look at the techniques and practices of retrieving data for pipeline executions.\\n\\nWarning\\nBe aware that when an execution is launched on the platform, what is on its execution context is not persistent (i.e. it does not remain after the execution), all data kept in memory or on the disk is removed at the end of the execution. It is possible to read, write and manipulate data during the execution, but everything in the execution context at the end of the execution is deleted.\\nThis allows you to have an identical and stable execution context for each run, while avoiding needlessly saturating the disk with your executions.\\nTo ensure the persistence of your data, you can use data sources: It can be the Data Store, your own database or external data storage.\\n\\n\\nSummary\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1650}), Document(page_content='Summary\\uf0c1\\n\\nHow to store data on the Data Store\\nHow to retrieve data from the Data Store\\nHow to access an external Data Source\\n\\n\\n\\nHow to store data on the Data Store\\uf0c1\\nHere we will see how to store files created within a step.\\nTo do this, we will see 2 methods:\\n\\nWith the dedicated SDK function upload_data_store_object()\\nAdvantages:\\n\\nMore flexibility at the Data Store path level. This method allows you to upload files to any location on the Data Store, as you can easily change the file path by providing a different input to the code.\\n\\nDrawbacks:\\n\\nNeed to initialize the SDK in the step.\\nNo tracking (the file path values given as inputs are not stored).\\n\\n\\nWith an Output mapping to the Data Store\\nAdvantages:\\n\\nNo need to initialize the SDK on the step.\\ntracking of the file used in this execution.\\n\\nDrawbacks:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2599}), Document(page_content='Drawbacks:\\n\\nNeed to define the Storage location on the Dataupload_data_store_object() Store before creating the deployment and the path can’t be modified afterwards.\\nOnly one possible storage location.\\n\\n\\n\\n\\nWith the dedicated SDK function upload_data_store_object()\\uf0c1\\nYou can also access the Data Store directly from the step code by using the SDK function upload_data_store_object().\\n\\nWarning\\n⚠️ Note that you need to import the craft_ai_sdk library at the beginning of the code, and have it in your requirements.txt.\\n\\nThe SDK connection is initialized without token or environment URL, since the step will already be executed in the environment.\\nExample\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndef preprocess_data():\\n\\n    # SDK initialization\\n    sdk = CraftAiSdk()', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3400}), Document(page_content='def preprocess_data():\\n\\n    # SDK initialization\\n    sdk = CraftAiSdk()\\n\\n    # Using an existing function get_data() to get raw data\\n    df_data = get_data()\\n    # Using an existing function preprocess_data() to preprocess\\n    # the previously retrieved data\\n    df_preprocessed_data = preprocess_data(df_data)\\n    # The existing function create_csv()  writes the dataframe \\n    # df_preprocessed_data as a csv in path_to_preprocessed_data\\n    create_csv(df_preprocessed_data, path_to_preprocessed_data)\\n    \\n    # Upload into datastore with the SDK function upload_data_store_object()\\n    sdk.upload_data_store_object(\\n        filepath_or_buffer=path_to_preprocessed_data, \\n        object_path_in_datastore=\"data/preprocessed/data_preprocessed.txt\"\\n    )\\n\\n\\nThen, simply create the step and the pipeline to execute this code.\\n\\nWith an Output mapping to the Data Store\\uf0c1\\nHere we will see how to store files that were created within a step to the data store.\\nTo do so, we have a few points to follow:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4091}), Document(page_content='Adapt the code that will be executed in the step, especially by specifying the path on which the file is accessible on the step.\\n\\ndef yourFunction() :\\n    ...\\n    return {\"outputfile\" : \\n        {\"path\": **path of the file on the step**}\\n    }\\n\\n\\n\\nBefore creating the step, define the output of the step with a data_type as file, and create the step and the pipeline as we are used to.\\n\\nfrom craft_ai_sdk.io import Output\\n\\nstep_output = Output(\\n    name=step_output_name,\\n    data_type=\"file\", \\n)\\n\\nsdk.create_step(\\n    function_path=**the path to your function**,\\n    function_name=\"your_function\", \\n    step_name=**your step name**,\\n    outputs=[step_output]\\n)\\n\\nsdk.create_pipeline(pipeline_name=**your pipeline name**, \\n\\tstep_name=**your step name**)\\n\\n\\n\\nDefine the output mapping (with the sdk object OutputDestination) that will be used for the execution of your pipeline, in this case, a datastore_path is needed to map the output of the step to a specific path on the Data Store.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5090}), Document(page_content=\"from craft_ai_sdk.io import OutputDestination\\n\\noutput_mapping = OutputDestination(\\n\\tstep_output_name=step_output_name,\\n\\tdatastore_path=**path on which we want to store the file**,\\n)\\n\\nsdk.run_pipeline(\\n\\tpipeline_name=**your pipeline name**,\\n\\toutputs_mapping=[output_mapping]\\n)\\n\\n\\nExample\\nIn this part, we will create a deployment with an endpoint execution rule that returns 2 files, 1 file with text in .txt format and another with a fake confusion matrix in format .csv.\\nFirst, we write the code to create the 2 files.To be able to pass the files to the Data Store, we specify the paths of the 2 files on the step.\\n\\n\\n    📖 Note the step execution context is an isolated container, and it's the platform that will then copy the files from the execution context to the Data Store (thanks to the output we’ve created).\\n\\nDon’t forget to indicate the dependencies into requirements.txt.\\nimport numpy as np\\nimport pandas as pd\\n\\ndef createFiles() :\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6075}), Document(page_content='def createFiles() :\\n\\n    # Define file into local step contenaire \\n    path_text = \"file_text_output.txt\"\\n    path_matrix = \"confusion_matrix.csv\"\\n\\n    # Create a fake confusion matrix into .csv file\\n    confusion_matrix = np.array([[100, 20, 5],\\n                                [30, 150, 10],\\n                                [10, 5, 200]])\\n    class_labels = [\\'Class A\\', \\'Class B\\', \\'Class C\\'] # Define the class labels\\n    df = pd.DataFrame(confusion_matrix, index=class_labels, columns=class_labels) # Create a DataFrame from the confusion matrix\\n    df.to_csv(path_matrix, index=True, header=True) # Save the DataFrame as a CSV file\\n\\n    # Create .txt file\\n    text_file = open(path_text, \\'wb\\')  # Open the file in binary mode\\n    text_file.write(\"Result of step send in file output :) \".encode(\\'utf-8\\'))  # Encode the string to bytes\\n    text_file.close()', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6997}), Document(page_content='# Return the path of the file in the container of the current step execution.\\n    fileOjb = {\\n        \"txtFile\" : {\"path\": path_text}, \\n        \"csvFile\" : {\"path\": path_matrix}\\n    }\\n    return fileOjb\\n\\n\\n\\nWarning\\n⚠️ Remember to push step code to a GitHub repository defined in information project into Craft AI platform.\\n\\nAfter the initialization of SDK connection, we can create the 2 outputs and then the step.\\nFor this step, we assume that all the information is already specified in the project settings (language and repository information).\\n# Output creation\\nstep_output_txt = Output(\\n    name=\"txtFile\",\\n    data_type=\"file\", \\n)\\n\\nstep_output_csv = Output(\\n    name=\"csvFile\",\\n    data_type=\"file\", \\n)\\n\\n# Step creation with output (we supose repository is setup in info project)\\nsdk.create_step(\\n    function_path=\"src/createFiles.py\",\\n    function_name=\"createFiles\", \\n    step_name=\"doc-2o-datastore-step\",\\n    outputs=[step_output_txt, step_output_csv]\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7862}), Document(page_content='Now, we can create the pipeline.\\nsdk.create_pipeline(pipeline_name=\"doc-2o-datastore-pipeline\", \\n    step_name=\"doc-2o-datastore-step\")\\n\\n\\nTo create an accessible endpoint, we need to create a deployment with two output mappings to the Data Store we created earlier.\\nLet’s pretend we want to store the files at \"docExample/resultText.txt” and \"docExample/resultMatrix.csv\" on the Data Store.\\nendpoint_output_txt = OutputDestination(\\n    step_output_name=\"txtFile\",\\n    datastore_path=\"docExample/resultText.txt\",\\n)\\n\\nendpoint_output_csv = OutputDestination(\\n    step_output_name=\"csvFile\",\\n    datastore_path=\"docExample/resultMatrix.csv\",\\n)\\n\\nendpoint = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"doc-2o-datastore-pipeline\",\\n    deployment_name=\"doc-2o-datastore-dep\",\\n    outputs_mapping=[output_mapping_txt, output_mapping_csv]\\n)\\n\\n\\nAfter that, we can trigger the endpoint with the Python code below (we can use any other tool like postman, curl …).\\nimport requests', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8829}), Document(page_content='endpoint_url = **your-url-env**+\"/endpoints/doc-2o-datastore-dep\"\\nheaders = {\"Authorization\": \"EndpointToken \"+endpoint[\"endpoint_token\"]}\\n\\nresponse = requests.post(endpoint_url, headers=headers)\\n\\nprint (response.status_code, response.json())\\n\\n\\n\\n\\n\\n\\nHow to retrieve data from the Data Store\\uf0c1\\nHere, we will see how to retrieve files already stored on the Data Store within a step.\\nTo do this, we will see 2 methods:\\n\\nWith the dedicated SDK function download_data_store_object()\\nAdvantages:\\n\\nMore flexibility at the Data Store path level. This method allows you to download files from any location on the Data Store, as you can easily change the file path by providing a different input to the code.\\n\\nDrawbacks:\\n\\nNeed to initialize the SDK in the step.\\nNo tracking (the file path values given as inputs are not stored).\\n\\n\\nWith an Input mapping to the Data Store\\nAdvantages:\\n\\nNo need to initialize the SDK on the step.\\ntracking of the file used in this execution.\\n\\nDrawbacks:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 9826}), Document(page_content='Drawbacks:\\n\\nNeed to define the Storage location on the Data Store before creating the deployment and the path can’t be modified afterwards.\\nOnly one possible storage location.\\n\\n\\n\\n\\nWith the dedicated SDK function download_data_store_object()\\uf0c1\\nYou can access the Data Store directly from the step code by using the SDK function download_data_store_object().\\n\\n\\n✅ Note that you need to import the craft_ai_sdk library at the beginning of the code, and have it in your requirements.txt.\\n\\nThe connection is initialized without token or environment URL, since the Step will already be executed in the environment.\\nExample\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndef retrievePredictions(id_prediction: int):\\n\\n    # SDK initialization\\n    sdk = CraftAiSdk()', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 10787}), Document(page_content='def retrievePredictions(id_prediction: int):\\n\\n    # SDK initialization\\n    sdk = CraftAiSdk()  \\n\\n    # Download the file containing the predictions \\n    # of the id_prediction at the local path \"predictions.txt\"\\n    sdk.download_data_store_object(\\n        object_path_in_datastore=f\"data/predictions_{id_prediction}.csv\", \\n        filepath_or_buffer=\"predictions.txt\"\\n    )\\n\\n        # Open and print the content of the file now stored locally\\n    with open(\"predictions.txt\") as f:\\n        contents = f.readlines()\\n        print (contents)\\n\\n\\nThen, simply create the step and the pipeline to execute this code.\\n\\n\\nWith an Input mapping to the Data Store\\uf0c1\\nWe will need to define the Input as a file for the step, the input mapping that connects the Data Store to the step and therefore the code embedded within the step to correctly read the file. Let’s start with the latter.\\nTo do so, we have a few points to follow:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 11439}), Document(page_content='Adapt the code to access and read your file. Indeed, the input that will be passed to the step will have a predefined form as a dictionary with path as key and the file path as a value. You thus need to access it the same way you retrieve the value of a dictionary.\\nThe file will be downloaded in the execution environment before the step is executed. You can then use the file as you would use any other file in the execution environment.\\nHere, we have a function readFile that aims to read the input file and print its content.\\ndef read_file (entryFile: dict) :\\n\\n    # Access the file with its local path (entryFile[\"path\"]) on the step\\n    with open(entryFile[\"path\"]) as f:\\n        contents = f.readlines()\\n        print (contents)\\n\\n\\n\\nWarning\\n⚠️ One the code is updated, remember to push step code to a GitHub repository defined in information project into Craft AI platform.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 12356}), Document(page_content='Before creating the step, define the input of the step with a data_type as file, and create the step and pipeline as we are used to.\\n\\n\\n    ✅ We assume that all the information is already specified in the project settings (language, repository and branch information).\\n\\n\\n\\nfrom craft_ai_sdk.io import Input\\n\\n# Define the input of the step \\nstep_input = Input(\\n    name=\"entryFile\",\\n    data_type=\"file\",    \\n)\\n\\n# Create the step \\nsdk.create_step(\\n    function_path=\"src/read_file.py\",\\n    function_name=\"read_file\", \\n    step_name=\"file-datastore-step\",\\n    inputs=[step_input]\\n)\\n\\n# Create the pipeline\\nsdk.create_pipeline(pipeline_name=\"file-datastore-pipeline\", \\n    step_name=\"file-datastore-step\")\\n\\n\\nOnce the pipeline is created, we define the correct input mapping and run the pipeline with it.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 13238}), Document(page_content='Once the pipeline is created, we define the correct input mapping and run the pipeline with it.\\n\\nDefine the input mapping (with the SDK object InputSource) that will be used for the execution of your pipeline, in this case, a datastore_path is needed to map the input of the step to the file on the Data Store.\\nLet’s pretend the file we want to retrieve is stored at myFolder/text.txt on the Data Store.\\nfrom craft_ai_sdk.io import InputSource\\n\\ninput_mapping = InputSource(\\n    step_input_name=\"entryFile\"\\n    datastore_path=\"myFolder/text.txt\", # Path of the output file in the datastore\\n)\\n\\n# Run pipeline using mapping defined with InputSource object\\nsdk.run_pipeline(\\n\\tpipeline_name=\"file-datastore-pipeline\", \\n\\tinputs_mapping=[input_mapping]\\n)\\n\\n\\nYou can check the logs for your file content.\\npipeline_executions = sdk.list_pipeline_executions(\\n    pipeline_name=\"file-datastore-pipeline\"\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 13940}), Document(page_content='logs = sdk.get_pipeline_execution_logs(\\n    pipeline_name=pipeline_name, \\n    execution_id=pipeline_executions[0][\\'execution_id\\']\\n)\\n\\nprint(\\'\\\\n\\'.join(log[\"message\"] for log in logs))\\n\\n\\n\\n\\n\\n\\n\\nHow to access an external Data Source\\uf0c1\\nThe connection with an external data source (database or data storage) involves the following steps:\\n\\nUsing the same code you would use without the Craft AI platform to access your data storage. You only have to encapsulate your code within a step and a pipeline, like any code you would like to execute on the Craft AI platform. You also may have to adapt the inputs and outputs of your main function to respect the Craft AI formats.\\nEmbedding your credentials on your platform environment to use them in your step code to access the database.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 14835}), Document(page_content='🌟 To securely use credentials, a common good practice is to define **environment variables** to store the credentials safely. If you want to do so, the SDK offers you an easy solution: the `create_or_update_environment_variable()` function.\\nsdk.create_or_update_environment_variable(\"USERNAME\", \"username_db_1\")\\n\\n\\nThen, when you need the credentials, you access it with the following command :\\nimport os\\nimport dotenv\\n\\ndotenv.load_dotenv()\\n\\nusername = os.environ[\"USERNAME\"]\\n\\n\\n\\n\\n\\n\\nFrom an external Database\\uf0c1\\nMoreover, if you try to access to an external Database, we may have to whitelist the IP of your Craft AI platform environment(s) if necessary in your data source configuration. The environments IP are available on the page dedicated to your project environments.\\n\\nHere is an example of the few points to do in order to access a specific external database directly from our Craft AI environment platform by using the usual credentials and be able to filter on some data on the database:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 15611}), Document(page_content='🛡️ If needed, add the Craft AI environment platform URL to the whitelist of the external database you want to access.\\n\\n\\nFirst, we embed the credentials we usually use to access to the database in our environment by setting them as environment variables.\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_HOST\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_ADMIN\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_PASS\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_NAME\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"DB_PORT\",\\n    environment_variable_value=\"xxx\")', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 16610}), Document(page_content='Secondly, we encapsulate the code we would use without the Craft AI platform in a function, here filter_data_from_database(), that will be executed within a step:\\n\\nBelow, we use an existing function create_db_connection() that creates a connection to the database with the credentials brought on our Craft AI environment platform.\\nThen, we use an existing function filter_data() that retrieves some data in a specific table on the database by filtering with the ids inputs on a specific column.\\nAs input of our filter_data_from_database() function, we have this list of integer, ids, and as output we have a dictionary whose key is filtered_data. ids and filtered_data are respectively input and output of the step we would define after.\\n\\nHere is the corresponding code:\\nimport os\\nimport dotenv\\n\\ndotenv.load_dotenv()\\n\\nDB_HOST = os.environ[\"DB_HOST\"]\\nDB_ADMIN = os.environ[\"DB_ADMIN\"]\\nDB_PASS = os.environ[\"DB_PASS\"]\\nDB_NAME = os.environ[\"DB_NAME\"]\\nDB_PORT = os.environ[\"DB_PORT\"]', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 17483}), Document(page_content='def filter_data_from_database(ids: List[int]) :\\n    try:\\n        # With an existing function create_db_connection,\\n        # Creating a connection to the database with the \\n        # credentials brought on our Craft AI environment platform.\\n        conn = create_db_connection(\\n            host=DB_HOST,\\n            user=DB_ADMIN,\\n            password=DB_PASS,\\n            database=DB_NAME,\\n            port=DB_PORT\\n        )\\n\\n        # With an existing function filter_data, \\n        # Retrieving some data in a specific table on the database \\n        # by filtering with the ids inputs on a specific column.\\n        df_data_filtered = filter_data(conn, ids)\\n        df_data_filtered_final = df_data_filtered.tolist()\\n    finally:\\n        conn.close()\\n    return {\"filtered_data\": df_data_filtered_final}', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 18464}), Document(page_content='From an external Data Storage\\uf0c1\\nHere is an example of the few points to do in order to access a specific external data storage directly from a Craft AI environment platform by using the usual credentials and be able to retrieve one specific csv file:\\n\\nFirst, we embed the credentials we usually use to access to the data storage in our environment by setting them as environment variables.\\n\\nsdk = CraftAiSdk(\\n    sdk_token=**our-sdk-token**, \\n    environment_url=**our-environment-url**)\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"SERVER_PUBLIC_KEY\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"SERVER_SECRET_KEY\",\\n    environment_variable_value=\"xxx\")\\n\\nsdk.create_or_update_environment_variable(\\n    environment_variable_name=\"REGION_NAME\",\\n    environment_variable_value=\"xxx\")', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 19276}), Document(page_content='Secondly, we encapsulate the code we would use without the Craft AI platform in a function that will be executed within a step.\\n\\nBelow, we use an existing function configure_client() that configures a client (data storage specific) with the credentials brought on our Craft AI environment platform to access the data storage.\\nThen, we use an existing function get_object_from_bucket() **that, with the configured client, retrieves the object key in the bucket bucket on the data storage.\\n\\n\\n\\nimport os\\nimport dotenv\\n\\ndotenv.load_dotenv()\\n\\nSERVER_PUBLIC_KEY = os.environ[\"SERVER_PUBLIC_KEY\"]\\nSERVER_SECRET_KEY = os.environ[\"SERVER_SECRET_KEY\"]\\nREGION_NAME = os.environ[\"REGION_NAME\"]\\n\\ndef filter_data_from_data_storage(bucket: str, key:str)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 20159}), Document(page_content='def filter_data_from_data_storage(bucket: str, key:str)\\n\\n    # With an existing function configure_client, \\n    # Configuring a client (data storage specific) \\n    # with the credentials brought on our Craft AI environment platform.\\n    client = configure_client(\\n        public_key=SERVER_PUBLIC_KEY, \\n        secret_key=SERVER_SECRET_KEY, \\n        region=REGION_NAME)\\n\\n    # With an existing function get_object_from_bucket,\\n    # Retrieving the object key in the bucket bucket on the data storage.\\n    buffer = get_object_from_bucket(\\n        client=client,\\n        bucket_name=bucket,\\n        key=key\\n    )\\n\\n    dataframe = pd.read_csv(buffer)\\n    return dataframe\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/access_data.html', 'title': 'Accessing your data — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 20842}), Document(page_content='Run and serve your pipelines — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nUser workflow\\nRun and serve your pipelines', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nUser workflow\\nRun and serve your pipelines\\n\\n\\n\\n\\n\\n\\n\\n\\nRun and serve your pipelines\\uf0c1\\nData scientists have a crucial role in the creation and implementation of machine learning models, converting advanced algorithms into useful applications. In tasks like data processing, training, scheduling, and model deployment, data scientists often face the challenge of initiating pipelines using different methods while efficiently handling various inputs. This adaptability is vital because it allows them to address a wide range of scenarios and demands.\\nTo streamline this process, the MLOps platform provides a robust and user-friendly deployment features, empowering data scientists to execute and serve their pipelines efficiently.\\nLearn about the various ways to execute your pipelines, which include:\\n\\nPipeline run\\nPipeline deployment, with 2 execution rules\\n\\nEndpoint rule\\nPeriodic rule', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 842}), Document(page_content='Pipeline run\\nPipeline deployment, with 2 execution rules\\n\\nEndpoint rule\\nPeriodic rule\\n\\n\\n\\nOne of the fundamental elements provided by the MLOps platform to run and serve your models is the ability to customize the sources of the inputs you provide to a pipeline and the ways you want to retrieve your outputs. The main idea is to easily connect different data sources to your inputs (data directly coming from the final user, or coming from the environment for example) and destinations (delivering output to the final user, writing it in the data store …). This is known as mapping in the platform, and it plays a central role when creating deployments or running pipelines.\\n\\nSummary\\uf0c1\\n\\nHow to run your pipeline\\nHow to define source and destination for step input and output ?\\n\\n\\n\\nPrerequisites\\uf0c1\\nBefore using the Craft AI platform, make sure you have completed the following prerequisites:\\n\\nGet access to an environment\\nConnect the SDK\\nCreate a step\\nCreate a pipeline', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1687}), Document(page_content='Get access to an environment\\nConnect the SDK\\nCreate a step\\nCreate a pipeline\\n\\n\\n\\n    ℹ️ Make sure that your code inside your step with inputs/outputs is able to read inputs from function parameters and send outputs with the return of the function.\\n    More information [here](../stepPipeline/createStep).\\n\\n\\n\\nHow to execute your pipeline\\uf0c1\\nAs a data scientist, I want to be able to execute my Python code contained in my pipelines in various scenarios :\\n\\nI (or data scientist in my team) want to launch my pipeline on the fly via the craft AI SDK, so I can use the run pipeline.\\nI want to be able to make my pipeline accessible to external users, inside or outside my organization, via a  Application Programmatic Interface (API), I can use the endpoint deployment feature to accomplish this.\\nI want my pipeline to be automatically execute at regular intervals without manual intervention, the periodic deployment feature can be employed.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2576}), Document(page_content='Run your pipeline\\uf0c1\\nThe run pipeline functionality enables you to trigger a pipeline directly from the Craft AI platform’s SDK. This option offers the advantage of being user-friendly and efficient.\\nPlease note that the run pipeline feature is exclusively accessible through the Craft AI SDK and requires proper connection to the appropriate platform environment. It is recommended for conducting experiments and conducting internal testing purposes.\\nTo execute a run on a pipeline with no input and no output, simply use this function:\\n# Run pipeline function \\nsdk.run_pipeline(pipeline_name=\"your-pipeline-name\")', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3514}), Document(page_content='To execute a run on a pipeline with inputs and outputs, you can use the same function and add the parameter inputs and give an object with inputs’ names as keys (should be the same name as defined in input object give at the step creation) and values you want to provide to your step at execution:\\n# Creating an object with predefined input values for the run,\\n# which will be provided as inputs to the run pipeline function\\ninputs_values = {\\n\\t\"number1\": 9, # Value for Input \"number1\" of the step (defined during step creation)\\n\\t\"number2\": 6 # Value for Input \"number2\" of the step (defined during step creation) \\n}\\n\\n# Running the Pipeline and Receiving Output\\noutput_values = sdk.run_pipeline(pipeline_name=\"your-pipeline-name\", inputs=inputs_values)\\n\\nprint (outputs_values[\"outputs\"])\\n\\n\\nFor example, you can obtain an output like this :\\n>> {\\n    \\'output_name_1\\': \\'Lorem ipsum dolor sit amet, consectetur adipiscing elit.\\',\\n    \\'output_name_2\\': 42\\n }', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4130}), Document(page_content='This function will return you the output of your pipeline. Like the inputs, it’s represented as an object with the outputs’ name as keys and the outputs values as values.\\n\\nWarning\\n⚠️ Don’t forget to adapt your code to get value from input and return output. You must have created inputs and outputs objects at the step creation stage to get values.\\nMore information here.\\n\\nIt is also possible to run a pipeline with specific mapping (to connect inputs/outputs to the data store, environment variable, etc.), that will be covered in this section.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5085}), Document(page_content='Create an endpoint\\uf0c1\\nTriggering a pipeline via an endpoint will enable you to make your application available from any programming language/tool (website, mobile application, etc.).\\nTo make your pipeline available via an endpoint, we need to create a deployment using the create_deployment() function which has the  execution_rule parameter set as endpoint. In return, we’ll get the URL of the endpoint and its authentication token so that we can call it and trigger our pipeline.\\n# Deployment creation with endpoint as execution rule\\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n)\\n\\n\\n\\n\\n    ℹ️ Deployment creation will be soon available on web UI.\\n\\nYou can trigger this endpoint from anywhere with any programming language if you have:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5633}), Document(page_content='You can trigger this endpoint from anywhere with any programming language if you have:\\n\\nEnvironment URL (Can be found in web UI and it’s the same you have used to initiate your SDK)\\nDeployment name\\nEndpoint token (given as result of the deployment creation, which secures access to the endpoint)\\n\\nTo trigger the deployed pipeline as an endpoint, you have a couple of options:\\n\\nUtilize the dedicated function provided by the Craft AI SDK.\\n# Execution of pipeline using the endpoint\\nsdk.trigger_endpoint(\\n\\tendpoint_name=\"your-deployment-name\", \\n\\tendpoint_token=endpoint_info[\"endpoint_token\"] # Token recieve after deployment creation \\n)\\n\\n\\n\\nImplement a HTTP request to the endpoint using any programming language, similar to the example shown with the command curl :\\ncurl -X POST -H \"Authorization: EndpointToken <ENDPOINT_TOKEN>\" \"<CRAFT_AI_ENVIRONMENT_URL>/endpoints/<DEPLOYMENT_NAME>\"\\n\\n\\n\\nAn other example with Javascript syntax (using axios) :\\nconst axios = require(\\'axios\\');', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6389}), Document(page_content=\"An other example with Javascript syntax (using axios) :\\nconst axios = require('axios');\\n\\nconst ENDPOINT_TOKEN = '<ENDPOINT_TOKEN>';\\nconst CRAFT_AI_ENVIRONMENT_URL = '<CRAFT_AI_ENVIRONMENT_URL>';\\nconst DEPLOYMENT_NAME = '<DEPLOYMENT_NAME>';\\n\\nconst headers = {\\n  'Authorization': `EndpointToken ${ENDPOINT_TOKEN}`\\n};\\n\\nconst url = `${CRAFT_AI_ENVIRONMENT_URL}/endpoints/${DEPLOYMENT_NAME}`;\\n\\naxios.post(url, null, { headers })\\n  .then(response => {\\n    console.log('Response:', response.data);\\n  })\\n  .catch(error => {\\n    console.error('Error:', error.message);\\n  });\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7278}), Document(page_content='ℹ️  The URL of your endpoint follows the structure :   <CRAFT_AI_ENVIRONMENT_URL>/endpoints/<YOUR_DEPLOYMENT_NAME>\\nThe inputs and outputs defined in your step are automatically linked to the deployment and consequently to the endpoint. You can therefore send a JSON within your request where the parameters correspond to the step inputs so that it can be used as a parameter for your function in the step. The deployment will return a similar object with your outputs as parameters.\\nYou can call the endpoint with inputs using the SDK function or with any other programming language (like before) by specifying the inputs in JSON format in the request body.\\n\\nWith Craft AI SDK :\\n# Value of inputs to be given to trigger pipeline function\\ninputs_values = {\\n\\t\"number1\": 9, # Input \"number1\" defined in step creation \\n\\t\"number2\": 6 # Input \"number2\" defined in step creation \\n}', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7854}), Document(page_content='# Execution of pipeline using the endpoint\\nsdk.trigger_endpoint(\\n\\tendpoint_name=\"your-deployment-name\", \\n\\tendpoint_token=endpoint_info[\"endpoint_token\"] # Token recieve after deployment creation \\n\\tinputs=inputs_values\\n)\\n\\n\\n\\nWith curl :\\ncurl -X POST -H \"Authorization: EndpointToken <ENDPOINT_TOKEN>\" -H \"Content-Type: application/json\" -d \\'{\"number1\": 9, \"number2\": 6}\\' \"<CRAFT_AI_ENVIRONMENT_URL>/endpoints/<YOUR-DEPLOYMENT-NAME>\"\\n\\n\\n\\nWith JavaScript using axios :\\nconst axios = require(\\'axios\\');\\n\\nconst ENDPOINT_TOKEN = \\'<ENDPOINT_TOKEN>\\';\\nconst CRAFT_AI_ENVIRONMENT_URL = \\'<CRAFT_AI_ENVIRONMENT_URL>\\';\\nconst YOUR_DEPLOYMENT_NAME = \\'<YOUR-DEPLOYMENT-NAME>\\';\\n\\nconst requestData = {\\n  number1: 9,\\n  number2: 6\\n};\\n\\nconst config = {\\n  headers: {\\n    \\'Authorization\\': `EndpointToken ${ENDPOINT_TOKEN}`,\\n    \\'Content-Type\\': \\'application/json\\'\\n  }\\n};\\n\\nconst url = `${CRAFT_AI_ENVIRONMENT_URL}/endpoints/${YOUR_DEPLOYMENT_NAME}`;', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8730}), Document(page_content=\"const url = `${CRAFT_AI_ENVIRONMENT_URL}/endpoints/${YOUR_DEPLOYMENT_NAME}`;\\n\\naxios.post(url, requestData, config)\\n  .then(response => {\\n    console.log('Response:', response.data);\\n  })\\n  .catch(error => {\\n    console.error('Error:', error);\\n  });\\n\\n\\n\\n\\n\\n\\n    ℹ️ For inputs and outputs, don't forget to have adapted the step code, to have declared the inputs and outputs at step level and to have used types (string, integer, etc.) compatible with the data you are going to manipulate.\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 9575}), Document(page_content='Periodic\\uf0c1\\nYou might need to trigger your Python code regularly, whether it’s every X minutes, every hour, or at a specific date, automatically. To achieve this, you can create a periodic deployment for your pipeline. This type of deployment uses the CRON format for scheduling its triggers.\\nTo set up a periodic deployment, you employ the same function as you would for endpoints. However, you specify the periodic trigger mode and define when it should trigger by providing a CRON rule in the schedule parameter.\\ndeployment_info = sdk.create_deployment(\\n    execution_rule=\"periodic\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    schedule=\"* * * * *\" # Will be executed every minute of every day \\n)\\n\\n\\n\\nMore information and help about CRON here.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 10063}), Document(page_content='More information and help about CRON here.\\n\\n\\n\\nHow to define sources and destinations for step inputs and outputs ?\\uf0c1\\nIn the previous section, we saw that deployments can be used to trigger the execution of a pipeline, but they can also be used to give and receive information via inputs and outputs. When a pipeline is triggered, it may need to receive or send this information to different sources or destinations.\\nExample:\\nIn the diagram below, we assume that we have deployed an endpoint pipeline. An API is therefore available to the user who triggers the execution of the pipeline each time a request is sent to the API. The request can contain information required to execute the pipeline, just as the pipeline can send information back to the user, as shown in the diagram below.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 10809}), Document(page_content='To direct these flows to the right place, the platform allows you to map the step inputs and outputs to different sources and destinations using InputSource and OutputDestination objects. We’ll look at four different types of mapping:\\n\\nConstant mapping\\nEndpoint mapping (value or file)\\nEnvironment variable mapping\\nNone / void mapping\\n\\nInputs and outputs are not compatible with all types of deployment. To make things clearer, here is a summary table:\\n\\n\\n\\nConstant\\nEndpoint value\\nEndpoint file\\nEnvironment variable\\nData store file\\nNone / void\\n\\n\\n\\nRun\\n✅\\n❌\\n❌\\n✅ (input only)\\n✅\\n✅\\n\\nEndpoint\\n✅\\n✅\\n✅ (limited to 1 file per call)\\n✅ (input only)\\n✅\\n✅\\n\\nPeriodic\\n✅\\n❌\\n❌\\n✅ (input only)\\n✅\\n✅\\n\\n\\n\\n\\nConstant\\uf0c1\\nIf I want my deployment to always use the same value as input, I can use a mapping to a constant.\\n\\n\\n    ℹ️ Note that the same pipeline can have multiple deployments with multiple different constant values for the same input.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 11596}), Document(page_content='We will be using the constant value for an endpoint deployment here, but the process is the same for other types of deployment (periodic).\\nTo do this, we create an InputSource object for each input in the pipeline that we want to deploy, specifying the name of the input for each mapping and the value of the input using the constant_value parameter.\\nExample :\\n# Endpoint Input mapping\\nendpoint_input1 = InputSource(\\n    step_input_name=\"number1\",\\n    constant_value=6,\\n)\\n\\nendpoint_input2 = InputSource(\\n    step_input_name=\"number2\",\\n    constant_value=3,\\n)\\n\\n# Deployment creation using inputSource object \\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    inputs_mapping=[endpoint_input1, endpoint_input2]\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 12510}), Document(page_content='Warning\\n⚠️ Step must be created with one or more inputs and the code contained in my step must be suitable for receiving a constant. More information about it here.\\nAll input types are compatible, except for file.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 13331}), Document(page_content='Endpoint\\uf0c1\\nAs explained above, when a deployment is of the endpoint type, the inputs and outputs of the associated steps have as their default source and destination the parameters of the HTTP request.\\nYou can therefore send a JSON within your request where the keys correspond to the step inputs so that it can be used as parameters for your function in the step. The deployment will return a similar object with your outputs as keys.\\nIf you need to change this default behavior, you can do so using InputSource and OutputDestination. You can define new names that will only be seen at the endpoint level for the external user. It allows you to specify under which names should inputs be passed in the request JSON by your final user or for the outputs, under which names they will be returned.\\nThis input mapping works with any type of input or output (integer, file, string, etc.).\\n# Endpoint Input mapping\\nendpoint_input1 = InputSource(\\n    step_input_name=\"number1\",', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 13548}), Document(page_content='# Endpoint Input mapping\\nendpoint_input1 = InputSource(\\n    step_input_name=\"number1\",\\n    endpoint_input_name=\"number_a\",\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 14432}), Document(page_content='endpoint_input2 = InputSource(\\n    step_input_name=\"number2\",\\n    endpoint_input_name=\"number_b\",\\n) \\n\\nendpoint_output = OutputDestination(\\n    step_output_name=\"number3\",\\n    endpoint_output_name=\"result\",\\n)\\n\\n# Deployment creation using 2 input mapping and 1 ouptut mapping\\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    inputs_mapping=[endpoint_input1, endpoint_input2],\\n    outputs_mapping=[endpoint_output]\\n)\\n\\n\\nNow that we have created our deployment, we can trigger the pipeline through the endpoint by passing the inputs and reading the outputs with the new mappings.\\n# Value of inputs to be given to trigger pipeline function\\ninputs_values = {\\n\\t\"number_a\": 9, # Using mapping defined before linked to number1\\n\\t\"number_b\": 6 # Using mapping defined before linked to number2\\n}', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 14558}), Document(page_content='# Execution of pipeline using the endpoint\\nendpointOutput = sdk.trigger_endpoint(\\n\\tendpoint_name=\"your-deployment-name\", \\n\\tendpoint_token=endpoint_info[\"endpoint_token\"] # Token received after deployment creation \\n\\tinputs=inputs_values\\n)\\n\\n# Print result, note you should go into \"outputs\" before\\nprint (endpointOutput[\"outputs\"][\"result\"])\\n\\n\\nThese changes are effective for any endpoint trigger method (curl, JS, etc.).\\n\\n\\n    ℹ️ Obviously, this type of mapping is only available if the deployment is an endpoint.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 15451}), Document(page_content='ℹ️ Obviously, this type of mapping is only available if the deployment is an endpoint.\\n\\n\\n\\nEnvironment variable\\uf0c1\\nAs a data scientist, I may need to use data common for my entire environment in my pipeline. This can be achieved by mapping environment variables to inputs.\\nTo do this, we also use the mapping system with the InputSource object. Environment variables are only available for inputs and not for outputs (but it is still possible to define them directly in the step code).\\nFirst, let’s look at how to initialize the two environment variables (on the platform environment), we’re going to use:\\n# Creation of env variable for input1\\nsdk.create_or_update_environment_variable(\\n\\tenvironment_variable_name=\"RECETTE_VAR_ENV_INPUT1\",\\n\\tenvironment_variable_value=6\\n)\\n\\n# Creation of env variable for input2\\nsdk.create_or_update_environment_variable(\\n\\tenvironment_variable_name=\"RECETTE_VAR_ENV_INPUT2\",\\n\\tenvironment_variable_value=4\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 15877}), Document(page_content='Now, we can create an InputSource object and use it at the deployment creation.\\n# Creation of InputSource to get env variable into input \\nendpoint_input1 = InputSource(\\n\\t step_input_name=\"number1\",\\n\\t environment_variable_name=\"RECETTE_VAR_ENV_INPUT1\",\\n)\\n\\nendpoint_input2 = InputSource(\\n\\t step_input_name=\"number2\",\\n\\t environment_variable_name=\"RECETTE_VAR_ENV_INPUT2\",\\n)\\n\\n# Endpoint creation\\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    inputs_mapping=[endpoint_input1, endpoint_input2]\\n)\\n\\n\\nThe deployment is created, we can trigger it without giving any object into data of HTTP request, it will take current environment variable value.\\n# Value of inputs to be given to execute pipeline function\\ninputs_value = {\\n\\t\"number_a\": 9, # Using mapping defined before linked to number1\\n\\t\"number_b\": 6 # Using mapping defined before linked to number2\\n}', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 16815}), Document(page_content='# Execution of pipeline using the endpoint\\nsdk.trigger_endpoint(\\n\\tendpoint_name=\"your-deployment-name\", \\n\\tendpoint_token=endpoint_info[\"endpoint_token\"] # Token recieve after deployment creation \\n\\tinputs=inputs_value\\n)\\n\\n\\n\\n\\nData store\\uf0c1\\nYou may need to transfer files between your data store and your step (in one direction or the other). To do this, you can also use inputs and outputs mapping system. You’ll find all the explanations you need on this page.\\n\\n\\nVoid / None\\uf0c1\\nAll step inputs and outputs have to be mapped when you deploy a pipeline with inputs and outputs (otherwise it will raise an error when creating the deployment). If you don’t really want to give/receive data in these inputs and outputs, you can map them to None. This will create a None object in Python for the inputs and send the outputs into the void.\\n# Endpoint Input mapping\\nendpoint_input1 = InputSource(\\n    step_input_name=\"number1\",\\n    is_null=True,\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 17774}), Document(page_content='endpoint_input2 = InputSource(\\n    step_input_name=\"number2\",\\n    is_null=True,\\n) \\n\\nendpoint_output = OutputDestination(\\n     step_output_name=\"number3\",\\n     is_null=True\\n)\\n\\n# Deployment creation using 2 input mappings and 1 ouptut mapping\\nendpoint_info = sdk.create_deployment(\\n    execution_rule=\"endpoint\",\\n    pipeline_name=\"your-pipeline-name\",\\n    deployment_name=\"your-deployment-name\",\\n    inputs_mapping=[endpoint_input1, endpoint_input2],\\n    outputs_mapping=[endpoint_output]\\n)\\n\\n\\nNow that the deployment has been created, it can be triggered without giving or receiving any input or output.\\n# Execution of pipeline using the endpoint who will return any output (object with value)\\nsdk.trigger_endpoint(\\n\\tendpoint_name=\"your-deployment-name\", \\n\\tendpoint_token=endpoint_info[\"endpoint_token\"] # Token received after deployment creation\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/user_workflow/run_serve_pipeline.html', 'title': 'Run and serve your pipelines — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 18709}), Document(page_content='Environments — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nEnvironments', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/introduction.html', 'title': 'Environments — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nEnvironments\\n\\n\\n\\n\\n\\n\\n\\n\\nEnvironments\\uf0c1\\nAn environment is the infrastructure used by the platform to store\\ndata and run computation.\\n\\nBy default, our environments are created on AWS, but it is possible, if\\nneeded, to configure our platform on another Kubernetes-compatible cloud (e.g.\\xa0OVH,\\nGCP, Azure, etc.).\\nUsers can create as many environments as they want in each\\nproject. Each environment is\\nhermetically sealed from the others.\\nIt is composed of:\\n\\na cluster: cloud computing resources based on a fully managed Kubernetes service. The ML pipelines created in an environment are executed on the associated cluster.\\na data store: cloud storage to save and retrieve any amount of data at any time. The results of the pipelines and metrics generated by the platform are also stored on the data store.\\n\\nIt is possible to tag environments with 3 types: development, test\\nand production.\\nThe main objectives of the environments are:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/introduction.html', 'title': 'Environments — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 826}), Document(page_content='Building and managing infrastructures in a few clicks without DevOps\\nskills.\\nEnsuring an efficient and automated use of computing resources, thanks\\nto Kubernetes.\\nEnabling large scale deployment of Python jobs without rewriting to a\\nmore production friendly language.\\nFine-tuning the size of your environments according to the needs of\\nyour projects.\\nEasily building dedicated environments for each phase of the project\\nto adopt software development best practices.\\n\\n\\nEnvironments\\n\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/introduction.html', 'title': 'Environments — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1801}), Document(page_content='Environments — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nEnvironments', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/introduction.html', 'title': 'Environments — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nEnvironments\\n\\n\\n\\n\\n\\n\\n\\n\\nEnvironments\\uf0c1\\nAn environment is the infrastructure used by the platform to store\\ndata and run computation.\\n\\nBy default, our environments are created on AWS, but it is possible, if\\nneeded, to configure our platform on another Kubernetes-compatible cloud (e.g.\\xa0OVH,\\nGCP, Azure, etc.).\\nUsers can create as many environments as they want in each\\nproject. Each environment is\\nhermetically sealed from the others.\\nIt is composed of:\\n\\na cluster: cloud computing resources based on a fully managed Kubernetes service. The ML pipelines created in an environment are executed on the associated cluster.\\na data store: cloud storage to save and retrieve any amount of data at any time. The results of the pipelines and metrics generated by the platform are also stored on the data store.\\n\\nIt is possible to tag environments with 3 types: development, test\\nand production.\\nThe main objectives of the environments are:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/introduction.html', 'title': 'Environments — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 826}), Document(page_content='Building and managing infrastructures in a few clicks without DevOps\\nskills.\\nEnsuring an efficient and automated use of computing resources, thanks\\nto Kubernetes.\\nEnabling large scale deployment of Python jobs without rewriting to a\\nmore production friendly language.\\nFine-tuning the size of your environments according to the needs of\\nyour projects.\\nEasily building dedicated environments for each phase of the project\\nto adopt software development best practices.\\n\\n\\nEnvironments\\n\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/introduction.html', 'title': 'Environments — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1801}), Document(page_content='Create an environment — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nEnvironments\\nCreate an environment', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/createEnvironment.html', 'title': 'Create an environment — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nEnvironments\\nCreate an environment\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate an environment\\uf0c1\\nAn environment is the infrastructure used by the platform to store\\ndata and run computation. The first thing to do when you have created a\\nnew project is to create an environment composed of a cluster and a data store.\\n\\nDevelopment : For experimenting your projects and models on the\\nplatform.\\nTesting : For testing your projects just before the production\\nwith the same settings.\\nProduction : To use your pipelines in production and secure the\\naccess.\\n\\n\\nSummary\\uf0c1\\n\\nSet up an environment\\nManage an environment', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/createEnvironment.html', 'title': 'Create an environment — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 835}), Document(page_content='Summary\\uf0c1\\n\\nSet up an environment\\nManage an environment\\n\\n\\n\\nSet up an environment\\uf0c1\\nThe environment creation is available by email for the moment. In a next\\nversion, it will be possible to create, edit and delete an environment\\ndirectly on the platform UI.\\nFor the moment, you can send us your request by email to the address\\nyou have been given or on your dedicated Slack channel using this template:\\n——————————— My demand ——————————-\\n\\nChoose between :\\n- Create an environment\\n- Edit an environment\\n- Delete an environment\\n\\n——————————- Name of environment ——————————-\\n\\nPrecise the name of environment to create or the name of existing environment you want\\nto edit/delete\\n\\n———————————- Hardware ——————————-\\n\\nChoose environment size (see table below).\\n\\n———————————- Tag ———————————–\\n\\nChoose between this tag to set an environnement type :\\n- Dev\\n- Pre-prod\\n- Prod', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/createEnvironment.html', 'title': 'Create an environment — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1404}), Document(page_content='Choose between this tag to set an environnement type :\\n- Dev\\n- Pre-prod\\n- Prod\\n\\n\\nEach environment is based on hardware infrastructure on the cloud.\\nIt’s possible to select different levels of size and computing power for this cluster\\naccording to your business use case and your usage.\\n\\n\\nSize\\nHardware\\nWorkers\\n\\n\\n\\nCPU size 1\\n2CPU 4GB\\n2\\n\\nCPU size 2\\n4CPU 16GB\\n2\\n\\nCPU size 3\\n8CPU 32GB\\n2\\n\\nCPU size 4\\n16CPU 64GB\\n2\\n\\nGPU size 1\\n1GPU (T4) 16GB (VRAM)\\n2\\n\\nGPU size 2\\n1GPU (A10G) 24GB (VRAM)\\n2\\n\\n\\n\\n\\n\\nManage an environment\\uf0c1\\nIf you need more size and computing power than expected, you can also change your\\nenvironment size once your environment is already created. All you\\nhave to do is simply send a Slack message on your dedicated channel\\nor an email to support@craft.ai with the name of the environment\\nand the new size you want.\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/createEnvironment.html', 'title': 'Create an environment — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2183}), Document(page_content='Work with environment variables — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nEnvironments\\nWork with environment variables', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/workOnEnvironment.html', 'title': 'Work with environment variables — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='home\\nEnvironments\\nWork with environment variables\\n\\n\\n\\n\\n\\n\\n\\n\\nWork with environment variables\\uf0c1\\nAn environment is the infrastructure used by the platform to store\\ndata and run computation. Once created, you can start working in the\\nenvironments by creating pipelines and running them in the platform.\\nIn addition, you can create and save environment variables that\\nwill allow you to parameterize certain variables in order to call them\\nwhen running or deploying a pipeline.\\n\\n\\nFunction name\\nMethod\\nReturn type\\nDescription\\n\\n\\n\\ncreate_or_update_environment_variable\\nCraftAiSdk.create_or_update_environment_variable (environment_variable_name, environment_variable_value)\\ndict\\nTo create or update an environment variable available for all pipelines executions.\\n\\nlist_environment_variables\\nCraftAiSdk.list_environment_variables()\\nList of dict\\nGet a list of all environments variables in the current environment.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/workOnEnvironment.html', 'title': 'Work with environment variables — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 888}), Document(page_content='delete_environment_variable\\nCraftAiSdk.delete_environment_variable(environment_variable_name)\\ndict\\nDelete a specified environment variable.\\n\\n\\n\\n\\nSet up an environment variable\\uf0c1\\nAn environment variable is a value that can be passed to your step\\ncode. It is used to store information that may be needed by the\\noperating system or by applications that run on the platform (for example, by endpoints you deploy on the platform).\\n\\nCreate and update an environment variable\\uf0c1\\nTo create or update an environment variable available for all pipelines\\nexecutions.\\nCraftAiSdk.create_or_update_environment_variable(environment_variable_name,\\nenvironment_variable_value)\\n\\n\\n\\nParameters\\uf0c1\\n\\nenvironment_variable_name (str) – Name of the environment\\nvariable to create.\\nenvironment_variable_value (str) – Value of the environment\\nvariable to create.\\n\\n\\n\\nReturns\\uf0c1\\nA dict object containing the ID of environment variable (with keys “id”)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/workOnEnvironment.html', 'title': 'Work with environment variables — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1790}), Document(page_content='Returns\\uf0c1\\nA dict object containing the ID of environment variable (with keys “id”)\\n\\n\\n\\nGet the list of environment variables\\uf0c1\\nGet the list of all environment variables in the current environment.\\nCraftAiSdk.list_environment_variables()\\n\\n\\n\\nParameter\\uf0c1\\nNo parameter\\n\\n\\nReturns\\uf0c1\\nList of dicts of environment variables (with keys “name” and\\n“value”)\\n\\n\\n\\nDelete an environment variable\\uf0c1\\nDelete a specified environment variable.\\nCraftAiSdk.delete_environment_variable(environment_variable_name)\\n\\n\\n\\nParameter\\uf0c1\\n\\nenvironment_variable_name (str) – Name of the environment\\nvariable to delete.\\n\\n\\n\\nReturns\\uf0c1\\nDict (with keys “name” and “value”) of the deleted environment variable\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/workOnEnvironment.html', 'title': 'Work with environment variables — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2623}), Document(page_content='Save my data on the store — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nEnvironments\\nSave my data on the store', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/saveOnDataStore.html', 'title': 'Save my data on the store — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nEnvironments\\nSave my data on the store\\n\\n\\n\\n\\n\\n\\n\\n\\nSave my data on the store\\uf0c1\\nEach environment has its own storage to save and retrieve any amount\\nof data at any time, from anywhere on the web. This storage is an\\nAmazon S3 bucket that you can manage with the Python SDK of the platform.\\nYou can upload any kind of raw data on the store. Furthermore, the\\nresults of the pipelines and metrics generated by the platform can also\\nbe saved on the data store.\\nInfo: For the moment, it is not possible to copy data from an\\nenvironment to another. You have to download it and to re-upload it. In\\nfuture versions, you will be able to transfer your data.\\n\\n\\n\\n🆕 The graphical interface of the data store will arrive later on the\\nplatform.\\n\\n\\n\\n\\nSummary\\uf0c1\\n\\nGet files\\nGet file info\\nUpload a file\\nDownload a file\\nDelete a file\\n\\n\\n\\nFunction name\\nMethod\\nReturn type\\nDescription', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/saveOnDataStore.html', 'title': 'Save my data on the store — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 839}), Document(page_content='Function name\\nMethod\\nReturn type\\nDescription\\n\\n\\n\\nlist_data_store_objects\\nCraftAiSdk.list_data_store_objects()\\nlist of dict\\nGet the list of the objects stored in the data store.\\n\\nupload_data_store_object\\nCraftAiSdk.upload_data_store_object(filepath_or_buffer, object_path_in_datastore)\\nNone\\nUpload a file as an object into the data store.\\n\\ndownload_data_store_object\\nCraftAiSdk.download_data_store_object(object_path_in_datastore, filepath_or_buffer)\\nNone\\nDownload an object in the data store and save it into a file.\\n\\ndelete_data_store_object\\nCraftAiSdk.delete_data_store_object(object_path_in_datastore)\\ndict\\nDelete an object on the data store.\\n\\n\\n\\n\\n\\nGet files\\uf0c1\\n\\nFunction definition\\uf0c1\\nGet the list of the objects stored in the data store.\\nCraftAiSdk.list_data_store_objects()\\n\\n\\n\\nParameter\\uf0c1\\nNo parameter\\n\\n\\nReturns\\uf0c1\\nList of objects (dict) in the data store, each object being represented as dict with :', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/saveOnDataStore.html', 'title': 'Save my data on the store — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1695}), Document(page_content='Returns\\uf0c1\\nList of objects (dict) in the data store, each object being represented as dict with :\\n\\n“path” (str): Location of the object in the data store.\\n“last_modified” (str): The creation date or last modification date in ISO format.\\n“size” (str): The size of the object with units of digital storage measurement (MB, GB, …).\\n\\n\\n\\n\\n\\nGet file info\\uf0c1\\n\\nFunction definition\\uf0c1\\nGet information about a single object in the data store.\\nCraftAiSdk.get_data_store_object_information(object_path_in_datastore)\\n\\n\\n\\nParameter\\uf0c1\\n\\nobject_path_in_datastore (str) – Location of the object in the data store.\\n\\n\\n\\nReturns\\uf0c1\\nObject information, with the following keys:\\n\\n“path” (str): Location of the object in the data store.\\n“last_modified” (str): The creation date or last modification date in ISO format.\\n“size” (str): The size of the object with units of digital storage measurement (MB, GB, …).\\n\\n\\n\\n\\n\\nUpload a file\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/saveOnDataStore.html', 'title': 'Save my data on the store — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2498}), Document(page_content='Upload a file\\uf0c1\\n\\nFunction definition\\uf0c1\\nUpload a file as an object into the data store.\\nCraftAiSdk.upload_data_store_object(filepath_or_buffer, object_path_in_datastore)\\n\\n\\n\\nParameters\\uf0c1\\n\\nfilepath_or_buffer (str, or file-like object) – String, path to\\nthe file to be uploaded ; or file-like object implementing a read()\\nmethod (e.g.\\xa0via buildin open function). The file object must be\\nopened in binary mode, not text mode.\\nobject_path_in_datastore (str) – Destination of the uploaded\\nfile.\\n\\n\\n\\nReturns\\uf0c1\\nThis function returns None.\\n\\n\\n\\n\\nDownload a file\\uf0c1\\n\\nFunction definition\\uf0c1\\nDownload an object in the data store and save it into a file.\\nCraftAiSdk.download_data_store_object(object_path_in_datastore, filepath_or_buffer)\\n\\n\\n\\nParameters\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/saveOnDataStore.html', 'title': 'Save my data on the store — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3378}), Document(page_content='Parameters\\uf0c1\\n\\nobject_path_in_datastore (str) – Location of the object to\\ndownload from the data store.\\nfilepath_or_buffer (str or file-like object) – String, file path\\nto save the file to ; or a file-like object implementing a write()\\nmethod, (e.g.\\xa0via builtin open function). The file object must be\\nopened in binary mode, not text mode.\\n\\n\\n\\nReturns\\uf0c1\\nThis function return an None.\\n\\n\\n\\n\\nDelete a file\\uf0c1\\n\\nFunction definition\\uf0c1\\nDelete an object on the data store.\\nCraftAiSdk.delete_data_store_object(object_path_in_datastore)\\n\\n\\n\\nParameters\\uf0c1\\n\\nobject_path_in_datastore (str) – Location of the object to delete\\nin the data store.\\n\\n\\n\\nReturns\\uf0c1\\nDeleted object represented as dict (with key “path”).\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/environment/saveOnDataStore.html', 'title': 'Save my data on the store — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4095}), Document(page_content='Step & Pipeline — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nStep & Pipeline', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/introduction.html', 'title': 'Step & Pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nStep & Pipeline\\n\\n\\n\\n\\n\\n\\n\\n\\nStep & Pipeline\\uf0c1\\nA pipeline is a machine learning workflow, consisting of one or more\\nsteps, to deploy containerised code. Like a regular function, a\\nstep is defined by the input it ingests, the code it runs, and\\nthe output it returns. You can then create a full pipeline formed\\nwith a computed acyclic graph (DAG) by specifying the output of one step\\nas the input of another step.\\n\\n💡 The pipelines are written in Python with SDK calls for an easy\\nauthoring experience and executed on Kubernetes for scalability. The\\npipelines’ functioning are based on the open-source library Argo and\\nthe steps are based on code stored on Github / GitLab. Each step deploy\\nis containerised with Docker.\\nThe main objectives of the steps and pipelines are :', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/introduction.html', 'title': 'Step & Pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 829}), Document(page_content='Orchestrating end-to-end ML workflows\\nIncreasing reusability of Data Science components from a project to\\nanother\\nCollaboratively managing, tracking and viewing pipeline definitions.\\nDeploying in production in few clicks with several methods (endpoint,\\nCRON, manual, …)\\nEnabling large scale production of Python code without refactoring to\\na more production friendly language\\nEnsuring an efficient use of compute resources, thanks to\\nKubernetes\\n\\nExample :\\n🖊️ Training pipeline', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/introduction.html', 'title': 'Step & Pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1643}), Document(page_content='Example :\\n🖊️ Training pipeline\\n\\nData preparation and preprocessing: In this stage, raw data is\\ncollected, cleaned, and transformed into a format that is suitable\\nfor training a machine learning model. This may involve tasks such as\\nfiltering out missing or invalid data points, normalizing numerical\\nvalues, and encoding categorical variables.\\nModel training: In this stage, a machine learning model is trained on\\na prepared dataset. This may involve selecting a model type, tuning\\nhyperparameters, and training the model using an optimization\\nalgorithm.\\nModel evaluation: Once the model has been trained, it is important to\\nevaluate its performance to determine how well it generalizes to new\\ndata. This may involve tasks such as splitting the dataset into a\\ntraining set and a test set, evaluating the model’s performance on\\nthe test set, and comparing the results to a baseline model.\\n\\n🖊️ Inference pipeline', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/introduction.html', 'title': 'Step & Pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2089}), Document(page_content='🖊️ Inference pipeline\\n\\nModel loading: In this stage, a trained machine learning model is\\nloaded from storage and prepared for use. This may involve tasks such\\nas loading the model’s weights and any associated dependencies.\\nData preparation: In this stage, incoming data is cleaned and\\ntransformed into a format that is suitable for the model. This may\\ninvolve tasks such as normalizing numerical values and encoding\\ncategorical variables.\\nInference: In this final stage, the model is used to make predictions\\non the prepared data. This may involve tasks such as passing the data\\nthrough the model and processing the output to generate a final\\nprediction.\\n\\n\\nStep & pipeline\\n\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/introduction.html', 'title': 'Step & Pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2978}), Document(page_content='Step & Pipeline — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nStep & Pipeline', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/introduction.html', 'title': 'Step & Pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nStep & Pipeline\\n\\n\\n\\n\\n\\n\\n\\n\\nStep & Pipeline\\uf0c1\\nA pipeline is a machine learning workflow, consisting of one or more\\nsteps, to deploy containerised code. Like a regular function, a\\nstep is defined by the input it ingests, the code it runs, and\\nthe output it returns. You can then create a full pipeline formed\\nwith a computed acyclic graph (DAG) by specifying the output of one step\\nas the input of another step.\\n\\n💡 The pipelines are written in Python with SDK calls for an easy\\nauthoring experience and executed on Kubernetes for scalability. The\\npipelines’ functioning are based on the open-source library Argo and\\nthe steps are based on code stored on Github / GitLab. Each step deploy\\nis containerised with Docker.\\nThe main objectives of the steps and pipelines are :', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/introduction.html', 'title': 'Step & Pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 829}), Document(page_content='Orchestrating end-to-end ML workflows\\nIncreasing reusability of Data Science components from a project to\\nanother\\nCollaboratively managing, tracking and viewing pipeline definitions.\\nDeploying in production in few clicks with several methods (endpoint,\\nCRON, manual, …)\\nEnabling large scale production of Python code without refactoring to\\na more production friendly language\\nEnsuring an efficient use of compute resources, thanks to\\nKubernetes\\n\\nExample :\\n🖊️ Training pipeline', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/introduction.html', 'title': 'Step & Pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1643}), Document(page_content='Example :\\n🖊️ Training pipeline\\n\\nData preparation and preprocessing: In this stage, raw data is\\ncollected, cleaned, and transformed into a format that is suitable\\nfor training a machine learning model. This may involve tasks such as\\nfiltering out missing or invalid data points, normalizing numerical\\nvalues, and encoding categorical variables.\\nModel training: In this stage, a machine learning model is trained on\\na prepared dataset. This may involve selecting a model type, tuning\\nhyperparameters, and training the model using an optimization\\nalgorithm.\\nModel evaluation: Once the model has been trained, it is important to\\nevaluate its performance to determine how well it generalizes to new\\ndata. This may involve tasks such as splitting the dataset into a\\ntraining set and a test set, evaluating the model’s performance on\\nthe test set, and comparing the results to a baseline model.\\n\\n🖊️ Inference pipeline', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/introduction.html', 'title': 'Step & Pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2089}), Document(page_content='🖊️ Inference pipeline\\n\\nModel loading: In this stage, a trained machine learning model is\\nloaded from storage and prepared for use. This may involve tasks such\\nas loading the model’s weights and any associated dependencies.\\nData preparation: In this stage, incoming data is cleaned and\\ntransformed into a format that is suitable for the model. This may\\ninvolve tasks such as normalizing numerical values and encoding\\ncategorical variables.\\nInference: In this final stage, the model is used to make predictions\\non the prepared data. This may involve tasks such as passing the data\\nthrough the model and processing the output to generate a final\\nprediction.\\n\\n\\nStep & pipeline\\n\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/introduction.html', 'title': 'Step & Pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2978}), Document(page_content='Create a step — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nStep & Pipeline\\nCreate a step', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nStep & Pipeline\\nCreate a step\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate a step\\uf0c1\\nA step is an atomic component defined by its input and output\\nparameters and by the processing it applies. Steps are the building\\nblocks of Pipelines. In practice, a step is a function with inputs and\\noutputs coded in Python. They are assembled with each other to create a\\ncomplete ML pipeline. The Python code (only available language for\\nthe moment) used by the step is stored on a Git repository.\\nAn input of a step is an object you can use inside the code.\\nAn output of a step is defined from the results of the step\\nfunction.\\nYou will be able to connect inputs & outputs of a step with another step\\nto compose a complete ML pipeline by using a directed acyclic graph\\n(DAG).\\nEach step is considered as a specific container that is executed on\\nKubernetes.\\nThe steps are stored in a specific environment, and only people with\\naccess to this environment can read and write the steps.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 827}), Document(page_content='Summary\\uf0c1\\n\\nPrepare your code on Git repository\\nDefine step inputs and outputs\\nCreate a step\\n\\n\\n\\nFunction name\\nMethod\\nReturn type\\nDescription\\n\\n\\n\\nInput\\nInput(name, data_type=”string”, description=””, is_required=False, default_value=None)\\nInput SDK Object\\nCreate an Input object to give at create_steps() function for step a step input.\\n\\nOutput\\nOutput(name, data_type=”string”, description=””)\\nOutput SDK Object\\nCreate an Output object to give at create_steps() function for step a step output.\\n\\ncreate_step\\ncreate_step(step_name, function_path, function_name, repository_branch=None, description=None, timeout_s=180, container_config=None, inputs=None, outputs=None)\\nlist of dict[str, str]\\nCreate pipeline steps from a source code located on a remote repository.\\n\\n\\n\\n\\n\\nPrepare your code a Git repository\\uf0c1\\nPrerequisites: Before the creation of your first step, make sure you\\nhave already done this :\\n\\nSetup Project & Environment\\nGit repository link to the project', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1815}), Document(page_content='Setup Project & Environment\\nGit repository link to the project\\n\\n❓ Why do you need to put your code on a Git repository ?\\nThis simplifies the access to the source code by the Craft AI platform.\\nIndeed, the platform will be able to directly fetch your code from the repository,\\nwithout the need for you to send it directly each time you change it,\\nyou just have to push it to your GitHub / GitLab repository.\\nCurrently, you can create a step via the Python SDK and not with\\ngraphical interface. But, after the creation, you will be able to see\\nthe step on the UI platform.\\nIf it’s not already done, put the code of the step into a GitHub / GitLab repository linked to the platform.\\nThe file with the entry function of your step can be anywhere in your Git repository.\\nExample tree file\\nin repo :\\n| requirements.txt\\n| src\\n   | my_entry_function_step.py\\n...\\n\\n\\nExample my_entry_function_step.py :\\nimport numpy as np\\n# and other import\\n\\ndef entryStep(dataX_input, dataY_input) :', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2711}), Document(page_content='def entryStep(dataX_input, dataY_input) :\\n\\n    # Some machine learning code\\n\\n    return result_output\\n\\n\\n\\n\\nDefine step inputs and outputs\\uf0c1\\nA step may need to receive some information or give some result (just\\nlike a function). To do that, we use Input and Output object. These\\nobjects allow defining the properties of the input or output that will\\nbe expected in the step. The input and output objects thus created must\\nbe given as a parameter of the step creation. Each input is defined as\\nan Input object and, each Output is defined as an Output object,\\nthrough a class available in the SDK.\\n\\nInput object definition\\uf0c1\\nfrom craft_ai_sdk.io import Input\\n\\nInput(\\n   name=\"*your_input_name*\",\\n   data_type=\"*your_io_data_type*\",\\n   description=\"\",\\n   is_required=True\\n   default_value=\"*default_value*\"\\n)\\n\\n\\n\\nParameters\\uf0c1\\n\\nname just a name for identifying the input later.\\ndata_type, one of the following possible types:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3642}), Document(page_content='name just a name for identifying the input later.\\ndata_type, one of the following possible types:\\n\\nfile: reference to binary data, equivalent to a file’s\\ncontent. If the input/output is not available, an empty stream.\\njson: JSON-serializable Python object. The following sub-types\\nare provided for more precise type checking, but they are all JSON\\nstring\\nnumber\\narray of JSON\\nboolean\\nIf the input/output is not available, None in Python\\n\\n\\n\\ndefault_value (optional) - If the parameter is empty, this value\\nwill be set by default. If a deployment receives an empty parameter\\nand already put a default value in the input, the default value of\\ndeployment will be keep.\\nis_required (optional, True by default) - Push an error is\\nthe input is empty.\\ndescription (optional) - This parameter precise what it’s\\nexpected in this input. It’s not read by the machine, it’s like a\\ncomment.\\n\\n\\n\\nReturn\\uf0c1\\nNo return\\n\\n\\n\\nOutput object definition\\uf0c1\\nfrom craft_ai_sdk.io import Output', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4460}), Document(page_content='Return\\uf0c1\\nNo return\\n\\n\\n\\nOutput object definition\\uf0c1\\nfrom craft_ai_sdk.io import Output\\n\\nOutput(\\n   name=\"*your_input_name*\",\\n   data_type=\"*your_io_data_type*\",\\n   description=\"\",\\n)\\n\\n\\n\\nParameters\\uf0c1\\n\\nname just a name for identifying the input later.\\ndata_type, one of the following possible types:\\n\\nfile: reference to binary data, equivalent to a file’s\\ncontent. If the input/output is not available, an empty stream.\\njson: JSON-serializable Python object. The following sub-types\\nare provided for more precise type checking, but they are all JSON\\nstring\\nnumber\\narray of JSON\\nboolean\\n\\nIf the input/output is not available, None in Python\\n\\ndescription (optional) - This parameter precise what it’s\\nexpected in this input. It’s not read by the machine, it’s like a\\ncomment.\\n\\n\\n\\nReturn\\uf0c1\\nNo return\\n\\nNote\\nYou can use craft_ai_sdk.INPUT_OUTPUT_TYPES to get all possible types in Input and Output objects.\\nList of all possible types :', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5340}), Document(page_content='ARRAY = “array”\\nBOOLEAN = “boolean”\\nFILE = “file”\\nJSON = “json”\\nNUMBER = “number”\\nSTRING = “string”\\n\\nExample :\\nfrom craft_ai_sdk.io import Input, INPUT_OUTPUT_TYPES\\n\\nInput(\\n   name=\"inputName\",\\n   data_type=INPUT_OUTPUT_TYPES.JSON,\\n)\\n\\n\\n\\n\\n\\n\\nExample for input and output\\uf0c1\\nInput(\\n    name=\"inputName\",\\n    data_type=\"string\",\\n    description=\"A parameter for step input\",\\n    is_required=True,\\n    default_value=\"default_content_here\"\\n)\\n\\nOutput(\\n    name=\"inputName\",\\n    data_type=\"string\",\\n    description=\"A parameter for step input\",\\n)\\n\\n\\n\\nWarning\\nThe size of the I/O must not exceed 0.06MB (except for file type).\\n\\n\\n\\n\\nCreate a step\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6261}), Document(page_content='Warning\\nThe size of the I/O must not exceed 0.06MB (except for file type).\\n\\n\\n\\n\\nCreate a step\\uf0c1\\n\\nFunction definition\\uf0c1\\nCreate pipeline steps from a source code located on a remote repository.\\nsdk.create_step(\\n    function_path=\"src/my_reusable_funtion.py\",\\n    function_name=\"my_function\",\\n    inputs=[Input(...)],\\n    outputs=[Output(...)],\\n    [name=\"*your-custom-step-name*\"], # by default its the function name\\n    [description=\"*text with limit*\"],\\n    [repository_branch=\"*your-git-branch* or *your-git-tag*\"],\\n    [timeout_s=180]\\n    [container_config = {\\n        [language=\"python:3.8-slim\"],\\n        [repository_url=\"*your-git-url*\"],\\n        [repository_deploy_key=\"*your-private_key*\"],\\n        [requirements_path=\"*your-path-to-requirements*\"],\\n        [included_folders=[\"*your-list-of-path-to-sources*\"]],\\n        [system_dependencies=[\"package_1\", \"package_2\"]],\\n        [dockerfile_path=\"*your-dockerfile-path*\"],\\n}],\\n)\\n\\n\\n\\nParameters\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6801}), Document(page_content='Parameters\\uf0c1\\n\\nfunction_path (str) – Path to access to the file who had the entry\\nfunction of the step.\\nfunction_name (str) – Function name of entry function step.\\ninputs (list<Input>) – List of step inputs.\\noutputs (list<Output>) – List of step outputs.\\nname (str) – Step name. By default, it’s the function name. The\\nname must be unique inside an environment and without special\\ncharacter ( - _ & / ? …)\\ndescription (str, optional) – Description of the step, it’s no use by\\nthe code, it’s only for user.\\nrepository_branch (str, optional) – Branch name for Git\\nrepository. Defaults to None.\\ntimeout_s (int, optional) – Maximum time to wait for the step to be created.\\n3min by default, and must be at least 2min.\\ncontainer_config (dict, optional) – Dict Python object where each key\\ncan override default parameter values for this step defined at\\nproject level.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7737}), Document(page_content='language (str, optional) – Language of programming used inside the\\nstep. Defaults to python:3.8-slim. Versions python:3.9-slim and\\npython:3.10-slim are also available for CPU.\\nFor GPU enviromnent, python-cuda:3.8-11.3, python-cuda:3.9-11.3\\nand python-cuda:3.10-11.3 are available.\\nrepository_url (str, optional) – Remote repository URL.\\nrepository_deploy_key (str, optional) – Private SSH key related to\\nthe repository.\\nrequirements_path (str, optional) – Path to the file requirement\\nfor Python dependency.\\nincluded_folders (list, optional) – List of folders that need to be\\naccessible from step code.\\nsystem_dependencies (list, optional) – List of APT Linux packages\\nto install.\\ndockerfile_path (str, optional) – Path to a docker-file for having\\na custom config in step. (see the part after for more detail)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8597}), Document(page_content='Note\\nThe repository_branch parameters as well as the container_config elements (except dockerfile_path) can take one of the STEP_PARAMETER object’s values in addition to theirs.\\nIn fact, STEP_PARAMETER allows us to specify at the step level whether we want to take the project’s values (default behavior) or define a null value:\\n\\nSTEP_PARAMETER.FALLBACK_PROJECT : Allows to take the value defined in the project parameters (default behavior if the field is not defined).\\nSTEP_PARAMETER.NULL : Allows to set the field to null value and not to take the value defined in the project.\\n\\nExample with a code step that does not need a requirement.txt and does not take the one defined in the project settings:\\nfrom craft_ai_sdk import STEP_PARAMETER\\n\\n# Code for init SDK here ...\\n\\nsdk.create_step(\\n  function_path=\"src/helloWorld.py\",\\n  function_name=\"helloWorld\",\\n  step_name=\"stepName\",\\n  container_config = {\\n      \"requirements_path\": STEP_PARAMETER.NULL,\\n   }\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 9411}), Document(page_content='Warning\\nThe size of the embedded code from your repository must not exceed 5MB.\\nYou can select the part of your repository to import using the included_folders parameter.\\nIf the data you want to import is larger than 5MB, you can use the data store to store it and then import it into your step.\\n\\n\\nNote\\nWhen using a GPU environment, the language parameter must be python-cuda:3.X-11.3 with X equal to 8, 9 or 10 representing the version of python.\\nWithout this, the step will not be able to benefit from GPU computing using cuda.\\n\\n\\n\\nReturns\\uf0c1\\nThe return type is a dict with the following keys :\\n\\nparameters (dict): Information used to create the step with the following keys:\\n\\nstep_name (str): Name of the step.\\nfunction_path (str): Path to the file that contains the function.\\nfunction_name (str): Name of the function in that file.\\nrepository_branch (str): Branch name.\\ndescription (str): Description.\\ninputs (list of dict): List of inputs represented as a dict with the following keys:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 10375}), Document(page_content='name (str): Input name.\\ndata_type (str): Input data type.\\nis_required (bool): Whether the input is required.\\ndefault_value (str): Input default value.\\n\\n\\noutputs (list of dict): List of outputs represented as a dict with the following keys:\\n\\nname (str): Output name.\\ndata_type (str): Output data type.\\ndescription (str): Output description.\\n\\n\\ncontainer_config (dict[str, str]): Some step configuration, with the following optional keys:\\n\\nlanguage (str): Language and version used for the step.\\nrepository_url (str): Remote repository url.\\nincluded_folders (list[str]): List of folders and files in the repository required for the step execution.\\nsystem_dependencies (list[str]): List of system dependencies.\\ndockerfile_path (str): Path to the Dockerfile.\\nrequirements_path (str): Path to the requirements.txt file.\\n\\n\\n\\n\\ncreation_info (dict): Information about the step creation:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 11364}), Document(page_content='creation_info (dict): Information about the step creation:\\n\\ncreated_at (str): The creation date in ISO format.\\nupdated_at (str): The last update date in ISO format.\\ncommit_id (str): The commit id on which the step was built.\\nstatus (str): The step status, if the step creation process is under 2m40s (most of the time it is), is always Ready when this function returns.\\n\\n\\n\\n\\nNote\\nOnce our step is created, we need to create the pipeline that wraps\\nthe step. It is mandatory to create a pipeline once the step is created\\nto be able to use it later. This technical choice was made in\\nanticipation of future multistep functionality. This forces the use of a\\npipeline to contain the steps.\\n\\n\\n\\n\\nExample: Create step from scratch\\uf0c1\\nFunction usage\\nfrom craft_ai_sdk import Input, Output\\n\\ninput1 = Input(\\n    name=\"input1\",\\n    data_type=\"string\",\\n    description=\"A parameter named input1, its type is a string\",\\n    is_required=True,\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 12182}), Document(page_content='input2 = Input(\\n    name=\"input2\",\\n    data_type=\"file\",\\n    description=\"A parameter named input2, its type is a file\"\\n)\\n\\ninput3 = Input(\\n    name=\"input3\",\\n    data_type=\"number\",\\n)\\n\\nprediction_output = Output(\\n    name=\"prediction\",\\n    data_type=\"file\",\\n    default_value=\"default,content,here\",\\n)\\n\\nstep = sdk.create_step(\\n        function_path=\"src/my_reusable_funtion.py\",\\n        function_name=\"my_function\",\\n    inputs_list=[input1, input2, input3],\\n        outputs_list=[prediction_output],\\n    description=\"Apply the model to the sea\",\\n        ## ...\\n)\\n\\n\\n\\nNote\\nIf you need to create a step with more specific configuration, you can do this with a custom dockerfile, more detail about here.\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/createStep.html', 'title': 'Create a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 13111}), Document(page_content='Manage a step — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nStep & Pipeline\\nManage a step\\n\\n\\n\\n\\n\\n\\n\\n\\nManage a step\\uf0c1\\nSummary:\\n\\nFind and get information about steps\\nDelete steps', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/manageStep.html', 'title': 'Manage a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Manage a step\\uf0c1\\nSummary:\\n\\nFind and get information about steps\\nDelete steps\\n\\n\\n\\nFunction Name\\nMethod\\nReturn Type\\nDescription\\n\\n\\n\\nget_step\\nget_step (step_name)\\ndict\\nGet information about a step.\\n\\nlist_steps\\nlist_steps()\\nlist of dict\\nGet a list of available steps.\\n\\ndelete_step\\ndelete_step (step_name)\\ndict[str, str]\\nDelete one step.\\n\\n\\n\\n❓ For step update and deletion, you need the name (the name you provide\\nwhen creating the step) of the step you want to update/delete. You can\\nfind it with function list_steps() (see the previous part).\\n\\nFind and get information about steps\\uf0c1\\nTo get information about a step, we need its name in the environment.\\nYou can search its name in the list of step’s name of the environment.\\n\\nGet list of steps\\uf0c1\\n\\nFunction definition\\uf0c1\\nTo get all steps available in the current environment, you can get a\\nlist of step name with this function:\\nCraftAiSdk.list_steps()\\n\\n\\n\\n\\nReturns\\uf0c1\\nList of steps represented as dict with\\nthe following keys:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/manageStep.html', 'title': 'Manage a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 913}), Document(page_content='Returns\\uf0c1\\nList of steps represented as dict with\\nthe following keys:\\n\\nname (str): Name of the step.\\nstatus (str): either Pending or Ready.\\ncreated_at (str): The creation date in ISO format.\\nupdated_at (str): The last update date in ISO format.\\nrepository_branch (str): The branch of the\\nrepository where the step was built.\\nrepository_url (str): The url of the repository\\nwhere the step was built.\\ncommit_id (str): The commit id on which the step was\\nbuilt.\\n\\n\\n\\n\\nGet information about one step\\uf0c1\\n\\nFunction definition\\uf0c1\\nGet all information (repository, dependency, …) about one step in the\\ncurrent environment with its name.\\nCraftAiSdk.get_step(step_name)\\n\\n\\n\\n\\nParameters\\uf0c1\\n\\nstep_name (str) – The name of the step to get.\\n\\n\\n\\nReturns\\uf0c1\\ndict: None if the step does not exist; otherwise\\nthe step information, with the following keys:\\n\\nparameters (dict): Information used to create the step with the following keys:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/manageStep.html', 'title': 'Manage a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1805}), Document(page_content='parameters (dict): Information used to create the step with the following keys:\\n\\nstep_name (str): Name of the step.\\nfunction_path (str): Path to the file that contains the function.\\nfunction_name (str): Name of the function in that file.\\nrepository_branch (str): Branch name.\\ndescription (str): Description.\\ninputs (list of dict): List of inputs represented as a dict with the following keys:\\n\\nname (str): Input name.\\ndata_type (str): Input data type.\\nis_required (bool): Whether the input is required.\\ndefault_value (str): Input default value.\\n\\n\\noutputs (list of dict): List of outputs represented as a dict with the following keys:\\n\\nname (str): Output name.\\ndata_type (str): Output data type.\\ndescription (str): Output description.\\n\\n\\ncontainer_config (dict[str, str]): Some step configuration, with the following optional keys:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/manageStep.html', 'title': 'Manage a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2629}), Document(page_content='container_config (dict[str, str]): Some step configuration, with the following optional keys:\\n\\nlanguage (str): Language and version used for the step.\\nrepository_url (str): Remote repository url.\\nincluded_folders (list[str]): List of folders and files in the repository required for the step execution.\\nsystem_dependencies (list[str]): List of system dependencies.\\ndockerfile_path (str): Path to the Dockerfile.\\nrequirements_path (str): Path to the requirements.txt file.\\n\\n\\n\\n\\ncreation_info (dict): Information about the step creation:\\n\\ncreated_at (str): The creation date in ISO format.\\nupdated_at (str): The last update date in ISO format.\\ncommit_id (str): The commit id on which the step was built.\\nstatus (str): Either “Pending” or “Ready”.\\n\\n\\n\\n\\n\\n\\n\\nDelete steps\\uf0c1\\n\\nDelete steps function\\uf0c1\\n\\nFunction definition\\uf0c1\\nDelete step in the environment with his name.\\nCraftAiSdk.delete_step(step_name, force_dependents_deletion=False)\\n\\n\\n\\n\\nParameters\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/manageStep.html', 'title': 'Manage a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3365}), Document(page_content='Parameters\\uf0c1\\n\\nstep_name (str) – Name of the step to delete as defined in the create step function.\\nforce_dependents_deletion (bool, optional) – if True the\\nassociated step’s dependencies will be deleted too (pipeline,\\npipeline executions, deployments). Defaults to False.\\n\\n\\n\\nReturns\\uf0c1\\nDeleted step represented as dict (with key “name”). The return\\ntype is a dict [str, str].\\n\\nWarning\\nYou can’t delete a step that is used in a pipeline.\\nYou must delete the pipeline before or use the force_dependents_deletion parameter during step deletion.\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/manageStep.html', 'title': 'Manage a step — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4293}), Document(page_content='Compose a pipeline — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nStep & Pipeline\\nCompose a pipeline', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/composePipeline.html', 'title': 'Compose a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nStep & Pipeline\\nCompose a pipeline\\n\\n\\n\\n\\n\\n\\n\\n\\nCompose a pipeline\\uf0c1\\nA pipeline is a machine learning workflow, consisting of one or more\\nsteps, to deploy code using Docker containers. By specifying the\\noutput of one step as the input of another step, the user can create a\\nfull pipeline formed with a computed acyclic graph (DAG).\\nℹ️ Info: For the moment, the pipelines are only single-step. We are\\nactively working on the integration of multi-steps for the next versions\\nof the platform.\\nTo deploy ML code in production, you need to go through a pipeline. So\\nyou have to go through a single-step pipeline in any case to perform an\\nendpoint or another type of deployment.\\nSummary:\\n\\nCompose a mono-step pipeline\\nDelete a pipeline\\nFind and get pipeline information\\n\\n\\n\\nFunction name\\nMethod\\nReturn type\\nDescription\\n\\n\\n\\ncreate_pipeline\\ncreate_pipeline(pipeline_name, step_name)\\ndict[str, str]\\nCreate a pipeline containing a single step.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/composePipeline.html', 'title': 'Compose a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 832}), Document(page_content='get_pipeline\\nget_pipeline(pipeline_name)\\ndict\\nGet a single pipeline if it exists.\\n\\ndelete_pipeline\\ndelete_pipeline(pipeline_name, force_deployments_deletion=False)\\ndict\\nDelete a pipeline identified by its name and ID.\\n\\nlist_pipelines\\nlist_pipelines()\\nlist of dict\\nGet the list of all pipelines.\\n\\n\\n\\n\\nCreate a mono-step pipeline\\uf0c1\\nBefore creating a pipeline, the step creation must be finished.\\nYou can check this by checking that the step’s status are equal to Ready using the get_step() function.\\n\\nSDK function pipeline\\uf0c1\\n\\nFunction definition\\uf0c1\\npipeline = sdk.create_pipeline(\\n   pipeline_name=\"my_pipeline\",\\n   step_name=\"my_step\",\\n)\\n\\n\\n\\n\\nParameters\\uf0c1\\n\\npipeline_name (str) – Name of the pipeline to create.\\nstep_name (str) – Name of the step to include in the\\npipeline.\\n\\nNote\\nThe step should have the status “Ready” before being used to create the pipeline.\\n\\n\\n\\n\\n\\nReturns\\uf0c1\\nCreated pipeline represented as dict using this keys:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/composePipeline.html', 'title': 'Compose a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1807}), Document(page_content='Returns\\uf0c1\\nCreated pipeline represented as dict using this keys:\\n\\npipeline_name (str): Pipeline name.\\ncreated_at (str): Pipeline date of creation.\\nsteps (list[str]): List of step names.\\nopen_inputs (list[dist]): List of all input of step.\\n\\ninput_name (str): Name of the open input.\\nstep_name (str): Name of the step that provides the open input.\\ndata_type (str): Data type of the open input.\\ndescription (str): Description of the open input.\\ndefault_value (str): Default value of the open input.\\nis_required (bool): Whether the open input is required or not.\\n\\n\\nopen_outputs (list[dist]): List of all output of step.\\n\\noutput_name (str): Name of the open output.\\nstep_name (str): Name of the step that provides the open output.\\ndata_type (str): Data type of the open output.\\ndescription (str): Description of the open output.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/composePipeline.html', 'title': 'Compose a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2666}), Document(page_content='Information about pipeline store\\uf0c1\\nPipeline can have multiple inputs and outputs, or no one. In fact, it’s\\ndependent on the inputs and outputs of step inside. All input and output\\nwill be the same as the step inside. So, you have nothing to configure\\non pipeline creation for input and output.\\n\\n\\n\\nGet information pipeline\\uf0c1\\n\\nGet information about one pipeline\\uf0c1\\n\\nFunction definition\\uf0c1\\nGet all information about one pipeline, referred by its name.\\nCraftAiSdk.get_pipeline(pipeline_name)\\n\\n\\n\\n\\nParameters\\uf0c1\\n\\npipeline_name (str) – Name of the pipeline to get.\\n\\n\\n\\nReturns\\uf0c1\\nThe pipeline information in a dict (or None if the pipeline does not\\nexist), with the following keys:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/composePipeline.html', 'title': 'Compose a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3494}), Document(page_content='pipeline_name (str): Pipeline name.\\ncreated_at (str): Pipeline date of creation.\\ncreated_by (str): ID of the user who created the deployment.\\nlast_execution_id (str): ID of the last execution of the pipeline.\\nsteps (list[str]): List of step names.\\ndeployments (list[str]): List of deployment names which are associated to the pipeline.\\nopen_inputs (list[dist]): List of all input of step.\\n\\ninput_name (str): Name of the open input.\\nstep_name (str): Name of the step that provides the open input.\\ndata_type (str): Data type of the open input.\\ndescription (str): Description of the open input.\\ndefault_value (str): Default value of the open input.\\nis_required (bool): Whether the open input is required or not.\\n\\n\\nopen_outputs (list[dist]): List of all output of step.\\n\\noutput_name (str): Name of the open output.\\nstep_name (str): Name of the step that provides the open output.\\ndata_type (str): Data type of the open output.\\ndescription (str): Description of the open output.\\n\\n\\n\\n\\n\\n\\nGet all pipelines\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/composePipeline.html', 'title': 'Compose a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4159}), Document(page_content='Get all pipelines\\uf0c1\\n\\nFunction definition\\uf0c1\\nGet the list of all pipelines on your environment.\\nCraftAiSdk.list_pipelines()\\n\\n\\n\\n\\nReturns\\uf0c1\\nList of pipelines represented as dict with keys :\\n\\n“pipeline_name” (str): Name of pipeline\\n“created_at” (str): Create date of the pipeline.\\n\\n\\n\\n\\n\\nDelete a pipeline\\uf0c1\\n\\nFunction definition\\uf0c1\\nDelete a pipeline from the current environment.\\nCraftAiSdk.delete_pipeline(pipeline_name, force_deployments_deletion=False)\\n\\n\\n\\nParameters\\uf0c1\\n\\npipeline_name (str) – Name of the pipeline.\\nforce_deployments_deletion (bool, optional) – if True, the associated\\nendpoints will be deleted too. Defaults to False.\\n\\n\\n\\nReturns\\uf0c1\\nThe deleted pipeline and associated deleted endpoints in a dict. The\\nreturned dict contains two keys:\\n\\n“pipeline”(dict): Deleted pipeline represented as dict (with keys\\n“id” and “name”).\\n“endpoints” (list): List of deleted endpoints represented as dict\\n(with keys “id” and “name”).\\n\\n\\nWarning\\nYou can’t delete a pipeline that is used in a Deployment', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/composePipeline.html', 'title': 'Compose a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5139}), Document(page_content='Warning\\nYou can’t delete a pipeline that is used in a Deployment\\n\\n\\n\\n\\n\\nRun a pipeline\\uf0c1\\nNow that your pipeline is created you can run it  directly from the SDK.\\nOr you can configure a deployment.\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/stepPipeline/composePipeline.html', 'title': 'Compose a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6058}), Document(page_content='Deployment — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/introduction.html', 'title': 'Deployment — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\n\\n\\n\\n\\n\\n\\n\\n\\nDeployment\\uf0c1\\nA deployment is a way to trigger an execution of a Machine Learning\\npipeline in a repeatable and automated way. Each pipeline can be\\nassociated with multiple deployments.\\nFor each deployment, you can use 2 distinct execution rules :\\n\\nby endpoint (web API)\\nby periodic trigger (CRON)\\n\\nIn addition, for each deployment, you will need to connect the pipeline\\ninputs and outputs with the desired sources and\\ndestinations. When one of the deployment conditions is met, the\\npipeline is executed by using the computing resources available in\\nits environment.\\nThe results of the execution (predictions, metrics, data, …) can be\\nstored in the data store of the environment and can be easily\\nretrieved by the users. You can find all the information about the\\nexecutions in the execution tracking section.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/introduction.html', 'title': 'Deployment — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 824}), Document(page_content='In addition to the deployment functionality, it is possible to run a\\npipeline directly without deploying it. This allows you to run your\\npipeline on the fly without having to create a specific deployment,\\nwhich is very useful during the experimentation phase.\\nThe main objectives of the deployments are :\\n\\nNo longer take 6 months to deploy ML models in production but a few\\nclicks!\\nAutomating the execution of the pipelines to save time for Data\\nScience teams\\nCreating secured web API to deliver pipeline results to external\\nusers without any DevOps skills\\nAutomatically triggering re-training pipelines when model\\nperformance drops\\nVisualizing pipeline executions, experiment tracking, and ML\\nartifacts\\n\\n\\nDeployment\\n\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/introduction.html', 'title': 'Deployment — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1699}), Document(page_content='Deployment — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/introduction.html', 'title': 'Deployment — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\n\\n\\n\\n\\n\\n\\n\\n\\nDeployment\\uf0c1\\nA deployment is a way to trigger an execution of a Machine Learning\\npipeline in a repeatable and automated way. Each pipeline can be\\nassociated with multiple deployments.\\nFor each deployment, you can use 2 distinct execution rules :\\n\\nby endpoint (web API)\\nby periodic trigger (CRON)\\n\\nIn addition, for each deployment, you will need to connect the pipeline\\ninputs and outputs with the desired sources and\\ndestinations. When one of the deployment conditions is met, the\\npipeline is executed by using the computing resources available in\\nits environment.\\nThe results of the execution (predictions, metrics, data, …) can be\\nstored in the data store of the environment and can be easily\\nretrieved by the users. You can find all the information about the\\nexecutions in the execution tracking section.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/introduction.html', 'title': 'Deployment — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 824}), Document(page_content='In addition to the deployment functionality, it is possible to run a\\npipeline directly without deploying it. This allows you to run your\\npipeline on the fly without having to create a specific deployment,\\nwhich is very useful during the experimentation phase.\\nThe main objectives of the deployments are :\\n\\nNo longer take 6 months to deploy ML models in production but a few\\nclicks!\\nAutomating the execution of the pipelines to save time for Data\\nScience teams\\nCreating secured web API to deliver pipeline results to external\\nusers without any DevOps skills\\nAutomatically triggering re-training pipelines when model\\nperformance drops\\nVisualizing pipeline executions, experiment tracking, and ML\\nartifacts\\n\\n\\nDeployment\\n\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/introduction.html', 'title': 'Deployment — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1699}), Document(page_content='Choose an execution rule — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\nChoose an execution rule', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/chooseExecutionRule.html', 'title': 'Choose an execution rule — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\nChoose an execution rule\\n\\n\\n\\n\\n\\n\\n\\n\\nChoose an execution rule\\uf0c1\\nA deployment is a way to run a Machine Learning pipeline in a\\nrepeatable and automated way.\\nFor each deployment, you can configure an execution rule:\\n\\nby endpoint (web API) : the pipeline will be executed by a call\\nto a web API. In addition, this API will allow, if necessary, to\\nretrieve data as input and deliver the result of the pipeline as\\noutput. Access to the API can be securely communicated to external\\nusers.\\nby periodic trigger (CRON) : rules can be configured to trigger\\nthe pipeline periodically.\\n\\n\\nSummary\\uf0c1\\n\\nDeploy with execution rule: Endpoint\\nDeploy with execution rule: Periodic\\n\\n\\n\\nFunction name\\nMethod\\nReturn type\\nDescription', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/chooseExecutionRule.html', 'title': 'Choose an execution rule — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 838}), Document(page_content='Function name\\nMethod\\nReturn type\\nDescription\\n\\n\\n\\ncreate_deployment\\ncreate_deployment(name, pipeline_name, execution_rule, outputs_mapping=[], inputs_mapping=[], description)\\nDict\\nFunction that deploys a pipeline by creating a deployment which allows a user to trigger the pipeline execution\\n\\n\\n\\n\\n\\nDeploy with execution rule: Endpoint\\uf0c1\\n\\nDefinition function\\uf0c1\\nTo create an auto-mapping deployment where all inputs and outputs are\\nbased on API calls, you can use the create_deployment function. To\\ncreate a deployment with manual mapping, you can use the\\ncreate_deployment function with the additional parameters\\ninputs_mapping to specify the precise mapping between input and\\nsource.\\n\\nWarning\\nOutputs mapping always needs to be precised. Auto-mapping is only\\navailable for inputs.\\n\\n\\nParameters\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/chooseExecutionRule.html', 'title': 'Choose an execution rule — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1555}), Document(page_content='Parameters\\uf0c1\\n\\ndeployment_name (str) – Name of endpoint chosen by the user to\\nrefer to the endpoint\\npipeline_name (str) – Name of pipeline that will be run by the\\ndeployment / endpoint\\nexecution_rule (str) - Execution rule of the deployment. Must be\\n“endpoint” or “periodic”. For convenience, members of the\\nenumeration DEPLOYMENT_EXECUTION_RULES could be used too.\\ndescription (str, optional) – Text description of usage of\\npipeline for user only\\noutputs_mapping (List) - List of all OutputDestination objects\\nwith information for each output mapping.\\ninputs_mapping (List, optional) - List of input mappings, to map\\npipeline inputs to different sources (such as constant values,\\nendpoint inputs, data store or environment variables). See\\nInputSource for more details. For\\nendpoint rules, if an input of the step in the pipeline is not\\nexplicitly mapped, it will be automatically mapped to an endpoint\\ninput with the same name.\\ndescription (str, optional) – Description of the deployment.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/chooseExecutionRule.html', 'title': 'Choose an execution rule — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2333}), Document(page_content='Returns\\uf0c1\\nInformation about the deployment just create in a dict Python format. In\\nthis data, you will have :\\n\\nname - Name of the deployment\\nendpoint_token - Token of the endpoint used to trigger the\\ndeployment. Note that this token is only returned if\\nexecution_rule is “endpoint”.\\ninputs_mapping - Information about the mapping between the\\nsources of the deployment and the inputs of the pipeline\\n\\nstep_input_name - Name of the input in the pipeline\\nendpoint_input_name - Name of the source in the deployment\\nconstant_value - Default value for the input if the source\\nis missing\\nenvironment_variable_name - The name of an environment\\nvariable\\nis_null - A flag indicating that the input should not be\\nprovided any value at execution time\\n\\n\\noutputs_mapping - Information about the mapping between the\\ndestinations of the deployment and the outputs of the pipeline', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/chooseExecutionRule.html', 'title': 'Choose an execution rule — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3324}), Document(page_content='step_output_name - Name of the output in the pipeline\\nendpoint_output_name - Name of the destination in the\\ndeployment\\nis_null - This flag indicates that the output will be\\ndeleted after execution.\\n\\n\\npipeline - Information about the pipeline that will be run by\\nthe deployment\\n\\nname - Name of the pipeline\\n\\n\\n\\n\\n\\n\\nExample\\uf0c1\\n\\nExample auto mapping\\uf0c1\\nsdk.create_deployment(\\n   deployment_name=\"my_deployment\",\\n   pipeline_name=\"my_pipeline\",\\n    execution_rule=\"endpoint\",\\n   outputs_mapping=[],\\n   inputs_mapping=[],\\n)\\n\\n\\n\\n\\nExample manual mapping\\uf0c1\\nsdk.create_deployment(\\n   deployment_name=\"my_deployment\",\\n   pipeline_name=\"my_pipeline\",\\n    execution_rule=\"endpoint\",\\n    inputs_mapping=[\\n                seagull_endpoint_input,\\n                big_whale_input,\\n                salt_constant_input,\\n        ],\\n   outputs_mapping=[prediction_endpoint_ouput],\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/chooseExecutionRule.html', 'title': 'Choose an execution rule — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4188}), Document(page_content=\"Example of return object\\uf0c1\\n{\\n    'id': '29d0c371-49ec-4071-a879-10cdab00fda4', \\n    'name': 'my_deployment', \\n    'endpoint_token': 'EWwIii [...] kjdD5Q', \\n    'inputs_mapping': \\n    [{\\n        'step_input_name': 'seagull', \\n        'endpoint_input_name': 'sourceData'\\n    },\\n    {\\n        'step_input_name': 'big_whale', \\n        'constant_value': 5\\n    },], \\n    'outputs_mapping': \\n    [{\\n        'step_output_name': 'resultMulti', \\n        'endpoint_output_name': 'resultMulti'\\n    }], \\n    'pipeline': {\\n        'name': 'my_pipeline'\\n    }\\n}\\n\\n\\n\\n\\n\\n\\nDeploy with execution rule: Periodic\\uf0c1\\n\\nDefinition function\\uf0c1\\nTo create an auto-mapping deployment where all inputs and outputs are\\nbased on periodicity, you can use the create_deployment function. To\\ncreate a deployment with manual mapping, you can use the\\ncreate_deployment function with the additional parameters\\ninputs_mapping to specify the precise mapping between input and\\nsource.\", metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/chooseExecutionRule.html', 'title': 'Choose an execution rule — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5047}), Document(page_content='Warning\\nInput and output mapping must always be precise. Auto mapping isn’t\\navailable for periodic deployment.\\n\\n\\nParameters\\uf0c1\\n\\ndeployment_name (str) – Name of the deployment chosen\\npipeline_name (str) – Name of pipeline that will be run by the\\ndeployment\\ndescription (str, optional) – Text description of usage of\\npipeline for user only.\\nexecution_rule (str) - Execution rule of the deployment. Must be\\n“endpoint” or “periodic”. For convenience, members of the\\nenumeration DEPLOYMENT_EXECUTION_RULES could be used too.\\nschedule (str, optional) - Schedule of the deployment. Only required if execution_rule is “periodic”.\\nMust be a valid: cron expression.\\nThe deployment will be executed periodically according to this\\nschedule. The schedule must follow this format:\\n<minute> <hour> <day of month> <month> <day of week>. Note\\nthat the schedule is in UTC time zone. “*” means all possible\\nvalues. Here are some examples:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/chooseExecutionRule.html', 'title': 'Choose an execution rule — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5986}), Document(page_content='\"0 0 * * *\" will execute the deployment every day at midnight.\\n\"0 0 5 * *\" will execute the deployment every 5th day of the month at midnight.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/chooseExecutionRule.html', 'title': 'Choose an execution rule — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6905}), Document(page_content='inputs_mapping (List of instances of [InputSource], optional) - List of input mappings, to map pipeline inputs to different :\\nsources (such as constant values, endpoint inputs, or\\nenvironment variables). See InputSource for more details.\\nFor endpoint rules, if an input\\nof the step in the pipeline is not explicitly mapped, it will be\\nautomatically mapped to an endpoint input with the same name.\\nFor periodic rules, all inputs of the step in the pipeline must\\nbe explicitly mapped.\\noutputs_mapping (List of instances of [OutputDestination], optional) - List of output mappings, to map pipeline outputs to different :\\ndestinations. See OutputDestination for more details.\\nFor endpoint execution rules, if an output\\nof the step in the pipeline is not explicitly mapped, it will be\\nautomatically mapped to an endpoint input with the same name.\\nFor other rules, all outputs of the step in the pipeline must\\nbe explicitly mapped.\\ndescription (str, optional) – Description of the deployment.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/chooseExecutionRule.html', 'title': 'Choose an execution rule — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7050}), Document(page_content='Returns\\uf0c1\\nInformation about the deployment just create in a dict Python format.\\n\\nid - Unique identifier of the deployment\\nname - Name of the deployment\\nschedule - Schedule of the deployment. Note that this schedule\\nis only returned if execution_rule is “periodic”.\\nhuman_readable_schedule - Human readable schedule of the\\ndeployment. Note that this schedule is only returned if\\nexecution_rule is “periodic”.\\ninputs_mapping - Information about the mapping between the\\nsources of the deployment and the inputs of the pipeline\\n\\nstep_input_name - Name of the input in the pipeline\\nendpoint_input_name - Name of the source in the deployment\\nconstant_value - Default value for the input if the source\\nis missing\\nenvironment_variable_name - The name of an environment\\nvariable\\nis_null - A flag indicating that the input should not be\\nprovided any value at execution time\\n\\n\\noutputs_mapping - Information about the mapping between the\\ndestinations of the deployment and the outputs of the pipeline', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/chooseExecutionRule.html', 'title': 'Choose an execution rule — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8040}), Document(page_content='step_output_name - Name of the output in the pipeline\\nendpoint_output_name - Name of the destination in the\\ndeployment\\nis_null - This flag indicates that the output will be\\ndeleted after execution.\\n\\n\\npipeline - Information about the pipeline that will be run by\\nthe deployment\\n\\nname - Name of the pipeline\\n\\n\\n\\n\\n\\n\\nExample\\uf0c1\\nSet up deployment to be triggered automatically every 14 days.\\nsdk.create_deployment(\\n   deployment_name=\"my_deployment\",\\n   pipeline_name=\"my_pipeline\",\\n   execution_rule=\"periodic\",\\n   schedule=\"0 14 * * *\"\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/chooseExecutionRule.html', 'title': 'Choose an execution rule — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 9029}), Document(page_content='Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\nDefine the pipeline sources and destinations', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html', 'title': 'Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='home\\nDeployment\\nDefine the pipeline sources and destinations\\n\\n\\n\\n\\n\\n\\n\\n\\nDefine the pipeline sources and destinations\\uf0c1\\nA deployment is a way to run a Machine Learning pipeline in a\\nrepeatable and automated way. First, you have to choose one of the 2\\ndeployments methods.\\nThen, you need to connect the pipeline inputs and outputs with\\nthe desired sources and destinations.\\nSources : This is the origin of the data that you want to connect to\\nthe pipeline inputs. The data can come from the data store, from\\nenvironment variables, from constants or from the endpoint (if this\\ndeployment method has been chosen).\\nDestinations : This is the data drop point that you want to connect\\nto the pipeline outputs. The data can go to the data store or to the\\nendpoint (if this deployment method has been chosen).\\n\\n\\nSummary\\uf0c1\\n\\nGeneral function of I/O mapping\\nCreate input mapping\\nCreate output mapping\\n\\n\\n\\nObjects name\\nConstructor\\nReturn type\\nDescription', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html', 'title': 'Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 901}), Document(page_content='Objects name\\nConstructor\\nReturn type\\nDescription\\n\\n\\n\\nInputSource\\nInputSource(step_input_name, required=False, default=None)\\nInputSource Object\\nCreate a mapping source object for deployment.\\n\\nOutputDestination\\nOutputDestination(step_output_name, endpoint_output_name, required=False, default=None)\\nOutputDestination Object\\nCreate a mapping destination object for deployment.\\n\\n\\n\\n\\n\\nGeneral function of the I/O mapping\\uf0c1\\n\\nMapping rules\\uf0c1\\nWhen you start a new deployment, the data flow is configured with a\\nmapping. You can create this mapping in two ways: auto mapping (only\\navailable with endpoint trigger) or manual mapping in the SDK or UI.\\nAuto mapping automatically maps all inputs to endpoint variables for\\nsources. If you need a different mapping or another trigger, you must\\nmap your inputs and outputs manually.\\nAuto mapping Example', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html', 'title': 'Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1788}), Document(page_content='For each input / output, you had defined a data type in step. This data\\ntype will be the same as the mapped step input / output. You need to map\\nthe good source and destination with the good data type :\\n\\nEndpoint (input and output) → Variable (string, number, …) and\\nfile\\nData store → Only file\\nConstant → Only variable (string, number, …)\\nEnvironment variable → Only variable (string, number, …)\\nNone → Variable (string, number, …) and file\\n\\n\\n\\nLimitations\\uf0c1\\nThe inputs (as outputs) of a step mapped through the endpoint (default\\nmapping) can consist of only a single file or multiple variables. Thus,\\nif you use deployment by triggering an endpoint, you cannot have\\nmultiple files as inputs or outputs due to a technical limitation of the\\nAPI calls.\\nTo have multiple files as the inputs or outputs of an endpoint, you can\\ncompress the files into a single file (for example, with the\\n[tar]{.title-ref} command) to be sent in the API call.\\n\\nExamples: You can do\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html', 'title': 'Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2624}), Document(page_content='Examples: You can do\\uf0c1\\n\\n\\nWarning\\nThe data store is not yet available for the input/output mapping, but\\nit’s coming soon, so stay tuned.\\n\\n\\n\\n\\n\\nExamples: You can’t do\\uf0c1\\n\\n\\n\\n\\n\\n\\n\\nCreation input mapping\\uf0c1\\n\\nImport Dependencies\\uf0c1\\nBefore creating a mapping between the input/output of a pipeline and the\\nsources/destinations of an endpoint, you need to import the InputSource\\nand OutputDestination objects from the SDK.\\nfrom craft_ai_sdk.io import InputSource, OutputDestination\\n\\n\\n\\n\\nFunction Definition\\uf0c1\\nFor each input of your pipeline, in manual mapping, you need to create a\\nmapping object that will be given to the create_deployment() function.\\nboat_endpoint_input = InputSource(\\n    step_input_name=\"apply_model\",\\n\\n    ## Choose from one of these parameters\\n    endpoint_input_name=\"boat\", ## For mapping with an endpoint\\n    environment_variable_name=None, ## For mapping with an environment variable\\n    constant_value=None, ## For mapping with a constant value\\n    is_null=True ## For mapping with None', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html', 'title': 'Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3563}), Document(page_content='is_required=True,\\n    default_value=\"empty\",\\n)\\n\\n\\n\\nParameters\\uf0c1\\n\\nstep_input_name (str): name of the input at the step level to\\nwhich the deployment input will be linked to\\n\\nDifferent possible sources:\\n\\nendpoint_input_name (str, optional): name of the input at the\\nendpoint level to which the step input will be linked to\\nenvironment_variable_name (str, optional): name of the environment\\nvariable to which the step input will be linked to\\nconstant_value (Any, optional): a constant value to which the step\\ninput will be linked to\\n\\nOther parameters:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html', 'title': 'Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4564}), Document(page_content='Other parameters:\\n\\nis_null (True, optional): if specified, the input will not take\\nany value at execution time\\ndefault_value (Any, optional): this parameter can only be\\nspecified if the deployment is an endpoint. In this case, if nothing\\nis passed at the endpoint level, the step input will take the\\ndefault_value\\nis_required (bool, optional): this parameter can only be specified\\nif the deployment is an endpoint.If set to True, the corresponding\\nendpoint input should be provided at execution time.\\n\\n\\n\\nReturn\\uf0c1\\nAn InputSource object that can be used in the deployment creation in the\\ndictionary format.\\n\\n\\n\\nAdditional parameters for source definition\\uf0c1\\nBy default, the source is configured as an endpoint parameter, but you\\ncan configure a different source for your mapping. For each source, you\\nhave to add 1 parameter to :\\n\\nDefine the type of source with the name of parameter added\\nPrecise element about the source\\n\\nWe will list all parameters you can have in your input mapping.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html', 'title': 'Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5093}), Document(page_content='We will list all parameters you can have in your input mapping.\\n\\nWarning\\nYou can only add 1 parameter of source definition. By default, it’s\\nalways endpoint source that is configured.\\n\\n\\nEndpoint source\\nParameter name : endpoint_input_name\\nSource from : Outside through the endpoint\\nValue to put in parameter : name of the input received by the\\nendpoint in the body of the HTTP call\\nExample :\\nendpoint_input = InputSource(\\n    step_input_name=\"seagull\",\\n    endpoint_input_name=\"seagull\",\\n    required=False, #optional \\n    default=\"Eureka\", #optional\\n)\\n\\n\\n\\nData store source\\nParameter name : datastore_path\\nSource from : file content from the data store\\nValue to put in parameter : path to a data store file\\nExample :\\ndata_store_input = InputSource(\\n    step_input_name=\"trainingData\",\\n    datastore_path=\"path/to/trainingData.csv\",\\n)\\n\\n\\nExample step code to read file :\\ndef stepFileIO (trainingData) :\\n   print (trainingData)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html', 'title': 'Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6011}), Document(page_content='Example step code to read file :\\ndef stepFileIO (trainingData) :\\n   print (trainingData)\\n\\n   with open(trainingData[\"path\"]) as f:\\n      contents = f.readlines()\\n      print (contents)\\n\\n\\n\\nConstant source\\nParameter name : constant_value\\nSource from : static value\\nValue to put in parameter : direct value\\nExample :\\nconstant_input = InputSource(\\n  step_input_name=\"salt\",\\n    constant_value=3,\\n)\\n\\n\\n\\nEnvironment variable\\nParameter name : environment_variable_name\\nSource from : the variables set at the level of an Environment\\nin the platform\\nValue to put in parameter : name of the environment variable\\nExample :\\nenv_var_input = InputSource(\\n  step_input_name=\"fish\",\\n    environment_variable_name=\"nameOfEnvVar\",\\n)\\n\\n\\n\\nNone value\\nParameter name : no_value\\nDestination to : void\\nValue to put in parameter : True\\nExample :\\nnull_input = InputSource(\\n  step_input_name=\"fish\",\\n    is_null=True\\n)\\n\\n\\n\\n\\n\\n\\n\\nCreate output mapping\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html', 'title': 'Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6847}), Document(page_content='Create output mapping\\uf0c1\\n\\nImport dependency\\uf0c1\\nBefore creating mapping between input / output of pipeline and sources /\\ndestination of endpoint, you have to import InputSource and\\nOutputDestination objects from SDK.\\nfrom craft_ai_sdk.io import InputSource, OutputDestination\\n\\n\\n\\n\\nFunction definition\\uf0c1\\nFor each output of your pipeline, in manual mapping, you have to create\\nan object, that will be given to create_deployment() function.\\nendpoint_output = OutputDestination(\\n    step_output_name=\"pred_0\",\\n\\n    ## Choose from one of these parameters\\n    endpoint_output_name=\"pred_0\", ## For mapping with an endpoint\\n    is_null=True ## For mapping with None\\n)\\n\\n\\n\\nParameters\\uf0c1\\n\\nstep_output_name (str) - the specific output in the step\\nendpoint_output_name (str, optional) – Name of the endpoint\\noutput to which the output is mapped.\\nis_null (True, optional) – If specified, the output is not\\nexposed as a deployment output.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html', 'title': 'Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7744}), Document(page_content='Return\\uf0c1\\nAn OutputSource object who can be used in the deployment creation.\\n\\n\\n\\nAdditional parameter for destination definition\\uf0c1\\nBy default, the destination is configured as an endpoint parameter, but\\ncan configure a different source for your mapping. For each source, you\\nhave to add 1 parameter to :\\n\\nDefine the type of destination with the name of parameter added\\nPrecise element about the destination\\n\\nWe will list all parameters you can have in your output mapping.\\n\\nWarning\\nYou have to add just 1 parameter of destination definition. If a\\nparameter destination is missing, the function will generate an error\\n(as opposed to input mapping).\\n\\n\\nEndpoint destination\\nParameter name : endpoint_output_name\\nDestination to : Outside through the endpoint\\nValue to put in parameter : name of the output received by the\\nendpoint in the body of the HTTP call\\nExample :\\nprediction_deployment_ouput = OutputDestination(\\n    step_output_name=\"prediction\",\\n    endpoint_output_name=\"beautiful_prediction\",\\n)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html', 'title': 'Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8663}), Document(page_content='Data store destination\\nParameter name : datastore_path\\nDestination to : write a file into the data store\\nValue to put in parameter : path to a data store folder\\nExample :\\nprediction_deployment_ouput = OutputDestination(\\n    step_output_name=\"history_prediction\",\\n    datastore_path=\"path/to/history/folder.csv\",\\n)\\n\\n\\nExample step code to send file :\\n\\ndef stepFileIO () :\\n\\n   text_file = open(\\'history_prediction.txt\\', \\'wb\\')  # Open the file in binary mode\\n   text_file.write(\"Result of step send in file output :) \".encode(\\'utf-8\\'))  # Encode the string to bytes\\n   text_file.close()\\n\\n   fileOjb = {\"history_prediction\" : {\"path\": \"history_prediction.txt\"}}\\n\\n   return fileOjb \\n\\n\\n\\nDynamic path :\\nYou can also specify a dynamic path for the file to be uploaded by using\\none of the following patterns in your datastore path:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html', 'title': 'Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 9663}), Document(page_content='{execution_id}: The execution id of the deployment.\\n{date}: The date of the execution in truncated ISO 8601 (YYYYMMDD) format.\\n{date_time}: The date of the execution in ISO 8601 (YYYYM-MDD_hhmmss) format.\\n\\nExample with a dynamic path :\\nprediction_deployment_ouput = OutputDestination(\\n    step_output_name=\"history_prediction\",\\n    datastore_path=\"path/to/history/exec_{execution_id}.csv\",\\n)\\n\\n\\n\\nVoid destination\\nParameter name : no_destination\\nDestination to : void\\nValue to put in parameter : True\\nExample :\\nprediction_deployment_ouput = OutputDestination(\\n    step_output_name=\"history_prediction\",\\n    is_null=True,\\n)\\n\\n\\n\\n\\n\\n\\n\\n4. Generate new endpoint token\\uf0c1\\nIf you need to alter the endpoint token for an endpoint, you can\\ngenerate a new one with the following SDK function.\\nsdk.generate_new_endpoint_token(endpoint_name=\"*your-endpoint-name*\")\\n\\n\\n\\nWarning\\nThis will permanently deactivate the previous token.\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/definePipelineSourcesDestinations.html', 'title': 'Define the pipeline sources and destinations — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 10486}), Document(page_content='Execute a pipeline — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\nExecute a pipeline', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/executePipeline.html', 'title': 'Execute a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\nExecute a pipeline\\n\\n\\n\\n\\n\\n\\n\\n\\nExecute a pipeline\\uf0c1\\nAn execution of a pipeline creates an execution on the platform.\\nEach execution is associated with a pipeline with the definition of the\\nvalues of its inputs and outputs. The execution triggers the\\nexecution of the pipeline on one or more Kubernetes containers using the\\ncomputational resources available on the environment. All the\\nresults and artifacts of the execution can be retrieved in the\\nExecution Tracking tab.\\nThere are two ways to execute a pipeline:\\n\\nby creating a deployment: the execution will then depend on the\\nselected execution rule and will be performed when the execution\\ncondition is met (call for an endpoint, periodicity for a CRON,\\netc…)\\nby running it instantly with the sdk: It is then necessary to\\nindicate the values for each input of the pipeline.\\n\\n\\nSummary\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/executePipeline.html', 'title': 'Execute a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 832}), Document(page_content='Summary\\uf0c1\\n\\nRun a pipeline\\nTrigger a deployment by endpoint with SDK Craft AI\\nTrigger a deployment by endpoint with request\\nGet result of a past execution\\n\\n\\n\\nFunction name\\nMethod\\nReturn type\\nDescription\\n\\n\\n\\nrun_pipeline\\nrun_pipeline(pipeline_name, inputs=None, inputs_mapping=None, outputs_mapping=None)\\ndict\\nExecutes the pipeline on the platform.\\n\\nretrieve_endpoint_results\\nretrieve_endpoint_results(endpoint_name, execution_id, endpoint_token)\\ndict\\nGet result of endpoint execution.\\n\\n\\n\\n\\n\\nRun a pipeline\\uf0c1\\nA run is an execution of a pipeline on the platform. SDK\\nfunction that runs a pipeline to create an execution.\\nrun_pipeline(pipeline_name, inputs=None, inputs_mapping=None, outputs_mapping=None)\\n\\n\\nParameters', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/executePipeline.html', 'title': 'Execute a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1716}), Document(page_content='Parameters\\n\\npipeline_name (str) – Name of an existing pipeline.\\ninputs (dict, optional) - Dictionary of inputs to pass to the\\npipeline with input names as dict keys and corresponding values as\\ndict values. For files, the value should be the path to the file or\\na file content as an instance of io.IOBase. Defaults to None.\\ninputs_mapping (list of instances of [InputSource]{.title-ref}) -\\nList of input mappings, to map pipeline inputs to different sources\\n(such as environment variables). See [InputSource]{.title-ref} for\\nmore details.\\noutputs_mapping (list of instances of\\n[OutputDestination]{.title-ref}) - List of output mappings, to map\\npipeline outputs to different destinations (such as datastore). See\\n[OutputDestination]{.title-ref} for more details.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/executePipeline.html', 'title': 'Execute a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2416}), Document(page_content='Returns\\nCreated pipeline execution represented as dict with execution_id and outputs as keys.\\nThe output values will be in the output object represented as dict with output_names as keys and corresponding values as values.\\nExample of return object :\\n{\\n  \"execution_id\": \"my-pipeline-8iud6\",\\n  \"outputs\": {\\n      \"output_number\": 0.117,\\n      \"output_text\": \"This is working fine\",\\n   }\\n}\\n\\n\\n\\n\\nTrigger a deployment with execution rule by endpoint with SDK Craft AI\\uf0c1\\nSDK function that triggers the deployment of our pipeline.\\nsdk.trigger_endpoint(endpoint_name, endpoint_token, inputs={},\\nwait_for_results=True)\\n\\n\\nParameters\\n\\nendpoint_name (str) – Name of the endpoint.\\nendpoint_token (str) – Token to access endpoint.\\ninputs (dict) - Inputs value for endpoint call.\\nwait_for_results (bool, optional) – Automatically call\\nretrieve_endpoint_results (True by default)\\n\\nReturns\\nCreated pipeline execution represented as dict.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/executePipeline.html', 'title': 'Execute a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3178}), Document(page_content='Returns\\nCreated pipeline execution represented as dict.\\n\\n\\nTrigger a deployment with execution rule by endpoint with request\\uf0c1\\nFor trigger a deployment who is set up with an endpoint, you can also\\nsend request with your element defined in the pipeline input.\\nExamples in Python for variable :\\nimport requests\\n\\nr = requests.post(\\n    \"https://your_environment_url/my_endpoint\",\\n    json={\\n        \"input1\": \"value1\",\\n        \"input2\": [1,2,3]\\n        \"input3\": False\\n    },\\n    headers={\"Authorization\": \"EndpointToken \" + ENDPOINT_TOKEN }\\n)\\n\\n\\nExamples in Python for file (not available with auto mapping) :\\nimport requests\\n\\nr = requests.post(\\n    \"https://your_environment_url/my_multistep_endpoint\",\\n    files={\"data\": open(\"my_file.txt\", \"rb\")},\\n    headers={\"Authorization\": \"EndpointToken \" + ENDPOINT_TOKEN }\\n)\\n\\n\\n\\nNote\\nWe have explained in this documentation how to trigger the endpoint with\\nPython, but you can obviously send a request from any tool (curl,\\npostman, JavaScript, …).', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/executePipeline.html', 'title': 'Execute a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4042}), Document(page_content='Warning\\nInputs and outputs have size limits. This limit is 0.06MB for cumulative inputs and also 0.06MB for cumulative outputs. This input/output size limit is available for all trigger/deployment types (run, endpoint or CRON). This limit applies regardless of the source or destination of the input/output.\\nOnly file inputs/outputs are not affected by this limit. We recommend that you use this method when transferring large amounts of data.\\n\\n\\n\\nGet result of a past execution\\uf0c1\\nGet the results of an endpoint execution.\\nCraftAiSdk.retrieve_endpoint_results(endpoint_name, execution_id, endpoint_token)\\n\\n\\nParameters\\n\\nendpoint_name (str) - Name of the endpoint.\\nexecution_id (str) - Name of the execution returned by trigger_endpoint.\\nendpoint_token (str) - Token to access endpoint.\\n\\nReturns\\nCreated pipeline execution represented as dict with the following keys:\\n\\noutputs (dict): Dictionary of outputs of the pipeline with output names as keys and corresponding values as values.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/executePipeline.html', 'title': 'Execute a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5030}), Document(page_content='Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/executePipeline.html', 'title': 'Execute a pipeline — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6018}), Document(page_content='Follow the deployment executions — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\nFollow the deployment executions\\n\\n\\n\\n\\n\\n\\n\\n\\nFollow the deployment executions\\uf0c1\\nA deployment', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Follow the deployment executions\\uf0c1\\nA deployment\\n\\nis a way to run a Machine Learning pipeline in a\\nrepeatable and automated way. Once it is created, you can find the setup\\nof your deployments in the pipeline store. In addition, you can find\\nall the information about the executions of the deployments in the\\nexecution tracking.\\n\\n\\nWarning\\nDates provided by the Web UI and SDK are always expressed in Coordinated Universal Time (UTC).\\n\\n\\nSummary\\uf0c1\\n\\nGet information about a deployment\\nDelete a deployment or an execution\\nFollow the execution tracking\\n\\n\\n\\nFunction name\\nMethod\\nReturn type\\nDescription\\n\\n\\n\\nlist_deployments\\nlist_deployments()\\nlist of dict\\nGet the list of all deployments.\\n\\nget_deployment\\nget_deployment(deployment_name)\\ndict\\nGet information of a deployment.\\n\\ndelete_deployment\\ndelete_deployment(deployment_name)\\ndict\\nDelete a deployment identified by its name.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 946}), Document(page_content='get_pipeline_execution\\nget_pipeline_execution(execution_id)\\ndict\\nGet the status of one pipeline execution identified by its name.\\n\\ndelete_pipeline_execution\\ndelete_pipeline_execution(execution_id)\\ndict\\nDelete pipeline execution.\\n\\nget_pipeline_execution_input\\nget_pipeline_execution_input(execution_id, input_name)\\ndict\\nGet information about an input of an execution.\\n\\nget_pipeline_execution_output\\nget_pipeline_execution_output(execution_id, output_name)\\ndict\\nGet information about an output of an execution.\\n\\n\\n\\n\\n\\nGet information about a deployment\\uf0c1\\n\\nList of deployments\\uf0c1\\nGet the list of all deployments.\\nCraftAiSdk.list_deployments()\\n\\n\\nReturns\\nList of deployments represented as dict with the following keys:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1813}), Document(page_content='Returns\\nList of deployments represented as dict with the following keys:\\n\\nname (str): Name of the deployment.\\npipeline_name (str): Name of the pipeline associated to the deployment.\\nversion (str): Version of the pipeline associated to the deployment.\\nexecution_count (int): Number of times the deployment has been executed.\\ntype (str): Type of the deployment. Can be endpoint, run or periodic.\\n\\n\\n\\nGet deployment information\\uf0c1\\nGet information of a deployment.\\nCraftAiSdk.get_deployment(deployment_name)\\n\\n\\nParameters\\n\\ndeployment_name (str) – Name of the deployment.\\n\\nReturns\\nDeployment information represented as dict (with keys id, name and\\npipeline).\\n\\nname (str): Name of the deployment.\\npipeline (dict): Pipeline associated to the deployment represented as dict with the following keys:\\n\\nname (str): Name of the pipeline.\\n\\n\\ninputs_mapping (list of dict): List of inputs mapping represented as dict with the following keys:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2450}), Document(page_content='step_input_name (str): Name of the step input.\\ndata_type (str): Data type of the step input.\\ndescription (str): Description of the step input.\\nconstant_value (str): Constant value of the step input. Note that this key is only returned if the step input is mapped to a constant value.\\nenvironment_variable_name (str): Name of the environment variable. Note that this key is only returned if the step input is mapped to an environment variable.\\nendpoint_input_name (str): Name of the endpoint input. Note that this key is only returned if the step input is mapped to an endpoint input.\\nis_null (bool): Whether the step input is mapped to null. Note that this key is only returned if the step input is mapped to null.\\ndatastore_path (str): Datastore path of the step input. Note that this key is only returned if the step input is mapped to the datastore.\\nis_required (bool): Whether the step input is required. Note that this key is only returned if the step input is required.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3374}), Document(page_content='default_value (str): Default value of the step input. Note that this key is only returned if the step input has a default value.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4350}), Document(page_content='outputs_mapping (list of dict): List of outputs mapping represented as dict with the following keys:\\n\\nstep_output_name (str): Name of the step output.\\ndata_type (str): Data type of the step output.\\ndescription (str): Description of the step output.\\nendpoint_output_name (str): Name of the endpoint output. Note that this key is only returned if the step output is mapped to an endpoint output.\\nis_null (bool): Whether the step output is mapped to null. Note that this key is only returned if the step output is mapped to null.\\ndatastore_path (str): Datastore path of the step output. Note that this key is only returned if the step output is mapped to the datastore.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4481}), Document(page_content='endpoint_token (str): Token of the endpoint. Note that this key is only returned if the deployment is an endpoint.\\nschedule (str): Schedule of the deployment. Note that this key is only returned if the deployment is a periodic deployment.\\nhuman_readable_schedule (str): Human readable schedule of the deployment. Note that this key is only returned if the deployment is a periodic deployment.\\ncreated_at (str): Date of creation of the deployment.\\ncreated_by (str): ID of the user who created the deployment.\\nupdated_at (str): Date of last update of the deployment.\\nupdated_by (str): ID of the user who last updated the deployment.\\nlast_execution_id (str): ID of the last execution of the deployment.\\nactive (bool): Whether the deployment is active.\\ndescription (str): Description of the deployment.\\nexecution_rule (str): Execution rule of the deployment.\\n\\nReturn type\\ndict\\n\\n\\n\\nDelete a deployment or an execution\\uf0c1', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5150}), Document(page_content='Return type\\ndict\\n\\n\\n\\nDelete a deployment or an execution\\uf0c1\\n\\n\\nDelete a deployment\\uf0c1\\nDelete a deployment identified by its name.\\nCraftAiSdk.delete_deployment(deployment_name)\\n\\n\\nParameters\\n\\ndeployment_name (str) – Name of the deployment.\\n\\nReturns\\nDeleted deployment represented as dict (with keys id, name). The\\nreturn data type is dict.\\n\\nWarning\\nBe careful, deleting a deployment will delete all its executions.\\n\\n\\nDelete an execution\\uf0c1\\nDelete one pipeline execution identified by its execution_id.\\nCraftAiSdk.delete_pipeline_execution(execution_id)\\n\\n\\nParameters\\n\\nexecution_id (str) - Name of the pipeline execution.\\n\\nReturns\\nDeleted pipeline execution represented as dict with the following keys:\\n\\nexecution_id (str): Name of the pipeline execution deleted.\\n\\n\\n\\n\\nFollow the execution tracking\\uf0c1\\n\\nGet execution list\\uf0c1\\nGet the status of one pipeline execution identified by its name.\\nCraftAiSdk.get_pipeline_execution(execution_id)\\n\\n\\nParameters\\n\\nexecution_id (str) - ID of the pipeline execution.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6006}), Document(page_content='Parameters\\n\\nexecution_id (str) - ID of the pipeline execution.\\n\\nReturns\\nInformation on the pipeline execution with id execution_id represented as dict.\\n\\nexecution_id (str): Name of the pipeline execution.\\nstatus (str): Status of the pipeline execution.\\ncreated_at (str): Date of creation of the pipeline\\ncreated_by (str): ID of the user who created the pipeline execution. In the case of a pipeline run, this is the user who triggered the run. In the case of an execution via a deployment, this is the user who created the deployment.\\nend_date (str): Date of completion of the pipeline execution.\\npipeline_name (str): Name of the pipeline used for the execution.\\ndeployment_name (str): Name of the deployment used for the execution.\\nsteps (list of obj): List of the step executions represented as dict with the following keys:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6929}), Document(page_content='name (str): Name of the step.\\nstatus (str): Status of the step.\\nstart_date (str): Date of start of the step execution.\\nend_date (str): Date of completion of the step execution.\\ncommit_id (str): Id of the commit used to build the step.\\nrepository_url (str): Url of the repository used to build the step.\\nrepository_branch (str): Branch of the repository used to build the step.\\n\\n\\ninputs (list of dict): List of inputs represented as a dict with the following keys:\\n\\nstep_input_name (str): Name of the input.\\n`data_type (str): Data type of the input.\\nsource (str): Source of type of the input. Can be environment_variable, datastore, constant, is_null endpoint or run.\\nendpoint_input_name (str): Name of the input in the endpoint execution if source is endpoint.\\nconstant_value (str): Value of the constant if source is constant.\\nenvironment_variable_name (str): Name of the environment variable if source is environment_variable.\\nis_null (bool): True if source is is_null.\\nvalue: Value of the input.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7757}), Document(page_content='outputs (list of dict): List of outputs represented as a dict with the following keys:\\n\\nstep_output_name (str): Name of the output.\\n`data_type (str): Data type of the output.\\ndestination (str): Destination of type of the output. Can be datastore, is_null endpoint or run.\\nendpoint_output_name (str): Name of the output in the endpoint execution if destination is endpoint.\\nis_null (bool): True if destination is is_null.\\nvalue: Value of the output.\\n\\n\\n\\n\\n\\nGet execution logs\\uf0c1\\nGet the logs of an executed pipeline identified by its name.\\nCraftAiSdk.CraftAiSdk.get_pipeline_execution_logs(*pipeline_name*, *execution_id*,\\nfrom_datetime=None, to_datetime=None, limit=None)\\n\\n\\nParameters', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8758}), Document(page_content='Parameters\\n\\npipeline_name (str) – Name of an existing pipeline.\\nexecution_id (str) – ID of the pipeline execution.\\nfrom_datetime (datetime.time, optional) – Datetime from which the\\nlogs are collected.\\nto_datetime (datetime.time, optional) – Datetime until which the\\nlogs are collected.\\nlimit (int, optional) – Maximum number of logs that are\\ncollected.\\n\\nReturns\\nList of collected logs represented as dict (with keys message,\\ntimestamp and stream). The return type is a list.\\n\\n\\nGet Input and Output of an execution\\uf0c1\\n\\nInput\\uf0c1\\nGet the input value of an executed pipeline identified by its execution_id.\\nCraftAiSdk.get_pipeline_execution_input(execution_id, input_name)\\n\\n\\nParameters\\n\\nexecution_id (str) - ID of the pipeline execution.\\ninput_name (str) - Name of the input.\\n\\nReturns\\nInformation on the input represented as a dict with the following keys :', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 9428}), Document(page_content='Returns\\nInformation on the input represented as a dict with the following keys :\\n\\nstep_input_name (str): Name of the input.\\ndata_type (str): Data type of the input.\\nsource (str): Source of type of the input. Can be environment_variable, datastore, constant, is_null endpoint or run.\\nendpoint_input_name (str): Name of the input in the endpoint execution if source is endpoint.\\nconstant_value (str): Value of the constant if source is constant.\\nenvironment_variable_name (str): Name of the environment variable if source is environment_variable.\\nis_null (bool): True if source is is_null.\\nvalue: Value of the input.\\n\\n\\n\\nOutput\\uf0c1\\nGet the output value of an executed pipeline identified by its execution_id.\\nCraftAiSdk.get_pipeline_execution_output(execution_id, output_name)\\n\\n\\nParameters\\n\\nexecution_id (str) - ID of the pipeline execution.\\noutput_name (str) - Name of the output.\\n\\nReturns\\nInformation on the output represented as a dict with the following keys :', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 10197}), Document(page_content='Returns\\nInformation on the output represented as a dict with the following keys :\\n\\nstep_output_name (str): Name of the output.\\ndata_type (str): Data type of the output.\\ndestination (str): Destination of type of the output. Can be datastore, is_null endpointorrun`.\\nendpoint_output_name (str): Name of the output in the endpoint ex-\\necution if destination is endpoint.\\nis_null (bool): True if destination is is_null.\\nvalue: Value of the output.\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/followDeploymentExecutions.html', 'title': 'Follow the deployment executions — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 11074}), Document(page_content='Metrics — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\nMetrics', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/metrics.html', 'title': 'Metrics — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nDeployment\\nMetrics\\n\\n\\n\\n\\n\\n\\n\\n\\nMetrics\\uf0c1\\nIn the context of MLOps, tracking and monitoring metrics is critical for\\nassessing the performance and progress of machine learning pipelines.\\nThe CraftAiSdk platform provides a comprehensive set of features for\\ndefining and recording metrics at each pipeline execution.\\nWith measurement capabilities, you can efficiently track and retrieve\\nthe metrics associated with each execution in your machine learning\\npipelines. This enables you to valuable insights and make informed\\ndecisions about your models and deployments.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/metrics.html', 'title': 'Metrics — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 821}), Document(page_content='Pipeline Metrics\\uf0c1\\nThe record_metric_value function allows you to create or update a\\npipeline metric within a step code. This function allows you to store\\nthe name and corresponding value of a particular metric.\\nYou do not need to declare anything outside of the step, you can just\\nuse record_metrics_value() in your step code. Remember, if you want to\\nuse the SDK in your step code, you don’t need to specify your\\nenvironment URL or token in the builder parameters.\\nAfter the execution is finished, you can find all your metric values in\\nthe web interface on the Execution page and on the Metrics tab.\\n\\nUpload Metrics\\uf0c1\\nCurrently, pipeline metrics can only have one numeric value and one name\\nfor each execution metric. If multiple metrics are entered with\\nidentical names, only the last metric will be retained.\\n\\nWarning\\nThis function can only be used in the source code of the step running on\\nthe platform. When used outside a code step, it doesn’t send metrics\\nand displays a warning message.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/metrics.html', 'title': 'Metrics — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1427}), Document(page_content='CraftAiSdk.record_metric_value(name, value)\\n\\n\\nParameters\\n\\nname (str) - The name of the metric to store.\\nvalue (float) - The value of the metric to store.\\n\\nReturns\\nList of execution metrics as dict (with keys “name”, “value”,\\n“created_at”, “execution_id”, “deployment_name”,\\n“pipeline_name”).\\nExample\\nHere is a very simple example of step code that sends only 2 different\\nmetrics.\\n\\nNote\\nDon’t forget to import the craft-ai-sdk package in the step code and to\\nlist the library in your requirement.txt to install it on the step\\nexecution context.\\n\\nfrom craft_ai_sdk import CraftAiSdk\\n\\ndef metricsStep () :\\n\\n    sdk = CraftAiSdk()\\n\\n    # Some code \\n\\n    sdk.record_metric_value(\"accuracy\", 0.1409)\\n    sdk.record_metric_value(\"loss\", 1/3)\\n\\n    print (\"Metrics are sent\")', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/metrics.html', 'title': 'Metrics — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2423}), Document(page_content='print (\"Metrics are sent\")\\n\\n\\n\\n\\nGet metrics\\uf0c1\\nThe get_metrics function retrieves a list of pipeline metrics. You can\\nfilter the metrics based on the name, pipeline name, deployment name, or\\nexecution ID. It’s important to note that only one of the parameters\\n(name, pipeline_name, deployment_name, execution_id) can be set at a\\ntime.\\nCraftAiSdk.get_metrics(name=None, pipeline_name=None, deployment_name=None, execution_id=None)\\n\\n\\nParameters\\n\\nname (str, optional) - The name of the metric to retrieve.\\npipeline_name (str, optional) - Filter metrics by pipeline. If not\\nspecified, all pipelines will be considered.\\ndeployment_name (str, optional) - Filter metrics by deployment. If\\nnot specified, all deployments will be considered.\\nexecution_id (str, optional) - Filter metrics by execution. If not\\nspecified, all executions will be considered.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/metrics.html', 'title': 'Metrics — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3163}), Document(page_content='Returns\\nThe function returns a list of execution metrics as dictionaries. Each\\nmetric entry contains the following keys: “name”, “value”,\\n“created_at”, “execution_id”, “deployment_name”,\\n“pipeline_name”.\\n\\n\\n\\nList Metrics\\uf0c1\\nThe Craft AI platform provides robust features for defining and\\nrecording list metrics during pipeline execution. This functionality\\nallows you to store the name and corresponding list of values for a\\nspecific metric.\\nTo create or update a list metric within a step code, you can utilize\\nthe record_list_metric_values() function. Afterwards, you\\ncan retrieve your metrics outside the step using the\\nget_list_metrics() function. Additionally, you can access\\nall your metric values in the web interface via the Metrics tab on the\\nExecution page.\\nSimilar to pipeline metrics, list metrics can only consist of a list of\\nnumbers (integer or float).', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/metrics.html', 'title': 'Metrics — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4007}), Document(page_content='Upload list metrics\\uf0c1\\nThe record_list_metric_values() function enables you to\\nadd values to a metric list by specifying the name of the metric list\\nand the corresponding values. There is no need to declare anything\\noutside of the step; simply use\\nrecord_list_metric_values() in your step code, as you\\nwould for pipeline metrics.\\nIt’s important to note that when using the\\nrecord_list_metric_values() function, it can only be\\nutilized within the source code of the step running on the platform.\\nWhen uploading list metrics, you have the option to either specify a\\nPython list directly or upload values individually, specifying the same\\nmetric name (which will automatically accumulate into a list).\\nHere is an example of step code that sends two different lists metrics:\\n\\nWarning\\nThis function can only be used in the source code of the step running on\\nthe platform. When used outside of a code step, it doesn’t send metrics\\nand displays a warning message.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/metrics.html', 'title': 'Metrics — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4873}), Document(page_content='CraftAiSdk.record_list_metric_values(name, values)\\n\\n\\nParameters\\n\\nname (str) - Name of the metric list to add values.\\nvalues (list of float or float) - Values of the metric list to\\nadd.\\n\\nReturns\\nThis function returns nothing (None).\\nExample\\nHere is a very simple example of step code that sends only 2 different\\nlists metrics.\\n\\nNote\\nDon’t forget to import the craft-ai-sdk package in the step code and to\\nlist the library in your requirement.txt to install it on the step\\nexecution context.\\n\\nfrom craft_ai_sdk import CraftAiSdk\\nimport math \\n\\ndef metricsStep():\\n    sdk = CraftAiSdk()\\n\\n    # Some code \\n\\n    # Just one list upload\\n    sdk.record_list_metric_values(\"accuracy_list\", [0.89, 0.92, 0.95])\\n\\n    # Tow list upload, the lists will be concatenated in loss_list list metrics \\n    sdk.record_list_metric_values(\"loss_list\", [1.4, 1.2])\\n    sdk.record_list_metric_values(\"loss_list\", [1.1, 1.0])', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/metrics.html', 'title': 'Metrics — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5829}), Document(page_content='# Upload multiple values that will be concatenated into 1 metrics list *logx* with all values\\n    for i in range (1, 50) : \\n\\n        sdk.record_list_metric_values(\"logx\", math.log(i))\\n\\n    print(\"List metrics are sent\")\\n\\n\\n\\nWarning\\nA pipeline metrics and a list metrics can have the same name in the same\\nexecution. A metrics list is limited to a maximum of 50,000 values per\\nexecution.\\n\\n\\n\\nGet list metrics\\uf0c1\\nTo retrieve a list of metric lists, you can use the\\nget_list_metrics() function. This function allows you to\\nfilter the metric lists based on the name, pipeline name, deployment\\nname, or execution ID.\\nIt’s important to note that only one of the parameters\\n(name, pipeline_name,\\ndeployment_name, execution_id) can be set at\\na time.\\nCraftAiSdk.get_list_metrics(name=None, pipeline_name=None, deployment_name=None, execution_id=None)\\n\\n\\nParameters', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/metrics.html', 'title': 'Metrics — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6734}), Document(page_content='Parameters\\n\\nname (str, optional) - Name of the metric list to retrieve.\\npipeline_name (str, optional) - Filter metric lists by pipeline,\\ndefaults to all the pipelines.\\ndeployment_name (str, optional) - Filter metric lists by\\ndeployment, defaults to all the deployments.\\nexecution_id (str, optional) - Filter metric lists by execution,\\ndefaults to all the executions.\\n\\nReturns\\nThe function returns a list of execution metrics as dictionaries. Each\\nmetric entry contains the following keys: “name”, “value”,\\n“created_at”, “execution_id”, “deployment_name”,\\n“pipeline_name”.\\nHere is an example of how to use the get_list_metrics()\\nfunction:\\nlist_metrics = CraftAiSdk.get_list_metrics(name=\"accuracy_list\", pipeline_name=\"my_pipeline\")\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/deployment/metrics.html', 'title': 'Metrics — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7574}), Document(page_content='Craft AI SDK documentation — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nCraft AI SDK documentation', metadata={'source': 'https://mlops-platform-documentation.craft.ai/histo_doc_sdk.html', 'title': 'Craft AI SDK documentation — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nCraft AI SDK documentation\\n\\n\\n\\n\\n\\n\\n\\n\\nCraft AI SDK documentation\\uf0c1\\nTo check the version of the installed craft-ai-sdk, run the following command in a console:\\npip show craft-ai-sdk\\n\\n\\nOr in a Python shell:\\nimport craft_ai_sdk\\ncraft_ai_sdk.__version__\\n\\n\\n\\nCurrent\\uf0c1\\nsdk_0.31.2.pdf', metadata={'source': 'https://mlops-platform-documentation.craft.ai/histo_doc_sdk.html', 'title': 'Craft AI SDK documentation — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 840}), Document(page_content='Or in a Python shell:\\nimport craft_ai_sdk\\ncraft_ai_sdk.__version__\\n\\n\\n\\nCurrent\\uf0c1\\nsdk_0.31.2.pdf\\n\\n\\nArchives\\uf0c1\\nsdk_0.31.1.pdf\\nsdk_0.31.0.pdf\\nsdk_0.30.0.pdf\\nsdk_0.29.0.pdf\\nsdk_0.28.0.pdf\\nsdk_0.27.1.pdf\\nsdk_0.27.0.pdf\\nsdk_0.25.0.pdf\\nsdk_0.24.1.pdf\\nsdk_0.24.0.pdf\\nsdk_0.23.3.pdf\\nsdk_0.23.2.pdf\\nsdk_0.23.1.pdf\\nsdk_0.23.0.pdf\\nsdk_0.22.0.pdf\\nsdk_0.21.1.pdf\\nsdk_0.21.0.pdf\\nsdk_0.20.0.pdf\\nsdk_0.19.0.pdf\\nsdk_0.18.0.pdf\\nsdk_0.17.0.pdf\\nsdk_0.16.0.pdf\\nsdk_0.15.0.pdf\\nsdk_0.14.1.pdf\\nsdk_0.14.0.pdf\\nsdk_0.13.2.pdf\\nsdk_0.13.1.pdf\\nsdk_0.13.0.pdf\\nsdk_0.12.0.pdf\\nsdk_0.11.0.pdf\\nsdk_0.10.1.pdf\\nsdk_0.10.0.pdf\\nsdk_0.9.1.pdf\\nsdk_0.9.0.pdf\\nsdk_0.8.1.pdf\\nsdk_0.8.0.pdf\\nsdk_0.7.0.pdf\\nsdk_0.6.0.pdf\\nsdk_0.5.0.pdf\\nsdk_0.4.0.pdf\\nsdk_0.3.0.pdf\\nsdk_0.2.0.pdf\\nsdk_0.1.6.pdf\\nsdk_0.1.5.pdf\\nsdk_0.1.4.pdf\\nsdk_0.1.3.pdf\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/histo_doc_sdk.html', 'title': 'Craft AI SDK documentation — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1067}), Document(page_content='Craft AI changelog — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\n\\n\\nCraft AI changelog\\uf0c1\\n\\nV1.1.6 - 17/11/2023\\uf0c1\\nSDK version : 0.31.2 \\nAPI version : 2023-11-13', metadata={'source': 'https://mlops-platform-documentation.craft.ai/changeLog.html', 'title': 'Craft AI changelog — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI changelog\\uf0c1\\n\\nV1.1.6 - 17/11/2023\\uf0c1\\nSDK version : 0.31.2 \\nAPI version : 2023-11-13\\n\\nFixed bugs and improvements\\n\\n\\n\\nV1.1.5 - 08/11/2023\\uf0c1\\nSDK version : 0.31.1 \\nAPI version : 2023-11-05\\n\\nSearch bar added to projects page\\nAdded size limit for json entries in run_pipeline() SDK documentation\\nFixed bugs and improvements\\n\\n\\n\\nV1.1.4 - 02/11/2023\\uf0c1\\nSDK version : 0.31.0 \\nAPI version : 2023-10-30\\n\\nCompare execution for list metrics\\nAdd created_by , last_execution_id and deployments parameters to the return from get_pipeline()\\nDeployment and pipeline slider -> display will evolve again\\nFixed bugs and improvements\\n\\n\\n\\nV1.1.3 - 24/10/2023\\uf0c1\\nSDK version : 0.30.0 \\nAPI version : 2023-10-23-2\\n\\nAdded information to the return of get_deployment()\\nAdded the ability to add a description to a deployment\\nUpdate information bubble infra env + associated tooltips\\nCorrection of filter overflows on small tables in the front end\\nFixed bugs and improvements', metadata={'source': 'https://mlops-platform-documentation.craft.ai/changeLog.html', 'title': 'Craft AI changelog — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 906}), Document(page_content='V1.1.2 - 19/10/2023\\uf0c1\\nSDK version : 0.29.0 \\nAPI version : 2023-10-16-2\\n\\nChange of the output format of the size parameter of the functions get_data_store_object_information and list_data_store_objects.\\nWe now have str with the storage size units for a better readability of the file sizes.\\nFixed bugs and improvements\\n\\n\\n\\nV1.1.1 - 13/10/2023\\uf0c1\\nSDK version : 0.28.0 \\nAPI version : 2023-10-11\\n\\nCorrection of blue screen in execution tracking when changing environment\\nRequirement.txt path displayed in execution tracking\\nAddition of get_user() function in SDK\\nAdded documentation on the maximum size of included_folders\\nFixed bugs and improvements\\n\\n\\n\\nV1.1.0 - 11/10/2023\\uf0c1\\nSDK version : 0.27.1 \\nAPI version : 2023-10-03\\n\\nGPU envs are available\\nStep creation is limited to 3min max (the timeout_s parameter can be modified if necessary)\\nAddition of pipeline duration in the experiment tracking\\nFront-end correction (date in UTC and change GitHub references to Git)\\nFixed bugs and improvements', metadata={'source': 'https://mlops-platform-documentation.craft.ai/changeLog.html', 'title': 'Craft AI changelog — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1853}), Document(page_content='V1.0.28 - 03/10/2023\\uf0c1\\nSDK version : 0.27.0 \\nAPI version : 2023-09-25\\n\\nDownload button added to the execution comparison page\\nAdd inputs and outputs to the comparison execution page\\nWait for the step status to be ready before returning the result of create_step()\\nMaximum size of code imported into a step increased from 1 MB to 5 MB\\nAddition of a size limit for value-type I/O (JSON)\\nFixed bugs and improvements\\n\\n\\n\\nV1.0.27 - 27/09/2023\\uf0c1\\nSDK version : 0.25.0 \\nAPI version : 2023-09-14\\n\\nRemoval of the experimental mention on the tech doc for metrics and periodic deployment\\nFixed bugs and improvements\\n\\n\\n\\nV1.0.26 - 14/09/2023\\uf0c1\\nSDK version : 0.24.1 \\nAPI version : 2023-09-14\\n\\nRemoval of the experimental mention on the tech doc for metrics and periodic deployment\\nFixed bugs and improvements\\n\\n\\n\\nV1.0.25 - 10/09/2023\\uf0c1\\nSDK version : 0.24.0 \\nAPI version : 2023-09-06', metadata={'source': 'https://mlops-platform-documentation.craft.ai/changeLog.html', 'title': 'Craft AI changelog — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2842}), Document(page_content='V1.0.25 - 10/09/2023\\uf0c1\\nSDK version : 0.24.0 \\nAPI version : 2023-09-06\\n\\nI/O files can be downloaded from the Execution Tracking page\\nSimple metrics can be viewed on the Execution Comparison page\\nYou can select the columns you want to display and filter/sort the metadata and simple metrics on the Execution Comparison page\\nThe repo, branch and Git commit can be viewed from the Execution Tracking page\\nA limit of 100 simple metrics and 25 list metrics per step has been introduced\\nWe now use the d3 lib to display graphs\\nIt is now possible to have environments with GPUs (standard or medium)\\nAdded the Monitoring page (which lets you track pipeline metrics over a period)\\nAdded the delete_pipeline_execution(execution_id) function, which can be used to delete an execution using its execution_id\\n\\n\\n\\nV1.0.24 - 29/08/2023\\uf0c1\\nSDK version : 0.23.2 \\nAPI version : 2023-08-29\\n\\nFront improvement\\nBug fixes and improvements\\n\\n\\n\\nV1.0.24 - 29/08/2023\\uf0c1\\nSDK version : 0.23.2 \\nAPI version : 2023-08-29', metadata={'source': 'https://mlops-platform-documentation.craft.ai/changeLog.html', 'title': 'Craft AI changelog — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3635}), Document(page_content='V1.0.24 - 29/08/2023\\uf0c1\\nSDK version : 0.23.2 \\nAPI version : 2023-08-29\\n\\nFront improvement\\nBug fixes and improvements\\n\\n\\n\\nV1.0.23 - 09/08/2023\\uf0c1\\nSDK version : 0.23.1 \\nAPI version : 2023-08-01\\n\\nFront improvement\\nBug fixes and improvements\\n\\n\\n\\nV1.0.22 - 01/08/2023\\uf0c1\\nSDK version : 0.23.1 \\nAPI version : 2023-08-01\\n\\nFront improvement (ISO format for dates)\\nBug fixes and improvements\\n\\n\\n\\nV1.0.21 - 26/07/2023\\uf0c1\\nSDK version : 0.23.0 \\nAPI version : 2023-07-26\\n\\nV1 run comparison page added\\nCopy icon for the access path to the datastore in the front end\\nAdded parameter with created_by for the output of the list_pipeline_executions function in the SDK\\nBug fixes and improvements\\n\\n\\n\\nV1.0.20 - 18/07/2023\\uf0c1\\nSDK version : 0.22.0 \\nAPI version : 2023-07-18\\n\\nPossibility to delete a pipeline from front\\nBug fixes and improvements\\n\\n\\n\\nV1.0.19 - 06/07/2023\\uf0c1\\nSDK version : 0.21.1 \\nAPI version : 2023-07-06', metadata={'source': 'https://mlops-platform-documentation.craft.ai/changeLog.html', 'title': 'Craft AI changelog — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4550}), Document(page_content='V1.0.19 - 06/07/2023\\uf0c1\\nSDK version : 0.21.1 \\nAPI version : 2023-07-06\\n\\nDynamic path for outputs to datastore\\nSome fixes on the list metrics front\\nInternal usage metrics for list metrics added\\nBug fixes and improvements\\n\\n\\n\\nV1.0.18 - 28/06/2023\\uf0c1\\nSDK version : 0.21.0 \\nAPI version : 2023-06-28\\n\\nFront-end graphics for list metrics\\nCommit id and branch name available in get_step and list_step\\nEnhanced SDK doc with\\n\\nAdditional usage examples\\nMore details on function returns\\nFlags for experimental features\\n\\n\\nBug fixes and improvements\\n\\n\\n\\nV1.0.17 - 20/06/2023\\uf0c1\\nSDK version : 0.20.0 \\nAPI version : 2023-06-2\\n\\nList metrics (without list metrics values in the frontend, only via SDK for now)\\nBug fixes and improvements\\n\\n\\n\\nV1.0.16 - 15/06/2023\\uf0c1\\nSDK version : 0.19.0 \\nAPI version : 2023-06-14\\n\\nOutput mapping for run\\nAllow other repository than GitHub to fetch step code (example : GitLab)\\nBug fixes and improvements\\n\\n\\n\\nV1.0.15 - 07/06/2023\\uf0c1\\nSDK version : 0.18.0 \\nAPI version : 2023-06-07', metadata={'source': 'https://mlops-platform-documentation.craft.ai/changeLog.html', 'title': 'Craft AI changelog — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5363}), Document(page_content='V1.0.15 - 07/06/2023\\uf0c1\\nSDK version : 0.18.0 \\nAPI version : 2023-06-07\\n\\nPeriodic deployment\\nDatastore as source and destination for I/O files\\nFix for long runs with run_pipeline()\\nBug fixes and improvements\\n\\n\\n\\nV1.0.14 - 26/05/2023\\uf0c1\\nSDK version : 0.17.0 \\nAPI version : 2023-05-26\\n\\nMetrics pipeline (only for metrics with 1 value)\\nPipeline page in front\\nBug fixes and improvements\\n\\n\\n\\nV1.0.13 - 24/05/2023\\uf0c1\\nSDK version : 0.16.0 \\nAPI version : 2023-05-24\\n\\nCustom mappings for the execution pipeline are now available\\nAbility to place env variables in the requirement path when creating a step\\nBug fixes and improvements\\n\\n\\n\\nV1.0.12 - 10/05/2023\\uf0c1\\nSDK version : 0.15.0 \\nAPI version : 2023-05-04\\n\\nUsage metrics (For internal use at Craft AI only)\\nrun_pipeline without custom mapping\\nBug fixes and improvements\\n\\n\\n\\nV1.0.11 - 04/05/2023\\uf0c1\\nSDK version : 0.15.0 \\nAPI version : 2023-05-04\\n\\nBug fixes and improvements\\n\\n\\n\\nV1.0.10 - 26/04/2023\\uf0c1\\nSDK version : 0.14.1 \\nAPI version : 2023-04-26', metadata={'source': 'https://mlops-platform-documentation.craft.ai/changeLog.html', 'title': 'Craft AI changelog — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6274}), Document(page_content='Bug fixes and improvements\\n\\n\\n\\nV1.0.10 - 26/04/2023\\uf0c1\\nSDK version : 0.14.1 \\nAPI version : 2023-04-26\\n\\nBug fixes and improvements\\n\\n\\n\\nV1.0.9 - 21/04/2023\\uf0c1\\nSDK version : 0.14.0 \\nAPI version : 2023-04-20\\n\\nBug fixes and improvements\\n\\n\\n\\nV1.0.8 - 11/04/2023\\uf0c1\\nSDK version : 0.13.2 \\nAPI version : 2023-04-22\\n\\nAdd environment page in the UI\\nBug fixes and improvements\\n\\n\\n\\nV1.0.7 - 28/03/2023\\uf0c1\\nSDK version : 0.13.1 \\nAPI version : 2023-03-22\\n\\nBug fixes and improvements\\n\\n\\n\\nV1.0.6 - 23/03/2023\\uf0c1\\nSDK version : 0.13.0 \\nAPI version : 2023-03-22\\n\\nAdd error messages on the build of steps\\nAdded object STEP_PARAMETER to explain step creation parameters between “fall back on project info” and “null”\\nAdded function get_data_store_object_information()\\nis_null source is now available for all input type\\nFix endpoint mapping with is_required parameter (which was mandatory)\\nVarious fixes and improvements\\n\\n\\n\\nV1.0.5 - 15/03/2023\\uf0c1\\nSDK version : 0.13.0 \\nAPI version : 2023-03-15', metadata={'source': 'https://mlops-platform-documentation.craft.ai/changeLog.html', 'title': 'Craft AI changelog — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7147}), Document(page_content='V1.0.5 - 15/03/2023\\uf0c1\\nSDK version : 0.13.0 \\nAPI version : 2023-03-15\\n\\nNew front page for input/output in execution tracking\\nAdd ability to create a step with a custom dockerfile\\nAdd function list_pipelines() to get list of all pipelines\\nBug fixes\\n\\n\\n\\nV1.0.4 - 28/02/2023\\uf0c1\\nSDK version : 0.12.0 \\nAPI version : 2023-02-24\\n\\nAdded ability to trigger endpoint with file input/output in io feature\\nFixed input mapping ‘isRequired’ matches step input\\nFixed issue with refreshing environments and deployments lists in the frontend\\nFixed default value for included folders\\n\\n\\n\\nV1.0.3 - 17/02/2023\\uf0c1\\nSDK version : 0.11.0 \\nAPI version : 2023-02-17\\n\\nFix input mapping bug with constant source\\nAdd Google Tag Manager\\nRework login page\\nOther bug fixes\\n\\n\\n\\nV1.0.2 - 15/02/2023\\uf0c1\\nSDK version : 0.10.1 \\nAPI version : 2023-02-13\\n\\nRename force_step_deletion to force_dependents_deletion\\nBug fix\\n\\n\\n\\nV1.0.1 - 10/02/2023\\uf0c1\\nSDK version : 0.10.0 \\nAPI version : 2023-02-06', metadata={'source': 'https://mlops-platform-documentation.craft.ai/changeLog.html', 'title': 'Craft AI changelog — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8032}), Document(page_content='V1.0.1 - 10/02/2023\\uf0c1\\nSDK version : 0.10.0 \\nAPI version : 2023-02-06\\n\\nProject information from GitHub is taken into account for step creation (and therefore optional at this stage)\\nImprove error messages for step creation\\nMessage for first login\\nFront improvement/correction\\nBug correction\\n\\n\\n\\nV1.0.0 - 27/01/2023\\uf0c1\\nSDK version : 0.9.0 \\nAPI version : 2023-01-26\\n\\nAdds separate environments for each project.\\nInput/output (IO) added to steps.\\nAdded IO mapping for the endpoint\\nAdd automatic redirection to wait for pipeline output.\\nSyntax change: force_endpoints_deletion replaced by force_deployments_deletion.\\nWhen executions are retrieved, two variables are added to the result:\\n\\nstart_date for the ;\\nend_date for pipelines and stages.\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/changeLog.html', 'title': 'Craft AI changelog — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8904}), Document(page_content='Administration — Craft AI MLOps platform documentation  documentation\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMenu\\n\\nIntroduction\\nAdministration\\nGet Started\\nPart 0: Setup\\nPart 1: Execute a simple pipeline\\nPart 2: Execute a simple ML model\\nPart 3: Execute a ML use case with inputs and outputs\\nPart 4: Deploy a ML use case with inputs and outputs\\n\\n\\nUser workflow\\nIntroduction\\nAccessing your data\\nRun and serve your pipelines\\n\\n\\nEnvironments\\nIntroduction\\nCreate an environment\\nWork with environment variables\\nSave my data on the store\\n\\n\\nStep & Pipeline\\nIntroduction\\nCreate a step\\nManage a step\\nCompose a pipeline\\n\\n\\nDeployment\\nIntroduction\\nChoose an execution rule\\nDefine the pipeline sources and destinations\\nExecute a pipeline\\nFollow the deployment executions\\nMetrics\\n\\n\\nCraft AI SDK documentation\\nCraft AI changelog\\n\\n\\n\\n\\n\\n\\nCraft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nAdministration', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5}), Document(page_content='Craft AI MLOps platform documentation\\n\\n\\n\\n\\n\\nhome\\nAdministration\\n\\n\\n\\n\\n\\n\\n\\n\\nAdministration\\uf0c1\\nThe MLOps - Craft AI Platform is composed of a graphical interface\\nallowing you to visualize, create, manage and monitor the objects\\nnecessary for the realization of your AI projects.\\nIt is composed of:\\n\\nHomepage: Visualize and create projects.\\nParameters: Manage the company’s account and users.\\nProject settings: Manage the configuration of a project.\\nEnvironments: See the environment(s) and their information within\\na project.\\nExecutions: See the execution(s) and their information within an\\nenvironment or a deployment.\\n\\n\\nNote: We strongly recommend Google Chrome browser to use the\\ngraphical interface of the platform.\\n\\nSummary:\\n\\nUsers\\nProjects\\nToken SDK\\n\\n\\nUsers\\uf0c1\\nSummary:\\n\\nManage user\\nLogin\\nGet user with ID\\n\\n\\nManage user\\uf0c1\\nThe management of user is available by email for the moment. In a next\\nversion, it will be possible to add, edit and delete a user directly on\\nthe platform UI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 828}), Document(page_content='Add a user\\uf0c1\\nSend a message to Craft AI with your request and the following\\ninformation:\\n\\nFirst and last name of the user\\nEmail of the user\\n\\n\\n\\n   🆕 Adding users will arrive later on the platform.\\n\\n\\n\\nAccess rights\\uf0c1\\nEach user has access to one or more defined projects.\\nEach user who has access to a project has access to all the information and actions in it.\\n\\n\\n   🆕 Advanced access rights will arrive later on the platform.\\n\\n\\n\\nDelete a user\\uf0c1\\nSend a message to Craft AI with your request and the following\\ninformation:\\n\\nFirst and last name of the user\\nEmail of the user\\n\\n\\n\\n   🆕 The deletion of users will arrive later on the platform.\\n\\n\\n\\n\\nLogin\\uf0c1\\nHere is the URL to access the platform:\\nhttps://mlops-platform.craft.ai\\n\\nFirst connection\\uf0c1\\nPrerequisites: The user must be added to the platform (Add a user).', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 1806}), Document(page_content='First connection\\uf0c1\\nPrerequisites: The user must be added to the platform (Add a user).\\n\\nThe user connects to the platform: https://mlops-platform.craft.ai\\nThe connection will not work, but Craft AI will receive the request\\nand can add the user.\\nThe user receives an email/slack from Craft AI to inform him that he\\ncan log in.\\nThe user logs in with the same link and has access to the platform.\\n\\n\\n\\nForgot your password\\uf0c1\\n\\nIn the Login popup, click on Don’t remember your password?\\nEnter your email, you will receive an email to modify your password.\\n\\n\\n\\n\\nGet user with ID\\uf0c1\\n\\nFunction definition\\uf0c1\\nWhile using the SDK you may encounter outputs parameters containing a user when using specific functions. Each user is identified with a unique ID. That is the case for example in the sdk.get_pipeline_execution() function output with the parameter created_by. To match the user ID with the corresponding information (name and email) you can use the get_user() function.\\nCraftAiSdk.get_user(user_id)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 2523}), Document(page_content='Parameters\\uf0c1\\n\\nuser_id (str) – The ID of the user.\\n\\n\\n\\nReturns\\uf0c1\\nThe user information in dict type, with the following keys:\\n\\nid (str): ID of the user.\\nname (str): Name of the user.\\nemail (str): Email of the user.\\n\\n\\n\\n\\n\\nProjects\\uf0c1\\nA project is a complete use case. From data import, through\\nexperimentation, testing and production, to performance monitoring over\\ntime.\\nUsers can create as many projects as they want. Each project is\\nhermetically sealed from the others. All users on the platform can\\naccess to all projects.\\nTo work on a project, it must include at least one environment. The\\nuser can create several environments in a project.\\nSummary:\\n\\nCreate a project\\nManange a prject\\n\\n\\nCreate a project\\uf0c1\\n\\nFrom the homepage, you can create a new project by clicking on the\\n“New project” button.\\nA project creation page opens in which you have to fill in the fields\\n:', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 3517}), Document(page_content='[Mandatory]Project name : Enter the name of your project (in\\nlowercase and with “-”), you will not be able to modify it later.\\n[Mandatory]Python version : Select the version of Python\\nthat will be applied to this project. It will be possible to\\nchoose a different version when creating each step.\\n[Mandatory]Repository URL : Enter the SSH URL of your repository.\\n\\nHow to get my repository URL\\nOn GitHub, from your repository, click on the “Code” button and\\nchoose SSH. The URL must start with “git@”.\\n\\nOn GitLab, from your repository, click on the “Clone” button and\\ncopy SSH URL. The URL must start with “git@”.\\n\\n\\n\\n[Mandatory]Deploy key : Enter your Github / GitLab private key.\\nHow to generate my deploy key :', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4381}), Document(page_content='[Mandatory]Deploy key : Enter your Github / GitLab private key.\\nHow to generate my deploy key :\\n\\nFor security reasons, to get access to your GitHub project, the\\nplatform uses a Deploy Key with the RSA SSH KEY standard. The\\ndeploy key is a special key that grants access to a specific\\nrepository; it is not the same as personal keys used commonly\\nby users to access their repositories, although they are both\\nSSH keys.\\nThe deploy key has two elements:\\n\\nThe public key, which must be set in the GitHub\\nadministration settings for the repository.\\nThe private key, which must be sent to the Craft AI MLOps\\nPlatform, so it can access the repository.\\n\\nPlease follow these steps:\\n\\nYou will need to generate an SSH key on your computer.\\n\\nOn Linux and macOS', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 4997}), Document(page_content='You will need to generate an SSH key on your computer.\\n\\nOn Linux and macOS\\n\\nMove to a new directory and run the following command, by\\nreplacing *your-key-filename* by a name of\\nyour choosing.\\nssh-keygen -m PEM -t rsa -b 4096 -f *your-key-filename* -q -N \"\" -C \"\"\\nThis will generate two files: a file named\\n*your-key-filename* with the “private key” used\\nfor creating a step, and a file named\\n*your-key-filename*.pub with the “public key”\\nused to create the Deploy Key on GitHub. These files\\nshould not be included in the step’s directory. Only the\\ntype of SSH key generated by this command is accepted\\nwhen creating the Step.\\nExample on Linux / macOS :\\nssh-keygen -m PEM -t rsa -b 4096 -f *keyAccesPlatformCraftAI* -q -N \"\" -C \"\"\\n\\n\\n\\n\\nOn window', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 5671}), Document(page_content='Check that OpenSSH is installed and install it if it\\nis not the case. (Go in Settings > Apps & features >\\nOptional feature to get list of features and click on\\nadd feature to find and install OpenSSH)\\nPress the Windows key.\\nType cmd.\\nUnder Best Match, right-click Command Prompt.\\nClick Run as Administrator.\\nIf prompted, click Yes in\\nthe ``Do you want to allow this app to make changes to your device?``\\npop-up.\\nIn the command prompt, type the following : ssh-keygen\\nBy default, the system will save the keys to C:Usersyour_username/.ssh/id_rsa. You\\ncan use the default name, or you can choose more\\ndescriptive names. This can help distinguish between\\nkeys, if you are using multiple key pairs. To stick to\\nthe default option, press  Enter.\\nYou’ll be asked to enter a\\npassphrase. Hit Enter to skip this step.\\nThe system will generate the key pair, and display\\nthe key fingerprint and a randomart image.\\nOpen your file browser.\\nNavigate\\nto C:Usersyour_username/.ssh.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 6417}), Document(page_content='Open your file browser.\\nNavigate\\nto C:Usersyour_username/.ssh.\\nYou should see two files. The identification is saved\\nin the id_rsa file and the public key is\\nlabeled id_rsa.pub. This is your SSH key pair.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7320}), Document(page_content='For GitHub :\\n\\nHead to the homepage of your repository on GitHub.\\nGo to the Settings page.\\nOnce there, select the tab on the left named Deploy Keys\\nSelect Add deploy key on the Deploy Keys page.\\n\\n\\nInsert the name you want for your deploy key\\nCopy/paste the public key (content of\\n*your-key-filename*.pub) in the second text box.\\nClick on “Add key” (you don’t need to allow write\\naccess)\\n\\nFor GitLab :\\n\\nHead to the homepage of your repository on GitLab.\\nClick on Settings (left bar) then go to Repository\\nClick on Expand in the Deploy keys section\\nInsert the name you want for your deploy key.\\nCopy/paste the public key (content of\\n*your-key-filename*.pub) in the second text box.\\nClick on Add key (you don’t need to Grant write permissions to this key)', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 7531}), Document(page_content='Default branch : Enter the Git branch you want as default\\nfor this project. If this field is empty, we will use the default\\nGit branch. It will be possible to choose a different default\\nbranch within an environment.\\n[Mandatory]Folders : Enter file(s) or folder(s) that will be\\naccessible by the platform. By default, type / to have access\\nto all the repositories. It will be possible to choose different\\nfiles or folders within an environment.\\nRequirements.txt : Enter the path to the requirements.txt file\\nwith the list of library to install automatically on this project.\\nIt will be possible to add a different file within an environment.\\nSystem dependencies : Enter the list of the APT and/or APK\\npackages that you want to install automatically on this project\\n(for Ubuntu distribution). It will be possible to add different\\npackages within an environment.\\n\\n\\nClick on “Create project”, you will see the project card displayed,\\nand you can enter in it to create a first environment.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 8286}), Document(page_content='There are no access rights at first, all users of the platform have\\naccess to all projects.\\n\\n\\nNote: You will be able to modify these elements (except the\\nproject name) in the Settings section of your project.\\n\\nNext step: Create an environment in this project.\\n\\n\\nManage a project\\uf0c1\\n\\nEdit a project\\uf0c1\\nWithin your project, click on Settings to view and edit your project\\ninformation.\\n\\nOn this page, you will find the information you set up when you created\\nthe project.\\nYou can change them, except the name of the project.\\n\\nWarning\\nDon’t forget to save and validate the confirmation slider\\nto make the changes effective.\\n\\nThe changes will apply to steps created after this modification. These\\nchanges may affect your steps and can make them non-functional.\\n\\n\\n\\nUsers in a project\\uf0c1\\nInitially, all users of the platform have access to all projects without\\nspecial access rights.\\n\\n\\n   🆕 The access rights per user within a project will arrive later on the\\n   platform.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 9272}), Document(page_content='🆕 The access rights per user within a project will arrive later on the\\n   platform.\\n\\n\\n\\nDelete a project\\uf0c1\\nInitially, you cannot delete a project.\\nSubmit an email request to delete a project from the platform.\\n\\n\\n   🆕 The deletion of a project will arrive later on the platform.\\n\\n\\n\\n\\n\\nAccess the SDK\\uf0c1\\nThe Craft AI MLOps Platform is composed of a Python SDK allowing you\\nto use, from your IDE, the functions developed by Craft AI to create\\nyour steps, pipelines and deployments.\\nSummary:\\n\\nGet token access\\nConnect to the SDK\\n\\n\\nGet token access\\uf0c1\\nIn the header, click on your name and go to the “Parameters” page.\\n\\nAt the bottom of the “Account” page, you will find your SDK token,\\nwhich will allow you to identify yourself to use the SDK.\\nYou can regenerate it, the old token will be obsolete, and you will\\nhave to identify yourself again to the SDK to access it.\\n\\nWarning\\nThe SDK token is strictly personal. You must keep it to yourself.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 10148}), Document(page_content='Warning\\nThe SDK token is strictly personal. You must keep it to yourself.\\n\\n\\n\\n\\nConnect to the SDK\\uf0c1\\nYou must install the SDK from a Python terminal, with the command :\\npip install craft-ai-sdk\\n\\n\\nRun the following commands in a Python terminal to initialize the\\nSDK.\\n\\nEnter the CRAFT_AI_ENVIRONMENT_URL, you can find on Get\\nenvironment URL.\\nIt should look like https://brisk-hawk-charcoal-zeta-42.mlops-platform.craft.ai.\\nIf you don’t have any environments in your project yet, you should\\nCreate an environment.\\n\\nEnter your personal CRAFT_AI_SDK_TOKEN, you can find it in “Parameters” on the platform web UI.\\nexport CRAFT_AI_ENVIRONMENT_URL=\"*your-env-URL*\"\\nexport CRAFT_AI_SDK_TOKEN=\"*your-token-acces*\"\\n\\n\\n\\nExecute the following Python code to set up SDK Python object.\\nThe SDK will automatically take into account your environment variables for the installation of the connection to the platform.\\nimport os\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk()', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 11007}), Document(page_content='sdk = CraftAiSdk()\\n\\n\\nYou can also specify it directly in the constructor, although this method is not recommended.\\nimport os\\nfrom craft_ai_sdk import CraftAiSdk\\n\\nsdk = CraftAiSdk(\\n    environment_url=\"*your-env-URL*\",\\n    sdk_token=\"*your-token-acces*\"\\n)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n Previous\\nNext \\n\\n\\n\\n© Copyright 2023, Craft AI.', metadata={'source': 'https://mlops-platform-documentation.craft.ai/administration.html', 'title': 'Administration — Craft AI MLOps platform documentation  documentation', 'language': 'en', 'start_index': 11950})]\n"
     ]
    }
   ],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/Documents/dev/RAG/.conda/lib/python3.10/site-packages/pinecone/index.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import pinecone\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY']) # set openai_api_key = 'your_openai_api_key'\n",
    "pinecone.init(\n",
    "            api_key= os.environ['PINECONE_API_KEY'], # set api_key = 'yourapikey'\n",
    "            environment= 'gcp-starter'\n",
    ")\n",
    "\n",
    "index_label = 'index-1'\n",
    "index_name = pinecone.Index(index_label)\n",
    "\n",
    "# you can also set your api keys as environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "llm.predict(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Pinecone.from_documents(texts, embeddings, index_name=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load existing vdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['Pinecone', 'OpenAIEmbeddings'] vectorstore=<langchain.vectorstores.pinecone.Pinecone object at 0x7f78254d3a90>\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Pinecone.from_existing_index(index_label, embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pinecone' has no attribute 'GRPCIndex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/hugo/Documents/dev/RAG/RAG_poc.ipynb Cell 12\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hugo/Documents/dev/RAG/RAG_poc.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Initialize Pinecone with your API key and service name\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hugo/Documents/dev/RAG/RAG_poc.ipynb#X44sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pinecone\u001b[39m.\u001b[39minit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hugo/Documents/dev/RAG/RAG_poc.ipynb#X44sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             api_key\u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron[\u001b[39m'\u001b[39m\u001b[39mPINECONE_API_KEY\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m# set api_key = 'yourapikey'\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hugo/Documents/dev/RAG/RAG_poc.ipynb#X44sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m             environment\u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgcp-starter\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hugo/Documents/dev/RAG/RAG_poc.ipynb#X44sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hugo/Documents/dev/RAG/RAG_poc.ipynb#X44sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m index \u001b[39m=\u001b[39m pinecone\u001b[39m.\u001b[39;49mGRPCIndex(\u001b[39m'\u001b[39m\u001b[39mindex-1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pinecone' has no attribute 'GRPCIndex'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Pinecone with your API key and service name\n",
    "\n",
    "pinecone.init(\n",
    "            api_key= os.environ['PINECONE_API_KEY'], # set api_key = 'yourapikey'\n",
    "            environment= 'gcp-starter'\n",
    ")\n",
    "\n",
    "index = pinecone.GRPCIndex('index-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages= True)\n",
    "chain = ConversationalRetrievalChain.from_llm(llm, retriever= retriever, memory= memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'redit la meme chose en francais et en moins long'\n",
    "res = chain.run({'question': query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reflexion est un cadre qui donne aux agents des capacités de mémoire dynamique et d'autoréflexion pour améliorer leurs compétences en raisonnement. Il utilise un modèle d'apprentissage par renforcement standard avec une fonction de récompense binaire et un espace d'action augmenté avec du langage pour permettre des étapes de raisonnement complexes. Après chaque action, l'agent calcule une heuristique pour déterminer si la trajectoire est inefficace ou contient des hallucinations. L'autoréflexion est créée en montrant des exemples d'échecs à l'agent et en ajoutant ces exemples dans sa mémoire de travail pour guider les futurs changements dans le plan.\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConversationalRetrievalChain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/hugo/Documents/dev/RAG/RAG_poc.ipynb Cell 19\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hugo/Documents/dev/RAG/RAG_poc.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mllms\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAI\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hugo/Documents/dev/RAG/RAG_poc.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m qa \u001b[39m=\u001b[39m ConversationalRetrievalChain\u001b[39m.\u001b[39mfrom_llm(OpenAI(model_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m'\u001b[39m,temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), retriever, return_source_documents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ConversationalRetrievalChain' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(OpenAI(model_name='gpt-3.5-turbo',temperature=0), retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(ChatOpenAI(model_name='gpt-3.5-turbo',temperature=0), retriever, return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "query = 'do you know anything about the reflexion framework'\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To create a step in the Craft AI MLOps platform, you can follow these steps:\n",
      "\n",
      "1. Connect to the platform and access the desired environment.\n",
      "2. Use the `sdk.create_step()` function to create a step. This function requires several parameters:\n",
      "   - `step_name`: Specify a name for the step.\n",
      "   - `function_path`: Provide the path to the Python file containing the step's code.\n",
      "   - `function_name`: Specify the name of the function within the Python file that represents the step.\n",
      "   - `description`: Add a description for the step.\n",
      "   - `inputs`: Define the inputs for the step. This can be a list of input objects.\n",
      "   - `outputs`: Define the outputs for the step. This can be a list of output objects.\n",
      "   - `container_config`: Configure the container for the step, including any required folders and the path to the requirements file.\n",
      "\n",
      "Here's an example of how the `sdk.create_step()` function can be used:\n",
      "\n",
      "```python\n",
      "sdk.create_step(\n",
      "   step_name=\"part-4-iris-deployment\",\n",
      "   function_path=\"src/part-4-iris-predict.py\",\n",
      "   function_name=\"predictIris\",\n",
      "   description=\"This function retrieves the trained model and classifies the input data by returning the prediction.\",\n",
      "   inputs=[prediction_input, model_input],\n",
      "   outputs=[prediction_output],\n",
      "   container_config={\n",
      "     \"included_folders\": [\"src\"],\n",
      "     \"requirements_path\": \"requirements.txt\",\n",
      "   },\n",
      ")\n",
      "```\n",
      "\n",
      "Make sure to replace the values in the example with your own step details. page_content='Create your step\\uf0c1\\nNow as in Part 3, it is time to create our step on the platform using\\nthe sdk.create_step() function, with our inputs\\nand output:\\nsdk.create_step(\\n   step_name=\"part-4-iris-deployment\",\\n   function_path=\"src/part-4-iris-predict.py\",\\n   function_name=\"predictIris\",\\n   description=\"This function retrieves the trained model and classifies the input data by returning the prediction.\",\\n   inputs=[prediction_input, model_input],\\n   outputs=[prediction_output],\\n   container_config={\\n     \"included_folders\": [\"src\"],\\n     \"requirements_path\": \"requirements.txt\",\\n   },\\n)' metadata={'language': 'en', 'source': 'https://mlops-platform-documentation.craft.ai/getStarted/part_4.html', 'start_index': 4872.0, 'title': 'Part 4: Deploy a ML use case with inputs and outputs — Craft AI MLOps platform documentation  documentation'}\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = 'How to create a step ?'\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "print(result['answer'], result['source_documents'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'do you know anything about the reflexion framework', 'chat_history': [], 'answer': \"Yes, I have information about the Reflexion framework. Reflexion is a framework that equips agents with dynamic memory and self-reflection capabilities to improve their reasoning skills. It follows a standard reinforcement learning (RL) setup, where the agent receives a simple binary reward and has an action space augmented with language to enable complex reasoning steps.\\n\\nThe Reflexion framework includes several components. One of them is the heuristic function, which determines when a trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success, while hallucination refers to encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\n\\nSelf-reflection is another important aspect of the Reflexion framework. The agent is shown two-shot examples, which consist of a failed trajectory and an ideal reflection that guides future changes in the plan. These reflections are added to the agent's working memory, up to three, and can be used as context for querying a language model.\\n\\nThe Reflexion framework also includes a memory stream, which is a long-term memory module that records a comprehensive list of the agent's experiences in natural language. This memory stream can be used for retrieval, where the agent surfaces relevant memories based on their relevance, recency, and importance. The reflection mechanism synthesizes these memories into higher-level inferences over time and guides the agent's future behavior.\\n\\nOverall, the Reflexion framework aims to enable agents to improve iteratively by refining past action decisions and correcting previous mistakes through self-reflection and memory-based reasoning.\", 'source_documents': [Document(page_content='Fig. 2.  Examples of reasoning trajectories for knowledge-intensive tasks (e.g. HotpotQA, FEVER) and decision-making tasks (e.g. AlfWorld Env, WebShop). (Image source: Yao et al. 2023).\\nIn both experiments on knowledge-intensive tasks and decision-making tasks, ReAct works better than the Act-only baseline where Thought: … step is removed.\\nReflexion (Shinn & Labash 2023) is a framework to equips agents with dynamic memory and self-reflection capabilities to improve reasoning skills. Reflexion has a standard RL setup, in which the reward model provides a simple binary reward and the action space follows the setup in ReAct where the task-specific action space is augmented with language to enable complex reasoning steps. After each action $a_t$, the agent computes a heuristic $h_t$ and optionally may decide to reset the environment to start a new trial depending on the self-reflection results.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 4859.0, 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}), Document(page_content='Fig. 3. Illustration of the Reflexion framework. (Image source: Shinn & Labash, 2023)\\nThe heuristic function determines when the trajectory is inefficient or contains hallucination and should be stopped. Inefficient planning refers to trajectories that take too long without success. Hallucination is defined as encountering a sequence of consecutive identical actions that lead to the same observation in the environment.\\nSelf-reflection is created by showing two-shot examples to LLM and each example is a pair of (failed trajectory, ideal reflection for guiding future changes in the plan). Then reflections are added into the agent’s working memory, up to three, to be used as context for querying LLM.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 5764.0, 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}), Document(page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 3379.0, 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}), Document(page_content='Memory stream: is a long-term memory module (external database) that records a comprehensive list of agents’ experience in natural language.\\n\\nEach element is an observation, an event directly provided by the agent.\\n- Inter-agent communication can trigger new natural language statements.\\n\\n\\nRetrieval model: surfaces the context to inform the agent’s behavior, according to relevance, recency and importance.\\n\\nRecency: recent events have higher scores\\nImportance: distinguish mundane from core memories. Ask LM directly.\\nRelevance: based on how related it is to the current situation / query.\\n\\n\\nReflection mechanism: synthesizes memories into higher level inferences over time and guides the agent’s future behavior. They are higher-level summaries of past events (<- note that this is a bit different from self-reflection above)', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 25744.0, 'title': \"LLM Powered Autonomous Agents | Lil'Log\"})]}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arize-phoenix\n",
      "  Downloading arize_phoenix-1.0.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting ddsketch (from arize-phoenix)\n",
      "  Downloading ddsketch-2.0.4-py3-none-any.whl (18 kB)\n",
      "Collecting hdbscan<1.0.0,>=0.8.33 (from arize-phoenix)\n",
      "  Downloading hdbscan-0.8.33.tar.gz (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/hugo/.local/lib/python3.10/site-packages (from arize-phoenix) (3.1.2)\n",
      "Requirement already satisfied: numpy in /home/hugo/.local/lib/python3.10/site-packages (from arize-phoenix) (1.25.2)\n",
      "Requirement already satisfied: pandas in /home/hugo/.local/lib/python3.10/site-packages (from arize-phoenix) (2.0.3)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.20 in /home/hugo/.local/lib/python3.10/site-packages (from arize-phoenix) (4.24.1)\n",
      "Requirement already satisfied: psutil in ./.conda/lib/python3.10/site-packages (from arize-phoenix) (5.9.0)\n",
      "Collecting pyarrow (from arize-phoenix)\n",
      "  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting scikit-learn<1.3.0 (from arize-phoenix)\n",
      "  Downloading scikit_learn-1.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy (from arize-phoenix)\n",
      "  Downloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sortedcontainers (from arize-phoenix)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting starlette (from arize-phoenix)\n",
      "  Downloading starlette-0.32.0.post1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting strawberry-graphql==0.208.2 (from arize-phoenix)\n",
      "  Downloading strawberry_graphql-0.208.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: typing-extensions in /home/hugo/.local/lib/python3.10/site-packages (from arize-phoenix) (4.7.1)\n",
      "Collecting umap-learn (from arize-phoenix)\n",
      "  Downloading umap-learn-0.5.4.tar.gz (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting uvicorn (from arize-phoenix)\n",
      "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting wrapt (from arize-phoenix)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting graphql-core<3.3.0,>=3.2.0 (from strawberry-graphql==0.208.2->arize-phoenix)\n",
      "  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.9/202.9 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.7.0 in /home/hugo/.local/lib/python3.10/site-packages (from strawberry-graphql==0.208.2->arize-phoenix) (2.8.2)\n",
      "Collecting cython<3,>=0.27 (from hdbscan<1.0.0,>=0.8.33->arize-phoenix)\n",
      "  Using cached Cython-0.29.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (3.1 kB)\n",
      "Collecting joblib>=1.0 (from hdbscan<1.0.0,>=0.8.33->arize-phoenix)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn<1.3.0->arize-phoenix)\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: six in ./.conda/lib/python3.10/site-packages (from ddsketch->arize-phoenix) (1.16.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->arize-phoenix)\n",
      "  Downloading MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->arize-phoenix)\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/hugo/.local/lib/python3.10/site-packages (from pandas->arize-phoenix) (2023.3)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in ./.conda/lib/python3.10/site-packages (from starlette->arize-phoenix) (3.7.1)\n",
      "Collecting numba>=0.51.2 (from umap-learn->arize-phoenix)\n",
      "  Downloading numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn->arize-phoenix)\n",
      "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: tqdm in ./.conda/lib/python3.10/site-packages (from umap-learn->arize-phoenix) (4.66.1)\n",
      "Collecting tbb>=2019.0 (from umap-learn->arize-phoenix)\n",
      "  Downloading tbb-2021.10.0-py2.py3-none-manylinux1_x86_64.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: click>=7.0 in ./.conda/lib/python3.10/site-packages (from uvicorn->arize-phoenix) (7.0)\n",
      "Requirement already satisfied: h11>=0.8 in ./.conda/lib/python3.10/site-packages (from uvicorn->arize-phoenix) (0.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette->arize-phoenix) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette->arize-phoenix) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in ./.conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette->arize-phoenix) (1.1.3)\n",
      "Collecting llvmlite<0.42,>=0.41.0dev0 (from numba>=0.51.2->umap-learn->arize-phoenix)\n",
      "  Downloading llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Downloading arize_phoenix-1.0.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading strawberry_graphql-0.208.2-py3-none-any.whl (269 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.0/269.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.32.0.post1-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.0/70.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached Cython-0.29.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading numba-0.58.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tbb-2021.10.0-py2.py3-none-manylinux1_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Downloading llvmlite-0.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: hdbscan, umap-learn, pynndescent\n",
      "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdbscan: filename=hdbscan-0.8.33-cp310-cp310-linux_x86_64.whl size=735903 sha256=2683ca4a04eac4207e62ce7ab61aa79858963b46700c702ce86de957a5a9708e\n",
      "  Stored in directory: /home/hugo/.cache/pip/wheels/75/0b/3b/dc4f60b7cc455efaefb62883a7483e76f09d06ca81cf87d610\n",
      "  Building wheel for umap-learn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for umap-learn: filename=umap_learn-0.5.4-py3-none-any.whl size=86770 sha256=87e82ef484f2f7dd8666a88d3c029bea118855d3cbeddcfc9fe099bef197d006\n",
      "  Stored in directory: /home/hugo/.cache/pip/wheels/fb/66/29/199acf5784d0f7b8add6d466175ab45506c96e386ed5dd0633\n",
      "  Building wheel for pynndescent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55615 sha256=969b9ee0237915ff0d03c05b521561d54c4d14a2f8ed5ab769e72456791b1eb7\n",
      "  Stored in directory: /home/hugo/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
      "Successfully built hdbscan umap-learn pynndescent\n",
      "Installing collected packages: tbb, sortedcontainers, pytz, wrapt, uvicorn, threadpoolctl, scipy, pyarrow, MarkupSafe, llvmlite, joblib, graphql-core, ddsketch, cython, strawberry-graphql, starlette, scikit-learn, numba, pynndescent, hdbscan, umap-learn, arize-phoenix\n",
      "Successfully installed MarkupSafe-2.1.3 arize-phoenix-1.0.0 cython-0.29.36 ddsketch-2.0.4 graphql-core-3.2.3 hdbscan-0.8.33 joblib-1.3.2 llvmlite-0.41.1 numba-0.58.1 pyarrow-14.0.1 pynndescent-0.5.10 pytz-2023.3.post1 scikit-learn-1.2.2 scipy-1.11.3 sortedcontainers-2.4.0 starlette-0.32.0.post1 strawberry-graphql-0.208.2 tbb-2021.10.0 threadpoolctl-3.2.0 umap-learn-0.5.4 uvicorn-0.24.0.post1 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install arize-phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌍 To view the Phoenix app in your browser, visit http://127.0.0.1:6060/\n",
      "📺 To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "📖 For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    }
   ],
   "source": [
    "import phoenix as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Launch phoenix\n",
    "session = px.launch_app()\n",
    "\n",
    "\n",
    "from phoenix.trace.langchain import OpenInferenceTracer, LangChainInstrumentor\n",
    "\n",
    "# If no exporter is specified, the tracer will export to the locally running Phoenix server\n",
    "tracer = OpenInferenceTracer()\n",
    "LangChainInstrumentor(tracer).instrument()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The types of memory discussed in the document are sensory memory, short-term memory, long-term memory, memory stream, retrieval model, and reflection mechanism. page_content='Short-Term Memory (STM) or Working Memory: It stores information that we are currently aware of and needed to carry out complex cognitive tasks such as learning and reasoning. Short-term memory is believed to have the capacity of about 7 items (Miller 1956) and lasts for 20-30 seconds.\\n\\n\\nLong-Term Memory (LTM): Long-term memory can store information for a remarkably long time, ranging from a few days to decades, with an essentially unlimited storage capacity. There are two subtypes of LTM:\\n\\nExplicit / declarative memory: This is memory of facts and events, and refers to those memories that can be consciously recalled, including episodic memory (events and experiences) and semantic memory (facts and concepts).\\nImplicit / procedural memory: This type of memory is unconscious and involves skills and routines that are performed automatically, like riding a bike or typing on a keyboard.\\n\\n\\n\\n\\nFig. 8. Categorization of human memory.\\nWe can roughly consider the following mappings:' metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 11271.0, 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = 'Give me the differants Types of Memory from your document'\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history}, callbacks=[tracer])\n",
    "print(result['answer'], result['source_documents'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in ./.conda/lib/python3.10/site-packages (0.104.1)\n",
      "Requirement already satisfied: uvicorn in ./.conda/lib/python3.10/site-packages (0.24.0.post1)\n",
      "Requirement already satisfied: pydantic in /home/hugo/.local/lib/python3.10/site-packages (1.10.12)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in ./.conda/lib/python3.10/site-packages (from fastapi) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in ./.conda/lib/python3.10/site-packages (from fastapi) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.conda/lib/python3.10/site-packages (from fastapi) (4.8.0)\n",
      "Requirement already satisfied: click>=7.0 in ./.conda/lib/python3.10/site-packages (from uvicorn) (7.0)\n",
      "Requirement already satisfied: h11>=0.8 in ./.conda/lib/python3.10/site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./.conda/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.conda/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in ./.conda/lib/python3.10/site-packages (from anyio<4.0.0,>=3.7.1->fastapi) (1.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install fastapi uvicorn pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Create a Pydantic model to define the expected data structure in the request body\n",
    "class Item(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "\n",
    "# Define a POST route to create a new item\n",
    "@app.post(\"/items/\")\n",
    "async def create_item(item: Item):\n",
    "    # In this example, the data sent in the request body is automatically parsed\n",
    "    # and validated based on the Item model.\n",
    "    # You can process and store the received data as needed.\n",
    "    return {\"item\": item}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-platform-documentation-YTJX0krz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
